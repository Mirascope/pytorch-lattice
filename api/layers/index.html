
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://willbakst.github.io/pytorch-lattice/api/layers/">
      
      
        <link rel="prev" href="../../walkthroughs/uci_adult_income/">
      
      
        <link rel="next" href="../models/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.21">
    
    
      
        <title>layers - PyTorch Lattice</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.66ac8b77.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-Q8WNH5KD11"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-Q8WNH5KD11",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-Q8WNH5KD11",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#layers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="PyTorch Lattice" class="md-header__button md-logo" aria-label="PyTorch Lattice" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            PyTorch Lattice
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              layers
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/willbakst/pytorch-lattice/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch-lattice
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../.." class="md-tabs__link">
          
  
  Get Started

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../concepts/classifier/" class="md-tabs__link">
          
  
  Concepts

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../walkthroughs/uci_adult_income/" class="md-tabs__link">
          
  
  Walkthroughs

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
  API Reference

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="PyTorch Lattice" class="md-nav__button md-logo" aria-label="PyTorch Lattice" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    PyTorch Lattice
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/willbakst/pytorch-lattice/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    pytorch-lattice
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Get Started
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Get Started
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Welcome to PyTorch Lattice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../why/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Why use PyTorch Lattice
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contributing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../help/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How to help
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Concepts
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/classifier/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Classifier
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/calibrators/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Calibrators
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/shape_constraints/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Shape Constraints
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/model_types/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model Types
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/plotting/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Plotting
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Walkthroughs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Walkthroughs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../walkthroughs/uci_adult_income/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UCI Adult Income
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    layers
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    layers
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator" class="md-nav__link">
    <span class="md-ellipsis">
      CategoricalCalibrator
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CategoricalCalibrator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.apply_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      apply_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.assert_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      assert_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.keypoints_inputs" class="md-nav__link">
    <span class="md-ellipsis">
      keypoints_inputs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.keypoints_outputs" class="md-nav__link">
    <span class="md-ellipsis">
      keypoints_outputs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice" class="md-nav__link">
    <span class="md-ellipsis">
      Lattice
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lattice">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice.apply_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      apply_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice.assert_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      assert_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear" class="md-nav__link">
    <span class="md-ellipsis">
      Linear
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear.apply_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      apply_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear.assert_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      assert_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator" class="md-nav__link">
    <span class="md-ellipsis">
      NumericalCalibrator
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NumericalCalibrator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.apply_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      apply_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.assert_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      assert_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.keypoints_inputs" class="md-nav__link">
    <span class="md-ellipsis">
      keypoints_inputs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.keypoints_outputs" class="md-nav__link">
    <span class="md-ellipsis">
      keypoints_outputs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../models/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    utils
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../classifier/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    classifier
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../constrained_module/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    constrained_module
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../datasets/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    datasets
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../enums/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    enums
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../feature_config/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    feature_config
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model_configs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    model_configs
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../plots/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    plots
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator" class="md-nav__link">
    <span class="md-ellipsis">
      CategoricalCalibrator
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CategoricalCalibrator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.apply_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      apply_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.assert_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      assert_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.keypoints_inputs" class="md-nav__link">
    <span class="md-ellipsis">
      keypoints_inputs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.CategoricalCalibrator.keypoints_outputs" class="md-nav__link">
    <span class="md-ellipsis">
      keypoints_outputs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice" class="md-nav__link">
    <span class="md-ellipsis">
      Lattice
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lattice">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice.apply_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      apply_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice.assert_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      assert_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Lattice.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear" class="md-nav__link">
    <span class="md-ellipsis">
      Linear
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear.apply_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      apply_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear.assert_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      assert_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.Linear.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator" class="md-nav__link">
    <span class="md-ellipsis">
      NumericalCalibrator
    </span>
  </a>
  
    <nav class="md-nav" aria-label="NumericalCalibrator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.apply_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      apply_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.assert_constraints" class="md-nav__link">
    <span class="md-ellipsis">
      assert_constraints
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.forward" class="md-nav__link">
    <span class="md-ellipsis">
      forward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.keypoints_inputs" class="md-nav__link">
    <span class="md-ellipsis">
      keypoints_inputs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch_lattice.layers.NumericalCalibrator.keypoints_outputs" class="md-nav__link">
    <span class="md-ellipsis">
      keypoints_outputs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="layers">layers</h1>


<div class="doc doc-object doc-class">



<h2 id="pytorch_lattice.layers.CategoricalCalibrator" class="doc doc-heading">
          <code>pytorch_lattice.layers.CategoricalCalibrator</code>


</h2>


  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pytorch_lattice.constrained_module.ConstrainedModule" href="../constrained_module/#pytorch_lattice.constrained_module.ConstrainedModule">ConstrainedModule</a></code></p>

  
      <p>A categorical calibrator.</p>
<p>This module takes an input of shape <code>(batch_size, 1)</code> and calibrates it by mapping a
given category to its learned output value. The output will have the same shape as
the input.</p>



  <p><span class="doc-section-title">Attributes:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.CategoricalCalibrator.All">All</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>__init__</code> arguments.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.CategoricalCalibrator.kernel">kernel</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>torch.nn.Parameter</code> that stores the categorical mapping weights.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>
      <p>Example:
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># shape: (batch_size, 1)</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="n">calibrator</span> <span class="o">=</span> <span class="n">CategoricalCalibrator</span><span class="p">(</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">num_categories</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">missing_input_value</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">output_min</span><span class="o">=</span><span class="mf">0.0</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">output_max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">monotonicity_pairs</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">kernel_init</span><span class="o">=</span><span class="n">CateegoricalCalibratorInit</span><span class="o">.</span><span class="n">UNIFORM</span><span class="p">,</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="p">)</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">calibrator</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</code></pre></div></p>

            <details class="quote">
              <summary>Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-0-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-0-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-0-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-0-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-0-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-0-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-0-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-0-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-0-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-0-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-0-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-0-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-0-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-0-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-0-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-0-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-0-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-0-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-0-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-0-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-0-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-0-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-0-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-0-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-0-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-0-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-0-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-0-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-0-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-0-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-0-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-0-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-0-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-0-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-0-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-0-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-0-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-0-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-0-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-0-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-0-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-0-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-0-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-0-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-0-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-0-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-0-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-0-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-0-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-0-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-0-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-0-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-0-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-0-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-0-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-0-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-0-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-0-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-0-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-0-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-0-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-0-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-0-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-0-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-0-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-0-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-0-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-0-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-0-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-0-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-0-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-0-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-0-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-0-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-0-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span>
<span class="normal"><a href="#__codelineno-0-179">179</a></span>
<span class="normal"><a href="#__codelineno-0-180">180</a></span>
<span class="normal"><a href="#__codelineno-0-181">181</a></span>
<span class="normal"><a href="#__codelineno-0-182">182</a></span>
<span class="normal"><a href="#__codelineno-0-183">183</a></span>
<span class="normal"><a href="#__codelineno-0-184">184</a></span>
<span class="normal"><a href="#__codelineno-0-185">185</a></span>
<span class="normal"><a href="#__codelineno-0-186">186</a></span>
<span class="normal"><a href="#__codelineno-0-187">187</a></span>
<span class="normal"><a href="#__codelineno-0-188">188</a></span>
<span class="normal"><a href="#__codelineno-0-189">189</a></span>
<span class="normal"><a href="#__codelineno-0-190">190</a></span>
<span class="normal"><a href="#__codelineno-0-191">191</a></span>
<span class="normal"><a href="#__codelineno-0-192">192</a></span>
<span class="normal"><a href="#__codelineno-0-193">193</a></span>
<span class="normal"><a href="#__codelineno-0-194">194</a></span>
<span class="normal"><a href="#__codelineno-0-195">195</a></span>
<span class="normal"><a href="#__codelineno-0-196">196</a></span>
<span class="normal"><a href="#__codelineno-0-197">197</a></span>
<span class="normal"><a href="#__codelineno-0-198">198</a></span>
<span class="normal"><a href="#__codelineno-0-199">199</a></span>
<span class="normal"><a href="#__codelineno-0-200">200</a></span>
<span class="normal"><a href="#__codelineno-0-201">201</a></span>
<span class="normal"><a href="#__codelineno-0-202">202</a></span>
<span class="normal"><a href="#__codelineno-0-203">203</a></span>
<span class="normal"><a href="#__codelineno-0-204">204</a></span>
<span class="normal"><a href="#__codelineno-0-205">205</a></span>
<span class="normal"><a href="#__codelineno-0-206">206</a></span>
<span class="normal"><a href="#__codelineno-0-207">207</a></span>
<span class="normal"><a href="#__codelineno-0-208">208</a></span>
<span class="normal"><a href="#__codelineno-0-209">209</a></span>
<span class="normal"><a href="#__codelineno-0-210">210</a></span>
<span class="normal"><a href="#__codelineno-0-211">211</a></span>
<span class="normal"><a href="#__codelineno-0-212">212</a></span>
<span class="normal"><a href="#__codelineno-0-213">213</a></span>
<span class="normal"><a href="#__codelineno-0-214">214</a></span>
<span class="normal"><a href="#__codelineno-0-215">215</a></span>
<span class="normal"><a href="#__codelineno-0-216">216</a></span>
<span class="normal"><a href="#__codelineno-0-217">217</a></span>
<span class="normal"><a href="#__codelineno-0-218">218</a></span>
<span class="normal"><a href="#__codelineno-0-219">219</a></span>
<span class="normal"><a href="#__codelineno-0-220">220</a></span>
<span class="normal"><a href="#__codelineno-0-221">221</a></span>
<span class="normal"><a href="#__codelineno-0-222">222</a></span>
<span class="normal"><a href="#__codelineno-0-223">223</a></span>
<span class="normal"><a href="#__codelineno-0-224">224</a></span>
<span class="normal"><a href="#__codelineno-0-225">225</a></span>
<span class="normal"><a href="#__codelineno-0-226">226</a></span>
<span class="normal"><a href="#__codelineno-0-227">227</a></span>
<span class="normal"><a href="#__codelineno-0-228">228</a></span>
<span class="normal"><a href="#__codelineno-0-229">229</a></span>
<span class="normal"><a href="#__codelineno-0-230">230</a></span>
<span class="normal"><a href="#__codelineno-0-231">231</a></span>
<span class="normal"><a href="#__codelineno-0-232">232</a></span>
<span class="normal"><a href="#__codelineno-0-233">233</a></span>
<span class="normal"><a href="#__codelineno-0-234">234</a></span>
<span class="normal"><a href="#__codelineno-0-235">235</a></span>
<span class="normal"><a href="#__codelineno-0-236">236</a></span>
<span class="normal"><a href="#__codelineno-0-237">237</a></span>
<span class="normal"><a href="#__codelineno-0-238">238</a></span>
<span class="normal"><a href="#__codelineno-0-239">239</a></span>
<span class="normal"><a href="#__codelineno-0-240">240</a></span>
<span class="normal"><a href="#__codelineno-0-241">241</a></span>
<span class="normal"><a href="#__codelineno-0-242">242</a></span>
<span class="normal"><a href="#__codelineno-0-243">243</a></span>
<span class="normal"><a href="#__codelineno-0-244">244</a></span>
<span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span>
<span class="normal"><a href="#__codelineno-0-265">265</a></span>
<span class="normal"><a href="#__codelineno-0-266">266</a></span>
<span class="normal"><a href="#__codelineno-0-267">267</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-17" name="__codelineno-0-17"></a><span class="k">class</span> <span class="nc">CategoricalCalibrator</span><span class="p">(</span><span class="n">ConstrainedModule</span><span class="p">):</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;A categorical calibrator.</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a><span class="sd">    This module takes an input of shape `(batch_size, 1)` and calibrates it by mapping a</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a><span class="sd">    given category to its learned output value. The output will have the same shape as</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a><span class="sd">    the input.</span>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a><span class="sd">    Attributes:</span>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a><span class="sd">        All: `__init__` arguments.</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a><span class="sd">        kernel: `torch.nn.Parameter` that stores the categorical mapping weights.</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a><span class="sd">    Example:</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a><span class="sd">    ```python</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a><span class="sd">    inputs = torch.tensor(...)  # shape: (batch_size, 1)</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a><span class="sd">    calibrator = CategoricalCalibrator(</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a><span class="sd">        num_categories=5,</span>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="sd">        missing_input_value=-1,</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a><span class="sd">        output_min=0.0</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a><span class="sd">        output_max=1.0,</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a><span class="sd">        monotonicity_pairs=[(0, 1), (1, 2)],</span>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="sd">        kernel_init=CateegoricalCalibratorInit.UNIFORM,</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a><span class="sd">    )</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a><span class="sd">    outputs = calibrator(inputs)</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a><span class="sd">    ```</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>        <span class="n">num_categories</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>        <span class="n">missing_input_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>        <span class="n">output_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>        <span class="n">output_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>        <span class="n">monotonicity_pairs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>        <span class="n">kernel_init</span><span class="p">:</span> <span class="n">CategoricalCalibratorInit</span> <span class="o">=</span> <span class="n">CategoricalCalibratorInit</span><span class="o">.</span><span class="n">UNIFORM</span><span class="p">,</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an instance of `CategoricalCalibrator`.</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a><span class="sd">            num_categories: The number of known categories.</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a><span class="sd">            missing_input_value: If provided, the calibrator will learn to map all</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">                instances of this missing input value to a learned output value just</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">                the same as it does for known categories. Note that `num_categories`</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">                will be one greater to include this missing category.</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a><span class="sd">            output_min: Minimum output value. If `None`, the minimum output value will</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a><span class="sd">                be unbounded.</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a><span class="sd">            output_max: Maximum output value. If `None`, the maximum output value will</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a><span class="sd">                be unbounded.</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a><span class="sd">            monotonicity_pairs: List of pairs of indices `(i,j)` indicating that the</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a><span class="sd">                calibrator output for index `j` should be greater than or equal to that</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a><span class="sd">                of index `i`.</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">            kernel_init: Initialization scheme to use for the kernel.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a><span class="sd">        Raises:</span>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a><span class="sd">            ValueError: If `monotonicity_pairs` is cyclic.</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a><span class="sd">            ValueError: If `kernel_init` is invalid.</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>            <span class="n">num_categories</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">missing_input_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_categories</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>        <span class="p">)</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="o">=</span> <span class="n">missing_input_value</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">=</span> <span class="n">output_min</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">=</span> <span class="n">output_max</span>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity_pairs</span> <span class="o">=</span> <span class="n">monotonicity_pairs</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>        <span class="k">if</span> <span class="n">monotonicity_pairs</span><span class="p">:</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_monotonicity_graph</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_reverse_monotonicity_graph</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">monotonicity_pairs</span><span class="p">:</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_monotonicity_graph</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_reverse_monotonicity_graph</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>            <span class="k">try</span><span class="p">:</span>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_monotonically_sorted_indices</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>                    <span class="o">*</span><span class="n">TopologicalSorter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reverse_monotonicity_graph</span><span class="p">)</span><span class="o">.</span><span class="n">static_order</span><span class="p">()</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a>                <span class="p">]</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>            <span class="k">except</span> <span class="n">CycleError</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;monotonicity_pairs is cyclic&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">exc</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">=</span> <span class="n">kernel_init</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>        <span class="k">if</span> <span class="n">kernel_init</span> <span class="o">==</span> <span class="n">CategoricalCalibratorInit</span><span class="o">.</span><span class="n">CONSTANT</span><span class="p">:</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>            <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>                <span class="n">init_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_min</span> <span class="o">+</span> <span class="n">output_max</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>            <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>                <span class="n">init_value</span> <span class="o">=</span> <span class="n">output_min</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>            <span class="k">elif</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a>                <span class="n">init_value</span> <span class="o">=</span> <span class="n">output_max</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>                <span class="n">init_value</span> <span class="o">=</span> <span class="mf">0.0</span>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">init_value</span><span class="p">)</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>        <span class="k">elif</span> <span class="n">kernel_init</span> <span class="o">==</span> <span class="n">CategoricalCalibratorInit</span><span class="o">.</span><span class="n">UNIFORM</span><span class="p">:</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>            <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>                <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>            <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a>                <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">output_max</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a>            <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>                <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_min</span> <span class="o">+</span> <span class="mf">0.05</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a>                <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown kernel init: </span><span class="si">{</span><span class="n">kernel_init</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Calibrates categorical inputs through a learned mapping.</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a><span class="sd">            x: The input tensor of category indices of shape `(batch_size, 1)`.</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a><span class="sd">            torch.Tensor of shape `(batch_size, 1)` containing calibrated input values.</span>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>            <span class="n">missing_category_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span><span class="p">,</span> <span class="n">missing_category_tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>        <span class="c1"># TODO: test if using torch.gather is faster than one-hot matmul.</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>        <span class="n">one_hot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a>        <span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">one_hot</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a>    <span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Projects kernel into desired constraints.&quot;&quot;&quot;</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a>        <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity_pairs</span><span class="p">:</span>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a>            <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_approximately_project_monotonicity_pairs</span><span class="p">(</span>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a>                <span class="n">projected_kernel_data</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a>            <span class="p">)</span>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a>            <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a>                <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_min</span><span class="p">)</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a>            <span class="p">)</span>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a>            <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a>                <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_max</span><span class="p">)</span>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>            <span class="p">)</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">projected_kernel_data</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a>    <span class="k">def</span> <span class="nf">assert_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Asserts that layer satisfies specified constraints.</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a><span class="sd">        This checks that weights at the indexes of monotonicity pairs are in the correct</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a><span class="sd">        order and that the output is within bounds.</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">            eps: the margin of error allowed</span>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a><span class="sd">            A list of messages describing violated constraints including violated</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a><span class="sd">            monotonicity pairs. If no constraints  violated, the list will be empty.</span>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a>        <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">+</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Max weight greater than output_max.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">-</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Min weight less than output_min.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity_pairs</span><span class="p">:</span>
<a id="__codelineno-0-179" name="__codelineno-0-179"></a>            <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-180" name="__codelineno-0-180"></a>                <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
<a id="__codelineno-0-181" name="__codelineno-0-181"></a>                <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity_pairs</span>
<a id="__codelineno-0-182" name="__codelineno-0-182"></a>                <span class="k">if</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">eps</span>
<a id="__codelineno-0-183" name="__codelineno-0-183"></a>            <span class="p">]</span>
<a id="__codelineno-0-184" name="__codelineno-0-184"></a>            <span class="k">if</span> <span class="n">violation_indices</span><span class="p">:</span>
<a id="__codelineno-0-185" name="__codelineno-0-185"></a>                <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Monotonicity violated at: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">violation_indices</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-186" name="__codelineno-0-186"></a>
<a id="__codelineno-0-187" name="__codelineno-0-187"></a>        <span class="k">return</span> <span class="n">messages</span>
<a id="__codelineno-0-188" name="__codelineno-0-188"></a>
<a id="__codelineno-0-189" name="__codelineno-0-189"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-190" name="__codelineno-0-190"></a>    <span class="k">def</span> <span class="nf">keypoints_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-191" name="__codelineno-0-191"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a tensor of keypoint inputs (category indices).&quot;&quot;&quot;</span>
<a id="__codelineno-0-192" name="__codelineno-0-192"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-193" name="__codelineno-0-193"></a>            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-194" name="__codelineno-0-194"></a>                <span class="p">(</span>
<a id="__codelineno-0-195" name="__codelineno-0-195"></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-0-196" name="__codelineno-0-196"></a>                    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span><span class="p">]),</span>
<a id="__codelineno-0-197" name="__codelineno-0-197"></a>                <span class="p">),</span>
<a id="__codelineno-0-198" name="__codelineno-0-198"></a>                <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-199" name="__codelineno-0-199"></a>            <span class="p">)</span>
<a id="__codelineno-0-200" name="__codelineno-0-200"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span><span class="p">)</span>
<a id="__codelineno-0-201" name="__codelineno-0-201"></a>
<a id="__codelineno-0-202" name="__codelineno-0-202"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-203" name="__codelineno-0-203"></a>    <span class="k">def</span> <span class="nf">keypoints_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-204" name="__codelineno-0-204"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a tensor of keypoint outputs.&quot;&quot;&quot;</span>
<a id="__codelineno-0-205" name="__codelineno-0-205"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-206" name="__codelineno-0-206"></a>
<a id="__codelineno-0-207" name="__codelineno-0-207"></a>    <span class="c1">################################################################################</span>
<a id="__codelineno-0-208" name="__codelineno-0-208"></a>    <span class="c1">############################## PRIVATE METHODS #################################</span>
<a id="__codelineno-0-209" name="__codelineno-0-209"></a>    <span class="c1">################################################################################</span>
<a id="__codelineno-0-210" name="__codelineno-0-210"></a>
<a id="__codelineno-0-211" name="__codelineno-0-211"></a>    <span class="k">def</span> <span class="nf">_approximately_project_monotonicity_pairs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_data</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-212" name="__codelineno-0-212"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Projects kernel such that the monotonicity pairs are satisfied.</span>
<a id="__codelineno-0-213" name="__codelineno-0-213"></a>
<a id="__codelineno-0-214" name="__codelineno-0-214"></a><span class="sd">        The kernel will be projected such that `kernel_data[i] &lt;= kernel_data[j]`. This</span>
<a id="__codelineno-0-215" name="__codelineno-0-215"></a><span class="sd">        results in calibrated outputs that adhere to the desired constraints.</span>
<a id="__codelineno-0-216" name="__codelineno-0-216"></a>
<a id="__codelineno-0-217" name="__codelineno-0-217"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-218" name="__codelineno-0-218"></a><span class="sd">            kernel_data: The tensor of shape `(self.num_categories, 1)` to be projected</span>
<a id="__codelineno-0-219" name="__codelineno-0-219"></a><span class="sd">                into the constraints specified by `self.monotonicity pairs`.</span>
<a id="__codelineno-0-220" name="__codelineno-0-220"></a>
<a id="__codelineno-0-221" name="__codelineno-0-221"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-222" name="__codelineno-0-222"></a><span class="sd">            Projected kernel data. To prevent the kernel from drifting in one direction,</span>
<a id="__codelineno-0-223" name="__codelineno-0-223"></a><span class="sd">            the data returned is the average of the min/max and max/min projections.</span>
<a id="__codelineno-0-224" name="__codelineno-0-224"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-225" name="__codelineno-0-225"></a>        <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">kernel_data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-0-226" name="__codelineno-0-226"></a>
<a id="__codelineno-0-227" name="__codelineno-0-227"></a>        <span class="k">def</span> <span class="nf">project</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">monotonicity_graph</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">minimum</span><span class="p">):</span>
<a id="__codelineno-0-228" name="__codelineno-0-228"></a>            <span class="n">projected_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-229" name="__codelineno-0-229"></a>            <span class="n">sorted_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_monotonically_sorted_indices</span>
<a id="__codelineno-0-230" name="__codelineno-0-230"></a>            <span class="k">if</span> <span class="n">minimum</span><span class="p">:</span>
<a id="__codelineno-0-231" name="__codelineno-0-231"></a>                <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">sorted_indices</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-0-232" name="__codelineno-0-232"></a>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sorted_indices</span><span class="p">:</span>
<a id="__codelineno-0-233" name="__codelineno-0-233"></a>                <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">monotonicity_graph</span><span class="p">:</span>
<a id="__codelineno-0-234" name="__codelineno-0-234"></a>                    <span class="n">projection</span> <span class="o">=</span> <span class="n">projected_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-0-235" name="__codelineno-0-235"></a>                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">monotonicity_graph</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
<a id="__codelineno-0-236" name="__codelineno-0-236"></a>                        <span class="k">if</span> <span class="n">minimum</span><span class="p">:</span>
<a id="__codelineno-0-237" name="__codelineno-0-237"></a>                            <span class="n">projection</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">projection</span><span class="p">,</span> <span class="n">projected_data</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
<a id="__codelineno-0-238" name="__codelineno-0-238"></a>                        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-239" name="__codelineno-0-239"></a>                            <span class="n">projection</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">projection</span><span class="p">,</span> <span class="n">projected_data</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
<a id="__codelineno-0-240" name="__codelineno-0-240"></a>                        <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
<a id="__codelineno-0-241" name="__codelineno-0-241"></a>                            <span class="n">projected_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">projection</span>
<a id="__codelineno-0-242" name="__codelineno-0-242"></a>                        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-243" name="__codelineno-0-243"></a>                            <span class="n">projected_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-244" name="__codelineno-0-244"></a>                                <span class="n">step</span> <span class="o">*</span> <span class="n">projection</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">step</span><span class="p">)</span> <span class="o">*</span> <span class="n">projected_data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<a id="__codelineno-0-245" name="__codelineno-0-245"></a>                            <span class="p">)</span>
<a id="__codelineno-0-246" name="__codelineno-0-246"></a>            <span class="k">return</span> <span class="n">projected_data</span>
<a id="__codelineno-0-247" name="__codelineno-0-247"></a>
<a id="__codelineno-0-248" name="__codelineno-0-248"></a>        <span class="n">projected_kernel_min_max</span> <span class="o">=</span> <span class="n">project</span><span class="p">(</span>
<a id="__codelineno-0-249" name="__codelineno-0-249"></a>            <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_monotonicity_graph</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="kc">True</span>
<a id="__codelineno-0-250" name="__codelineno-0-250"></a>        <span class="p">)</span>
<a id="__codelineno-0-251" name="__codelineno-0-251"></a>        <span class="n">projected_kernel_min_max</span> <span class="o">=</span> <span class="n">project</span><span class="p">(</span>
<a id="__codelineno-0-252" name="__codelineno-0-252"></a>            <span class="n">projected_kernel_min_max</span><span class="p">,</span>
<a id="__codelineno-0-253" name="__codelineno-0-253"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_reverse_monotonicity_graph</span><span class="p">,</span>
<a id="__codelineno-0-254" name="__codelineno-0-254"></a>            <span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-0-255" name="__codelineno-0-255"></a>            <span class="n">minimum</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-256" name="__codelineno-0-256"></a>        <span class="p">)</span>
<a id="__codelineno-0-257" name="__codelineno-0-257"></a>        <span class="n">projected_kernel_min_max</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">projected_kernel_min_max</span><span class="p">)</span>
<a id="__codelineno-0-258" name="__codelineno-0-258"></a>
<a id="__codelineno-0-259" name="__codelineno-0-259"></a>        <span class="n">projected_kernel_max_min</span> <span class="o">=</span> <span class="n">project</span><span class="p">(</span>
<a id="__codelineno-0-260" name="__codelineno-0-260"></a>            <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reverse_monotonicity_graph</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="kc">False</span>
<a id="__codelineno-0-261" name="__codelineno-0-261"></a>        <span class="p">)</span>
<a id="__codelineno-0-262" name="__codelineno-0-262"></a>        <span class="n">projected_kernel_max_min</span> <span class="o">=</span> <span class="n">project</span><span class="p">(</span>
<a id="__codelineno-0-263" name="__codelineno-0-263"></a>            <span class="n">projected_kernel_max_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_monotonicity_graph</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="kc">True</span>
<a id="__codelineno-0-264" name="__codelineno-0-264"></a>        <span class="p">)</span>
<a id="__codelineno-0-265" name="__codelineno-0-265"></a>        <span class="n">projected_kernel_max_min</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">projected_kernel_max_min</span><span class="p">)</span>
<a id="__codelineno-0-266" name="__codelineno-0-266"></a>
<a id="__codelineno-0-267" name="__codelineno-0-267"></a>        <span class="k">return</span> <span class="p">(</span><span class="n">projected_kernel_min_max</span> <span class="o">+</span> <span class="n">projected_kernel_max_min</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.CategoricalCalibrator.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">num_categories</span><span class="p">,</span> <span class="n">missing_input_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">monotonicity_pairs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_init</span><span class="o">=</span><span class="n">CategoricalCalibratorInit</span><span class="o">.</span><span class="n">UNIFORM</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initializes an instance of <code>CategoricalCalibrator</code>.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>num_categories</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of known categories.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>missing_input_value</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If provided, the calibrator will learn to map all
instances of this missing input value to a learned output value just
the same as it does for known categories. Note that <code>num_categories</code>
will be one greater to include this missing category.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>output_min</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Minimum output value. If <code>None</code>, the minimum output value will
be unbounded.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>output_max</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum output value. If <code>None</code>, the maximum output value will
be unbounded.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>monotonicity_pairs</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[list[tuple[int, int]]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>List of pairs of indices <code>(i,j)</code> indicating that the
calibrator output for index <code>j</code> should be greater than or equal to that
of index <code>i</code>.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>kernel_init</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="pytorch_lattice.enums.CategoricalCalibratorInit" href="../enums/#pytorch_lattice.enums.CategoricalCalibratorInit">CategoricalCalibratorInit</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Initialization scheme to use for the kernel.</p>
            </div>
          </td>
          <td>
                <code><span title="pytorch_lattice.enums.CategoricalCalibratorInit.UNIFORM">UNIFORM</span></code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Raises:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>monotonicity_pairs</code> is cyclic.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>kernel_init</code> is invalid.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-0-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-0-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-0-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-0-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-0-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-0-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-0-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-0-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-0-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-0-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-0-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-0-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-0-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-0-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-0-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-0-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-0-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-0-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-0-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-0-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-0-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-0-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-0-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-0-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-0-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-0-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-0-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-0-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-0-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-0-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-0-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-0-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-0-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-0-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-0-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-0-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-0-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-0-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-0-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-0-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-0-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-0-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-0-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-0-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-0-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-0-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-0-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-0-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-0-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-43" name="__codelineno-0-43"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>    <span class="n">num_categories</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>    <span class="n">missing_input_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>    <span class="n">output_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>    <span class="n">output_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>    <span class="n">monotonicity_pairs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>    <span class="n">kernel_init</span><span class="p">:</span> <span class="n">CategoricalCalibratorInit</span> <span class="o">=</span> <span class="n">CategoricalCalibratorInit</span><span class="o">.</span><span class="n">UNIFORM</span><span class="p">,</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Initializes an instance of `CategoricalCalibrator`.</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a><span class="sd">        num_categories: The number of known categories.</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a><span class="sd">        missing_input_value: If provided, the calibrator will learn to map all</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">            instances of this missing input value to a learned output value just</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">            the same as it does for known categories. Note that `num_categories`</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">            will be one greater to include this missing category.</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a><span class="sd">        output_min: Minimum output value. If `None`, the minimum output value will</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a><span class="sd">            be unbounded.</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a><span class="sd">        output_max: Maximum output value. If `None`, the maximum output value will</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a><span class="sd">            be unbounded.</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a><span class="sd">        monotonicity_pairs: List of pairs of indices `(i,j)` indicating that the</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a><span class="sd">            calibrator output for index `j` should be greater than or equal to that</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a><span class="sd">            of index `i`.</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">        kernel_init: Initialization scheme to use for the kernel.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a><span class="sd">    Raises:</span>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a><span class="sd">        ValueError: If `monotonicity_pairs` is cyclic.</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a><span class="sd">        ValueError: If `kernel_init` is invalid.</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>        <span class="n">num_categories</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">missing_input_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">num_categories</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>    <span class="p">)</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="o">=</span> <span class="n">missing_input_value</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">=</span> <span class="n">output_min</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">=</span> <span class="n">output_max</span>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity_pairs</span> <span class="o">=</span> <span class="n">monotonicity_pairs</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>    <span class="k">if</span> <span class="n">monotonicity_pairs</span><span class="p">:</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_monotonicity_graph</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_reverse_monotonicity_graph</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">monotonicity_pairs</span><span class="p">:</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_monotonicity_graph</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_reverse_monotonicity_graph</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>        <span class="k">try</span><span class="p">:</span>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_monotonically_sorted_indices</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>                <span class="o">*</span><span class="n">TopologicalSorter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_reverse_monotonicity_graph</span><span class="p">)</span><span class="o">.</span><span class="n">static_order</span><span class="p">()</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a>            <span class="p">]</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>        <span class="k">except</span> <span class="n">CycleError</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;monotonicity_pairs is cyclic&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="nn">exc</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">=</span> <span class="n">kernel_init</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>    <span class="k">if</span> <span class="n">kernel_init</span> <span class="o">==</span> <span class="n">CategoricalCalibratorInit</span><span class="o">.</span><span class="n">CONSTANT</span><span class="p">:</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>        <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>            <span class="n">init_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">output_min</span> <span class="o">+</span> <span class="n">output_max</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>        <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>            <span class="n">init_value</span> <span class="o">=</span> <span class="n">output_min</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>        <span class="k">elif</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a>            <span class="n">init_value</span> <span class="o">=</span> <span class="n">output_max</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>            <span class="n">init_value</span> <span class="o">=</span> <span class="mf">0.0</span>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">init_value</span><span class="p">)</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>    <span class="k">elif</span> <span class="n">kernel_init</span> <span class="o">==</span> <span class="n">CategoricalCalibratorInit</span><span class="o">.</span><span class="n">UNIFORM</span><span class="p">:</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>        <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>            <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>        <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a>            <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">output_max</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a>        <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>            <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_min</span> <span class="o">+</span> <span class="mf">0.05</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a>            <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown kernel init: </span><span class="si">{</span><span class="n">kernel_init</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.CategoricalCalibrator.apply_constraints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">apply_constraints</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Projects kernel into desired constraints.</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-138" name="__codelineno-0-138"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a><span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Projects kernel into desired constraints.&quot;&quot;&quot;</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a>    <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity_pairs</span><span class="p">:</span>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a>        <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_approximately_project_monotonicity_pairs</span><span class="p">(</span>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a>            <span class="n">projected_kernel_data</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a>        <span class="p">)</span>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a>        <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a>            <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_min</span><span class="p">)</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a>        <span class="p">)</span>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a>        <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a>            <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_max</span><span class="p">)</span>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>        <span class="p">)</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">projected_kernel_data</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.CategoricalCalibrator.assert_constraints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">assert_constraints</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Asserts that layer satisfies specified constraints.</p>
<p>This checks that weights at the indexes of monotonicity pairs are in the correct
order and that the output is within bounds.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>eps</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>the margin of error allowed</p>
            </div>
          </td>
          <td>
                <code>1e-06</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A list of messages describing violated constraints including violated</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>monotonicity pairs. If no constraints  violated, the list will be empty.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span>
<span class="normal"><a href="#__codelineno-0-179">179</a></span>
<span class="normal"><a href="#__codelineno-0-180">180</a></span>
<span class="normal"><a href="#__codelineno-0-181">181</a></span>
<span class="normal"><a href="#__codelineno-0-182">182</a></span>
<span class="normal"><a href="#__codelineno-0-183">183</a></span>
<span class="normal"><a href="#__codelineno-0-184">184</a></span>
<span class="normal"><a href="#__codelineno-0-185">185</a></span>
<span class="normal"><a href="#__codelineno-0-186">186</a></span>
<span class="normal"><a href="#__codelineno-0-187">187</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-156" name="__codelineno-0-156"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a><span class="k">def</span> <span class="nf">assert_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Asserts that layer satisfies specified constraints.</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a><span class="sd">    This checks that weights at the indexes of monotonicity pairs are in the correct</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a><span class="sd">    order and that the output is within bounds.</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">        eps: the margin of error allowed</span>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a><span class="sd">        A list of messages describing violated constraints including violated</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a><span class="sd">        monotonicity pairs. If no constraints  violated, the list will be empty.</span>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a>    <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">+</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Max weight greater than output_max.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">-</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Min weight less than output_min.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity_pairs</span><span class="p">:</span>
<a id="__codelineno-0-179" name="__codelineno-0-179"></a>        <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-180" name="__codelineno-0-180"></a>            <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
<a id="__codelineno-0-181" name="__codelineno-0-181"></a>            <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity_pairs</span>
<a id="__codelineno-0-182" name="__codelineno-0-182"></a>            <span class="k">if</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">eps</span>
<a id="__codelineno-0-183" name="__codelineno-0-183"></a>        <span class="p">]</span>
<a id="__codelineno-0-184" name="__codelineno-0-184"></a>        <span class="k">if</span> <span class="n">violation_indices</span><span class="p">:</span>
<a id="__codelineno-0-185" name="__codelineno-0-185"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Monotonicity violated at: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">violation_indices</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-186" name="__codelineno-0-186"></a>
<a id="__codelineno-0-187" name="__codelineno-0-187"></a>    <span class="k">return</span> <span class="n">messages</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.CategoricalCalibrator.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Calibrates categorical inputs through a learned mapping.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>x</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The input tensor of category indices of shape <code>(batch_size, 1)</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>torch.Tensor of shape <code>(batch_size, 1)</code> containing calibrated input values.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-120" name="__codelineno-0-120"></a><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Calibrates categorical inputs through a learned mapping.</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a><span class="sd">        x: The input tensor of category indices of shape `(batch_size, 1)`.</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a><span class="sd">        torch.Tensor of shape `(batch_size, 1)` containing calibrated input values.</span>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>        <span class="n">missing_category_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span><span class="p">,</span> <span class="n">missing_category_tensor</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>    <span class="c1"># TODO: test if using torch.gather is faster than one-hot matmul.</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>    <span class="n">one_hot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a>    <span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">one_hot</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.CategoricalCalibrator.keypoints_inputs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">keypoints_inputs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns a tensor of keypoint inputs (category indices).</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-189">189</a></span>
<span class="normal"><a href="#__codelineno-0-190">190</a></span>
<span class="normal"><a href="#__codelineno-0-191">191</a></span>
<span class="normal"><a href="#__codelineno-0-192">192</a></span>
<span class="normal"><a href="#__codelineno-0-193">193</a></span>
<span class="normal"><a href="#__codelineno-0-194">194</a></span>
<span class="normal"><a href="#__codelineno-0-195">195</a></span>
<span class="normal"><a href="#__codelineno-0-196">196</a></span>
<span class="normal"><a href="#__codelineno-0-197">197</a></span>
<span class="normal"><a href="#__codelineno-0-198">198</a></span>
<span class="normal"><a href="#__codelineno-0-199">199</a></span>
<span class="normal"><a href="#__codelineno-0-200">200</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-189" name="__codelineno-0-189"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-190" name="__codelineno-0-190"></a><span class="k">def</span> <span class="nf">keypoints_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-191" name="__codelineno-0-191"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a tensor of keypoint inputs (category indices).&quot;&quot;&quot;</span>
<a id="__codelineno-0-192" name="__codelineno-0-192"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-193" name="__codelineno-0-193"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-194" name="__codelineno-0-194"></a>            <span class="p">(</span>
<a id="__codelineno-0-195" name="__codelineno-0-195"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
<a id="__codelineno-0-196" name="__codelineno-0-196"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span><span class="p">]),</span>
<a id="__codelineno-0-197" name="__codelineno-0-197"></a>            <span class="p">),</span>
<a id="__codelineno-0-198" name="__codelineno-0-198"></a>            <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-199" name="__codelineno-0-199"></a>        <span class="p">)</span>
<a id="__codelineno-0-200" name="__codelineno-0-200"></a>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_categories</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.CategoricalCalibrator.keypoints_outputs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">keypoints_outputs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns a tensor of keypoint outputs.</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-202">202</a></span>
<span class="normal"><a href="#__codelineno-0-203">203</a></span>
<span class="normal"><a href="#__codelineno-0-204">204</a></span>
<span class="normal"><a href="#__codelineno-0-205">205</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-202" name="__codelineno-0-202"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-203" name="__codelineno-0-203"></a><span class="k">def</span> <span class="nf">keypoints_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-204" name="__codelineno-0-204"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns a tensor of keypoint outputs.&quot;&quot;&quot;</span>
<a id="__codelineno-0-205" name="__codelineno-0-205"></a>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_lattice.layers.Lattice" class="doc doc-heading">
          <code>pytorch_lattice.layers.Lattice</code>


</h2>


  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pytorch_lattice.constrained_module.ConstrainedModule" href="../constrained_module/#pytorch_lattice.constrained_module.ConstrainedModule">ConstrainedModule</a></code></p>

  
      <p>A Lattice Module.</p>
<p>Layer performs interpolation using one of 'units' d-dimensional lattices with
arbitrary number of keypoints per dimension. Each lattice vertex has a trainable
weight, and input is considered to be a d-dimensional point within the lattice.</p>



  <p><span class="doc-section-title">Attributes:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.Lattice.All">All</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>__init__</code> arguments.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.Lattice.kernel">kernel</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>torch.nn.Parameter</code> of shape <code>(prod(lattice_sizes), units)</code> which
stores
weights at each vertex of lattice.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>
      <p>Example:
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">lattice_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="n">inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="c1"># shape: (batch_size, len(lattice_sizes))</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="n">lattice</span><span class="o">=</span><span class="n">Lattice</span><span class="p">(</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">lattice_sizes</span><span class="p">,</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">clip_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">interpolation</span><span class="o">=</span><span class="n">Interpolation</span><span class="o">.</span><span class="n">HYPERCUBE</span><span class="p">,</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="p">)</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">Lattice</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</code></pre></div></p>

            <details class="quote">
              <summary>Source code in <code>pytorch_lattice/layers/lattice.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-16"> 16</a></span>
<span class="normal"><a href="#__codelineno-0-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-0-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-0-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-0-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-0-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-0-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-0-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-0-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-0-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-0-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-0-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-0-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-0-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-0-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-0-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-0-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-0-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-0-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-0-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-0-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-0-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-0-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-0-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-0-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-0-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-0-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-0-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-0-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-0-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-0-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-0-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-0-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-0-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-0-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-0-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-0-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-0-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-0-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-0-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-0-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-0-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-0-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-0-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-0-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-0-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-0-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-0-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-0-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-0-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-0-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-0-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-0-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-0-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-0-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-0-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-0-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-0-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-0-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-0-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-0-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-0-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-0-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-0-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-0-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-0-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-0-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-0-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-0-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-0-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-0-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-0-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-0-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-0-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-0-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-0-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-0-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span>
<span class="normal"><a href="#__codelineno-0-179">179</a></span>
<span class="normal"><a href="#__codelineno-0-180">180</a></span>
<span class="normal"><a href="#__codelineno-0-181">181</a></span>
<span class="normal"><a href="#__codelineno-0-182">182</a></span>
<span class="normal"><a href="#__codelineno-0-183">183</a></span>
<span class="normal"><a href="#__codelineno-0-184">184</a></span>
<span class="normal"><a href="#__codelineno-0-185">185</a></span>
<span class="normal"><a href="#__codelineno-0-186">186</a></span>
<span class="normal"><a href="#__codelineno-0-187">187</a></span>
<span class="normal"><a href="#__codelineno-0-188">188</a></span>
<span class="normal"><a href="#__codelineno-0-189">189</a></span>
<span class="normal"><a href="#__codelineno-0-190">190</a></span>
<span class="normal"><a href="#__codelineno-0-191">191</a></span>
<span class="normal"><a href="#__codelineno-0-192">192</a></span>
<span class="normal"><a href="#__codelineno-0-193">193</a></span>
<span class="normal"><a href="#__codelineno-0-194">194</a></span>
<span class="normal"><a href="#__codelineno-0-195">195</a></span>
<span class="normal"><a href="#__codelineno-0-196">196</a></span>
<span class="normal"><a href="#__codelineno-0-197">197</a></span>
<span class="normal"><a href="#__codelineno-0-198">198</a></span>
<span class="normal"><a href="#__codelineno-0-199">199</a></span>
<span class="normal"><a href="#__codelineno-0-200">200</a></span>
<span class="normal"><a href="#__codelineno-0-201">201</a></span>
<span class="normal"><a href="#__codelineno-0-202">202</a></span>
<span class="normal"><a href="#__codelineno-0-203">203</a></span>
<span class="normal"><a href="#__codelineno-0-204">204</a></span>
<span class="normal"><a href="#__codelineno-0-205">205</a></span>
<span class="normal"><a href="#__codelineno-0-206">206</a></span>
<span class="normal"><a href="#__codelineno-0-207">207</a></span>
<span class="normal"><a href="#__codelineno-0-208">208</a></span>
<span class="normal"><a href="#__codelineno-0-209">209</a></span>
<span class="normal"><a href="#__codelineno-0-210">210</a></span>
<span class="normal"><a href="#__codelineno-0-211">211</a></span>
<span class="normal"><a href="#__codelineno-0-212">212</a></span>
<span class="normal"><a href="#__codelineno-0-213">213</a></span>
<span class="normal"><a href="#__codelineno-0-214">214</a></span>
<span class="normal"><a href="#__codelineno-0-215">215</a></span>
<span class="normal"><a href="#__codelineno-0-216">216</a></span>
<span class="normal"><a href="#__codelineno-0-217">217</a></span>
<span class="normal"><a href="#__codelineno-0-218">218</a></span>
<span class="normal"><a href="#__codelineno-0-219">219</a></span>
<span class="normal"><a href="#__codelineno-0-220">220</a></span>
<span class="normal"><a href="#__codelineno-0-221">221</a></span>
<span class="normal"><a href="#__codelineno-0-222">222</a></span>
<span class="normal"><a href="#__codelineno-0-223">223</a></span>
<span class="normal"><a href="#__codelineno-0-224">224</a></span>
<span class="normal"><a href="#__codelineno-0-225">225</a></span>
<span class="normal"><a href="#__codelineno-0-226">226</a></span>
<span class="normal"><a href="#__codelineno-0-227">227</a></span>
<span class="normal"><a href="#__codelineno-0-228">228</a></span>
<span class="normal"><a href="#__codelineno-0-229">229</a></span>
<span class="normal"><a href="#__codelineno-0-230">230</a></span>
<span class="normal"><a href="#__codelineno-0-231">231</a></span>
<span class="normal"><a href="#__codelineno-0-232">232</a></span>
<span class="normal"><a href="#__codelineno-0-233">233</a></span>
<span class="normal"><a href="#__codelineno-0-234">234</a></span>
<span class="normal"><a href="#__codelineno-0-235">235</a></span>
<span class="normal"><a href="#__codelineno-0-236">236</a></span>
<span class="normal"><a href="#__codelineno-0-237">237</a></span>
<span class="normal"><a href="#__codelineno-0-238">238</a></span>
<span class="normal"><a href="#__codelineno-0-239">239</a></span>
<span class="normal"><a href="#__codelineno-0-240">240</a></span>
<span class="normal"><a href="#__codelineno-0-241">241</a></span>
<span class="normal"><a href="#__codelineno-0-242">242</a></span>
<span class="normal"><a href="#__codelineno-0-243">243</a></span>
<span class="normal"><a href="#__codelineno-0-244">244</a></span>
<span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span>
<span class="normal"><a href="#__codelineno-0-265">265</a></span>
<span class="normal"><a href="#__codelineno-0-266">266</a></span>
<span class="normal"><a href="#__codelineno-0-267">267</a></span>
<span class="normal"><a href="#__codelineno-0-268">268</a></span>
<span class="normal"><a href="#__codelineno-0-269">269</a></span>
<span class="normal"><a href="#__codelineno-0-270">270</a></span>
<span class="normal"><a href="#__codelineno-0-271">271</a></span>
<span class="normal"><a href="#__codelineno-0-272">272</a></span>
<span class="normal"><a href="#__codelineno-0-273">273</a></span>
<span class="normal"><a href="#__codelineno-0-274">274</a></span>
<span class="normal"><a href="#__codelineno-0-275">275</a></span>
<span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span>
<span class="normal"><a href="#__codelineno-0-329">329</a></span>
<span class="normal"><a href="#__codelineno-0-330">330</a></span>
<span class="normal"><a href="#__codelineno-0-331">331</a></span>
<span class="normal"><a href="#__codelineno-0-332">332</a></span>
<span class="normal"><a href="#__codelineno-0-333">333</a></span>
<span class="normal"><a href="#__codelineno-0-334">334</a></span>
<span class="normal"><a href="#__codelineno-0-335">335</a></span>
<span class="normal"><a href="#__codelineno-0-336">336</a></span>
<span class="normal"><a href="#__codelineno-0-337">337</a></span>
<span class="normal"><a href="#__codelineno-0-338">338</a></span>
<span class="normal"><a href="#__codelineno-0-339">339</a></span>
<span class="normal"><a href="#__codelineno-0-340">340</a></span>
<span class="normal"><a href="#__codelineno-0-341">341</a></span>
<span class="normal"><a href="#__codelineno-0-342">342</a></span>
<span class="normal"><a href="#__codelineno-0-343">343</a></span>
<span class="normal"><a href="#__codelineno-0-344">344</a></span>
<span class="normal"><a href="#__codelineno-0-345">345</a></span>
<span class="normal"><a href="#__codelineno-0-346">346</a></span>
<span class="normal"><a href="#__codelineno-0-347">347</a></span>
<span class="normal"><a href="#__codelineno-0-348">348</a></span>
<span class="normal"><a href="#__codelineno-0-349">349</a></span>
<span class="normal"><a href="#__codelineno-0-350">350</a></span>
<span class="normal"><a href="#__codelineno-0-351">351</a></span>
<span class="normal"><a href="#__codelineno-0-352">352</a></span>
<span class="normal"><a href="#__codelineno-0-353">353</a></span>
<span class="normal"><a href="#__codelineno-0-354">354</a></span>
<span class="normal"><a href="#__codelineno-0-355">355</a></span>
<span class="normal"><a href="#__codelineno-0-356">356</a></span>
<span class="normal"><a href="#__codelineno-0-357">357</a></span>
<span class="normal"><a href="#__codelineno-0-358">358</a></span>
<span class="normal"><a href="#__codelineno-0-359">359</a></span>
<span class="normal"><a href="#__codelineno-0-360">360</a></span>
<span class="normal"><a href="#__codelineno-0-361">361</a></span>
<span class="normal"><a href="#__codelineno-0-362">362</a></span>
<span class="normal"><a href="#__codelineno-0-363">363</a></span>
<span class="normal"><a href="#__codelineno-0-364">364</a></span>
<span class="normal"><a href="#__codelineno-0-365">365</a></span>
<span class="normal"><a href="#__codelineno-0-366">366</a></span>
<span class="normal"><a href="#__codelineno-0-367">367</a></span>
<span class="normal"><a href="#__codelineno-0-368">368</a></span>
<span class="normal"><a href="#__codelineno-0-369">369</a></span>
<span class="normal"><a href="#__codelineno-0-370">370</a></span>
<span class="normal"><a href="#__codelineno-0-371">371</a></span>
<span class="normal"><a href="#__codelineno-0-372">372</a></span>
<span class="normal"><a href="#__codelineno-0-373">373</a></span>
<span class="normal"><a href="#__codelineno-0-374">374</a></span>
<span class="normal"><a href="#__codelineno-0-375">375</a></span>
<span class="normal"><a href="#__codelineno-0-376">376</a></span>
<span class="normal"><a href="#__codelineno-0-377">377</a></span>
<span class="normal"><a href="#__codelineno-0-378">378</a></span>
<span class="normal"><a href="#__codelineno-0-379">379</a></span>
<span class="normal"><a href="#__codelineno-0-380">380</a></span>
<span class="normal"><a href="#__codelineno-0-381">381</a></span>
<span class="normal"><a href="#__codelineno-0-382">382</a></span>
<span class="normal"><a href="#__codelineno-0-383">383</a></span>
<span class="normal"><a href="#__codelineno-0-384">384</a></span>
<span class="normal"><a href="#__codelineno-0-385">385</a></span>
<span class="normal"><a href="#__codelineno-0-386">386</a></span>
<span class="normal"><a href="#__codelineno-0-387">387</a></span>
<span class="normal"><a href="#__codelineno-0-388">388</a></span>
<span class="normal"><a href="#__codelineno-0-389">389</a></span>
<span class="normal"><a href="#__codelineno-0-390">390</a></span>
<span class="normal"><a href="#__codelineno-0-391">391</a></span>
<span class="normal"><a href="#__codelineno-0-392">392</a></span>
<span class="normal"><a href="#__codelineno-0-393">393</a></span>
<span class="normal"><a href="#__codelineno-0-394">394</a></span>
<span class="normal"><a href="#__codelineno-0-395">395</a></span>
<span class="normal"><a href="#__codelineno-0-396">396</a></span>
<span class="normal"><a href="#__codelineno-0-397">397</a></span>
<span class="normal"><a href="#__codelineno-0-398">398</a></span>
<span class="normal"><a href="#__codelineno-0-399">399</a></span>
<span class="normal"><a href="#__codelineno-0-400">400</a></span>
<span class="normal"><a href="#__codelineno-0-401">401</a></span>
<span class="normal"><a href="#__codelineno-0-402">402</a></span>
<span class="normal"><a href="#__codelineno-0-403">403</a></span>
<span class="normal"><a href="#__codelineno-0-404">404</a></span>
<span class="normal"><a href="#__codelineno-0-405">405</a></span>
<span class="normal"><a href="#__codelineno-0-406">406</a></span>
<span class="normal"><a href="#__codelineno-0-407">407</a></span>
<span class="normal"><a href="#__codelineno-0-408">408</a></span>
<span class="normal"><a href="#__codelineno-0-409">409</a></span>
<span class="normal"><a href="#__codelineno-0-410">410</a></span>
<span class="normal"><a href="#__codelineno-0-411">411</a></span>
<span class="normal"><a href="#__codelineno-0-412">412</a></span>
<span class="normal"><a href="#__codelineno-0-413">413</a></span>
<span class="normal"><a href="#__codelineno-0-414">414</a></span>
<span class="normal"><a href="#__codelineno-0-415">415</a></span>
<span class="normal"><a href="#__codelineno-0-416">416</a></span>
<span class="normal"><a href="#__codelineno-0-417">417</a></span>
<span class="normal"><a href="#__codelineno-0-418">418</a></span>
<span class="normal"><a href="#__codelineno-0-419">419</a></span>
<span class="normal"><a href="#__codelineno-0-420">420</a></span>
<span class="normal"><a href="#__codelineno-0-421">421</a></span>
<span class="normal"><a href="#__codelineno-0-422">422</a></span>
<span class="normal"><a href="#__codelineno-0-423">423</a></span>
<span class="normal"><a href="#__codelineno-0-424">424</a></span>
<span class="normal"><a href="#__codelineno-0-425">425</a></span>
<span class="normal"><a href="#__codelineno-0-426">426</a></span>
<span class="normal"><a href="#__codelineno-0-427">427</a></span>
<span class="normal"><a href="#__codelineno-0-428">428</a></span>
<span class="normal"><a href="#__codelineno-0-429">429</a></span>
<span class="normal"><a href="#__codelineno-0-430">430</a></span>
<span class="normal"><a href="#__codelineno-0-431">431</a></span>
<span class="normal"><a href="#__codelineno-0-432">432</a></span>
<span class="normal"><a href="#__codelineno-0-433">433</a></span>
<span class="normal"><a href="#__codelineno-0-434">434</a></span>
<span class="normal"><a href="#__codelineno-0-435">435</a></span>
<span class="normal"><a href="#__codelineno-0-436">436</a></span>
<span class="normal"><a href="#__codelineno-0-437">437</a></span>
<span class="normal"><a href="#__codelineno-0-438">438</a></span>
<span class="normal"><a href="#__codelineno-0-439">439</a></span>
<span class="normal"><a href="#__codelineno-0-440">440</a></span>
<span class="normal"><a href="#__codelineno-0-441">441</a></span>
<span class="normal"><a href="#__codelineno-0-442">442</a></span>
<span class="normal"><a href="#__codelineno-0-443">443</a></span>
<span class="normal"><a href="#__codelineno-0-444">444</a></span>
<span class="normal"><a href="#__codelineno-0-445">445</a></span>
<span class="normal"><a href="#__codelineno-0-446">446</a></span>
<span class="normal"><a href="#__codelineno-0-447">447</a></span>
<span class="normal"><a href="#__codelineno-0-448">448</a></span>
<span class="normal"><a href="#__codelineno-0-449">449</a></span>
<span class="normal"><a href="#__codelineno-0-450">450</a></span>
<span class="normal"><a href="#__codelineno-0-451">451</a></span>
<span class="normal"><a href="#__codelineno-0-452">452</a></span>
<span class="normal"><a href="#__codelineno-0-453">453</a></span>
<span class="normal"><a href="#__codelineno-0-454">454</a></span>
<span class="normal"><a href="#__codelineno-0-455">455</a></span>
<span class="normal"><a href="#__codelineno-0-456">456</a></span>
<span class="normal"><a href="#__codelineno-0-457">457</a></span>
<span class="normal"><a href="#__codelineno-0-458">458</a></span>
<span class="normal"><a href="#__codelineno-0-459">459</a></span>
<span class="normal"><a href="#__codelineno-0-460">460</a></span>
<span class="normal"><a href="#__codelineno-0-461">461</a></span>
<span class="normal"><a href="#__codelineno-0-462">462</a></span>
<span class="normal"><a href="#__codelineno-0-463">463</a></span>
<span class="normal"><a href="#__codelineno-0-464">464</a></span>
<span class="normal"><a href="#__codelineno-0-465">465</a></span>
<span class="normal"><a href="#__codelineno-0-466">466</a></span>
<span class="normal"><a href="#__codelineno-0-467">467</a></span>
<span class="normal"><a href="#__codelineno-0-468">468</a></span>
<span class="normal"><a href="#__codelineno-0-469">469</a></span>
<span class="normal"><a href="#__codelineno-0-470">470</a></span>
<span class="normal"><a href="#__codelineno-0-471">471</a></span>
<span class="normal"><a href="#__codelineno-0-472">472</a></span>
<span class="normal"><a href="#__codelineno-0-473">473</a></span>
<span class="normal"><a href="#__codelineno-0-474">474</a></span>
<span class="normal"><a href="#__codelineno-0-475">475</a></span>
<span class="normal"><a href="#__codelineno-0-476">476</a></span>
<span class="normal"><a href="#__codelineno-0-477">477</a></span>
<span class="normal"><a href="#__codelineno-0-478">478</a></span>
<span class="normal"><a href="#__codelineno-0-479">479</a></span>
<span class="normal"><a href="#__codelineno-0-480">480</a></span>
<span class="normal"><a href="#__codelineno-0-481">481</a></span>
<span class="normal"><a href="#__codelineno-0-482">482</a></span>
<span class="normal"><a href="#__codelineno-0-483">483</a></span>
<span class="normal"><a href="#__codelineno-0-484">484</a></span>
<span class="normal"><a href="#__codelineno-0-485">485</a></span>
<span class="normal"><a href="#__codelineno-0-486">486</a></span>
<span class="normal"><a href="#__codelineno-0-487">487</a></span>
<span class="normal"><a href="#__codelineno-0-488">488</a></span>
<span class="normal"><a href="#__codelineno-0-489">489</a></span>
<span class="normal"><a href="#__codelineno-0-490">490</a></span>
<span class="normal"><a href="#__codelineno-0-491">491</a></span>
<span class="normal"><a href="#__codelineno-0-492">492</a></span>
<span class="normal"><a href="#__codelineno-0-493">493</a></span>
<span class="normal"><a href="#__codelineno-0-494">494</a></span>
<span class="normal"><a href="#__codelineno-0-495">495</a></span>
<span class="normal"><a href="#__codelineno-0-496">496</a></span>
<span class="normal"><a href="#__codelineno-0-497">497</a></span>
<span class="normal"><a href="#__codelineno-0-498">498</a></span>
<span class="normal"><a href="#__codelineno-0-499">499</a></span>
<span class="normal"><a href="#__codelineno-0-500">500</a></span>
<span class="normal"><a href="#__codelineno-0-501">501</a></span>
<span class="normal"><a href="#__codelineno-0-502">502</a></span>
<span class="normal"><a href="#__codelineno-0-503">503</a></span>
<span class="normal"><a href="#__codelineno-0-504">504</a></span>
<span class="normal"><a href="#__codelineno-0-505">505</a></span>
<span class="normal"><a href="#__codelineno-0-506">506</a></span>
<span class="normal"><a href="#__codelineno-0-507">507</a></span>
<span class="normal"><a href="#__codelineno-0-508">508</a></span>
<span class="normal"><a href="#__codelineno-0-509">509</a></span>
<span class="normal"><a href="#__codelineno-0-510">510</a></span>
<span class="normal"><a href="#__codelineno-0-511">511</a></span>
<span class="normal"><a href="#__codelineno-0-512">512</a></span>
<span class="normal"><a href="#__codelineno-0-513">513</a></span>
<span class="normal"><a href="#__codelineno-0-514">514</a></span>
<span class="normal"><a href="#__codelineno-0-515">515</a></span>
<span class="normal"><a href="#__codelineno-0-516">516</a></span>
<span class="normal"><a href="#__codelineno-0-517">517</a></span>
<span class="normal"><a href="#__codelineno-0-518">518</a></span>
<span class="normal"><a href="#__codelineno-0-519">519</a></span>
<span class="normal"><a href="#__codelineno-0-520">520</a></span>
<span class="normal"><a href="#__codelineno-0-521">521</a></span>
<span class="normal"><a href="#__codelineno-0-522">522</a></span>
<span class="normal"><a href="#__codelineno-0-523">523</a></span>
<span class="normal"><a href="#__codelineno-0-524">524</a></span>
<span class="normal"><a href="#__codelineno-0-525">525</a></span>
<span class="normal"><a href="#__codelineno-0-526">526</a></span>
<span class="normal"><a href="#__codelineno-0-527">527</a></span>
<span class="normal"><a href="#__codelineno-0-528">528</a></span>
<span class="normal"><a href="#__codelineno-0-529">529</a></span>
<span class="normal"><a href="#__codelineno-0-530">530</a></span>
<span class="normal"><a href="#__codelineno-0-531">531</a></span>
<span class="normal"><a href="#__codelineno-0-532">532</a></span>
<span class="normal"><a href="#__codelineno-0-533">533</a></span>
<span class="normal"><a href="#__codelineno-0-534">534</a></span>
<span class="normal"><a href="#__codelineno-0-535">535</a></span>
<span class="normal"><a href="#__codelineno-0-536">536</a></span>
<span class="normal"><a href="#__codelineno-0-537">537</a></span>
<span class="normal"><a href="#__codelineno-0-538">538</a></span>
<span class="normal"><a href="#__codelineno-0-539">539</a></span>
<span class="normal"><a href="#__codelineno-0-540">540</a></span>
<span class="normal"><a href="#__codelineno-0-541">541</a></span>
<span class="normal"><a href="#__codelineno-0-542">542</a></span>
<span class="normal"><a href="#__codelineno-0-543">543</a></span>
<span class="normal"><a href="#__codelineno-0-544">544</a></span>
<span class="normal"><a href="#__codelineno-0-545">545</a></span>
<span class="normal"><a href="#__codelineno-0-546">546</a></span>
<span class="normal"><a href="#__codelineno-0-547">547</a></span>
<span class="normal"><a href="#__codelineno-0-548">548</a></span>
<span class="normal"><a href="#__codelineno-0-549">549</a></span>
<span class="normal"><a href="#__codelineno-0-550">550</a></span>
<span class="normal"><a href="#__codelineno-0-551">551</a></span>
<span class="normal"><a href="#__codelineno-0-552">552</a></span>
<span class="normal"><a href="#__codelineno-0-553">553</a></span>
<span class="normal"><a href="#__codelineno-0-554">554</a></span>
<span class="normal"><a href="#__codelineno-0-555">555</a></span>
<span class="normal"><a href="#__codelineno-0-556">556</a></span>
<span class="normal"><a href="#__codelineno-0-557">557</a></span>
<span class="normal"><a href="#__codelineno-0-558">558</a></span>
<span class="normal"><a href="#__codelineno-0-559">559</a></span>
<span class="normal"><a href="#__codelineno-0-560">560</a></span>
<span class="normal"><a href="#__codelineno-0-561">561</a></span>
<span class="normal"><a href="#__codelineno-0-562">562</a></span>
<span class="normal"><a href="#__codelineno-0-563">563</a></span>
<span class="normal"><a href="#__codelineno-0-564">564</a></span>
<span class="normal"><a href="#__codelineno-0-565">565</a></span>
<span class="normal"><a href="#__codelineno-0-566">566</a></span>
<span class="normal"><a href="#__codelineno-0-567">567</a></span>
<span class="normal"><a href="#__codelineno-0-568">568</a></span>
<span class="normal"><a href="#__codelineno-0-569">569</a></span>
<span class="normal"><a href="#__codelineno-0-570">570</a></span>
<span class="normal"><a href="#__codelineno-0-571">571</a></span>
<span class="normal"><a href="#__codelineno-0-572">572</a></span>
<span class="normal"><a href="#__codelineno-0-573">573</a></span>
<span class="normal"><a href="#__codelineno-0-574">574</a></span>
<span class="normal"><a href="#__codelineno-0-575">575</a></span>
<span class="normal"><a href="#__codelineno-0-576">576</a></span>
<span class="normal"><a href="#__codelineno-0-577">577</a></span>
<span class="normal"><a href="#__codelineno-0-578">578</a></span>
<span class="normal"><a href="#__codelineno-0-579">579</a></span>
<span class="normal"><a href="#__codelineno-0-580">580</a></span>
<span class="normal"><a href="#__codelineno-0-581">581</a></span>
<span class="normal"><a href="#__codelineno-0-582">582</a></span>
<span class="normal"><a href="#__codelineno-0-583">583</a></span>
<span class="normal"><a href="#__codelineno-0-584">584</a></span>
<span class="normal"><a href="#__codelineno-0-585">585</a></span>
<span class="normal"><a href="#__codelineno-0-586">586</a></span>
<span class="normal"><a href="#__codelineno-0-587">587</a></span>
<span class="normal"><a href="#__codelineno-0-588">588</a></span>
<span class="normal"><a href="#__codelineno-0-589">589</a></span>
<span class="normal"><a href="#__codelineno-0-590">590</a></span>
<span class="normal"><a href="#__codelineno-0-591">591</a></span>
<span class="normal"><a href="#__codelineno-0-592">592</a></span>
<span class="normal"><a href="#__codelineno-0-593">593</a></span>
<span class="normal"><a href="#__codelineno-0-594">594</a></span>
<span class="normal"><a href="#__codelineno-0-595">595</a></span>
<span class="normal"><a href="#__codelineno-0-596">596</a></span>
<span class="normal"><a href="#__codelineno-0-597">597</a></span>
<span class="normal"><a href="#__codelineno-0-598">598</a></span>
<span class="normal"><a href="#__codelineno-0-599">599</a></span>
<span class="normal"><a href="#__codelineno-0-600">600</a></span>
<span class="normal"><a href="#__codelineno-0-601">601</a></span>
<span class="normal"><a href="#__codelineno-0-602">602</a></span>
<span class="normal"><a href="#__codelineno-0-603">603</a></span>
<span class="normal"><a href="#__codelineno-0-604">604</a></span>
<span class="normal"><a href="#__codelineno-0-605">605</a></span>
<span class="normal"><a href="#__codelineno-0-606">606</a></span>
<span class="normal"><a href="#__codelineno-0-607">607</a></span>
<span class="normal"><a href="#__codelineno-0-608">608</a></span>
<span class="normal"><a href="#__codelineno-0-609">609</a></span>
<span class="normal"><a href="#__codelineno-0-610">610</a></span>
<span class="normal"><a href="#__codelineno-0-611">611</a></span>
<span class="normal"><a href="#__codelineno-0-612">612</a></span>
<span class="normal"><a href="#__codelineno-0-613">613</a></span>
<span class="normal"><a href="#__codelineno-0-614">614</a></span>
<span class="normal"><a href="#__codelineno-0-615">615</a></span>
<span class="normal"><a href="#__codelineno-0-616">616</a></span>
<span class="normal"><a href="#__codelineno-0-617">617</a></span>
<span class="normal"><a href="#__codelineno-0-618">618</a></span>
<span class="normal"><a href="#__codelineno-0-619">619</a></span>
<span class="normal"><a href="#__codelineno-0-620">620</a></span>
<span class="normal"><a href="#__codelineno-0-621">621</a></span>
<span class="normal"><a href="#__codelineno-0-622">622</a></span>
<span class="normal"><a href="#__codelineno-0-623">623</a></span>
<span class="normal"><a href="#__codelineno-0-624">624</a></span>
<span class="normal"><a href="#__codelineno-0-625">625</a></span>
<span class="normal"><a href="#__codelineno-0-626">626</a></span>
<span class="normal"><a href="#__codelineno-0-627">627</a></span>
<span class="normal"><a href="#__codelineno-0-628">628</a></span>
<span class="normal"><a href="#__codelineno-0-629">629</a></span>
<span class="normal"><a href="#__codelineno-0-630">630</a></span>
<span class="normal"><a href="#__codelineno-0-631">631</a></span>
<span class="normal"><a href="#__codelineno-0-632">632</a></span>
<span class="normal"><a href="#__codelineno-0-633">633</a></span>
<span class="normal"><a href="#__codelineno-0-634">634</a></span>
<span class="normal"><a href="#__codelineno-0-635">635</a></span>
<span class="normal"><a href="#__codelineno-0-636">636</a></span>
<span class="normal"><a href="#__codelineno-0-637">637</a></span>
<span class="normal"><a href="#__codelineno-0-638">638</a></span>
<span class="normal"><a href="#__codelineno-0-639">639</a></span>
<span class="normal"><a href="#__codelineno-0-640">640</a></span>
<span class="normal"><a href="#__codelineno-0-641">641</a></span>
<span class="normal"><a href="#__codelineno-0-642">642</a></span>
<span class="normal"><a href="#__codelineno-0-643">643</a></span>
<span class="normal"><a href="#__codelineno-0-644">644</a></span>
<span class="normal"><a href="#__codelineno-0-645">645</a></span>
<span class="normal"><a href="#__codelineno-0-646">646</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-16" name="__codelineno-0-16"></a><span class="k">class</span> <span class="nc">Lattice</span><span class="p">(</span><span class="n">ConstrainedModule</span><span class="p">):</span>
<a id="__codelineno-0-17" name="__codelineno-0-17"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;A Lattice Module.</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a><span class="sd">    Layer performs interpolation using one of &#39;units&#39; d-dimensional lattices with</span>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a><span class="sd">    arbitrary number of keypoints per dimension. Each lattice vertex has a trainable</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a><span class="sd">    weight, and input is considered to be a d-dimensional point within the lattice.</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a><span class="sd">    Attributes:</span>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a><span class="sd">        All: `__init__` arguments.</span>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a><span class="sd">        kernel: `torch.nn.Parameter` of shape `(prod(lattice_sizes), units)` which</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a><span class="sd">            stores</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a><span class="sd">            weights at each vertex of lattice.</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a><span class="sd">    Example:</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a><span class="sd">    ```python</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a><span class="sd">    lattice_sizes = [2, 2, 4, 3]</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a><span class="sd">    inputs=torch.tensor(...) # shape: (batch_size, len(lattice_sizes))</span>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="sd">    lattice=Lattice(</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a><span class="sd">        lattice_sizes,</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a><span class="sd">        clip_inputs=True,</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a><span class="sd">        interpolation=Interpolation.HYPERCUBE,</span>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="sd">        units=1,</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a><span class="sd">    )</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a><span class="sd">    outputs = Lattice(inputs)</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a><span class="sd">    ```</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>        <span class="n">lattice_sizes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>        <span class="n">output_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>        <span class="n">output_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>        <span class="n">kernel_init</span><span class="p">:</span> <span class="n">LatticeInit</span> <span class="o">=</span> <span class="n">LatticeInit</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>        <span class="n">monotonicities</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Monotonicity</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>        <span class="n">clip_inputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>        <span class="n">interpolation</span><span class="p">:</span> <span class="n">Interpolation</span> <span class="o">=</span> <span class="n">Interpolation</span><span class="o">.</span><span class="n">HYPERCUBE</span><span class="p">,</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a>        <span class="n">units</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an instance of &#39;Lattice&#39;.</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">            lattice_sizes: List or tuple of size of lattice along each dimension.</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">            output_min: Minimum output value for weights at vertices of lattice.</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">            output_max: Maximum output value for weights at vertices of lattice.</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a><span class="sd">            kernel_init: Initialization scheme to use for the kernel.</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a><span class="sd">            monotonicities: `None` or list of `NONE` or</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a><span class="sd">                `Monotonicity.INCREASING` of length `len(lattice_sizes)` specifying</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a><span class="sd">                monotonicity of each feature of lattice. A monotonically decreasing</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a><span class="sd">                 feature should use `Monotonicity.INCREASING` in the lattice layer but</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a><span class="sd">                `Monotonicity.DECREASING` in the calibrator.</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a><span class="sd">            clip_inputs: Whether input points should be clipped to the range of lattice.</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">            interpolation: Interpolation scheme for a given input.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a><span class="sd">            units: Dimensionality of weights stored at each vertex of lattice.</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a><span class="sd">        Raises:</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a><span class="sd">            ValueError: if `kernel_init` is invalid.</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a><span class="sd">            NotImplementedError: Random monotonic initialization not yet implemented.</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">=</span> <span class="n">output_min</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">=</span> <span class="n">output_max</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">=</span> <span class="n">kernel_init</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">clip_inputs</span> <span class="o">=</span> <span class="n">clip_inputs</span>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">=</span> <span class="n">interpolation</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>        <span class="k">if</span> <span class="n">monotonicities</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span> <span class="o">=</span> <span class="n">monotonicities</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>        <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>            <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a>        <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>            <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_min</span> <span class="o">+</span> <span class="mf">4.0</span>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a>        <span class="k">elif</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>            <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_max</span> <span class="o">-</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a>            <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span> <span class="o">=</span> <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>        <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>        <span class="k">def</span> <span class="nf">initialize_kernel</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">==</span> <span class="n">LatticeInit</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">:</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear_initializer</span><span class="p">()</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">==</span> <span class="n">LatticeInit</span><span class="o">.</span><span class="n">RANDOM_MONOTONIC</span><span class="p">:</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>                    <span class="s2">&quot;Random monotonic initialization not yet implemented.&quot;</span>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a>                <span class="p">)</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown kernel init: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initialize_kernel</span><span class="p">())</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculates interpolation from input, using method of self.interpolation.</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a><span class="sd">            x: input tensor. If `units == 1`, tensor of shape:</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a><span class="sd">                `(batch_size, ..., len(lattice_size))` or list of `len(lattice_sizes)`</span>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a><span class="sd">                tensors of same shape: `(batch_size, ..., 1)`. If `units &gt; 1`, tensor of</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a><span class="sd">                shape `(batch_size, ..., units, len(lattice_sizes))` or list of</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a><span class="sd">                `len(lattice_sizes)` tensors OF same shape `(batch_size, ..., units, 1)`</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a><span class="sd">            torch.Tensor of shape `(batch_size, ..., units)` containing interpolated</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a><span class="sd">            values.</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a><span class="sd">        Raises:</span>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a><span class="sd">            ValueError: If the type of interpolation is unknown.</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a>        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">xi</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">==</span> <span class="n">Interpolation</span><span class="o">.</span><span class="n">HYPERCUBE</span><span class="p">:</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_hypercube_interpolation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">==</span> <span class="n">Interpolation</span><span class="o">.</span><span class="n">SIMPLEX</span><span class="p">:</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_simplex_interpolation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown interpolation type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>    <span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Aggregate function for enforcing constraints of lattice.&quot;&quot;&quot;</span>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_non_zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">):</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a>            <span class="n">lattice_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>            <span class="n">monotonicities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a>                <span class="n">lattice_sizes</span> <span class="o">=</span> <span class="n">lattice_sizes</span> <span class="o">+</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)]</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a>                    <span class="n">monotonicities</span> <span class="o">=</span> <span class="n">monotonicities</span> <span class="o">+</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a>            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a>            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_approximately_project_monotonicity</span><span class="p">(</span>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a>                <span class="n">weights</span><span class="p">,</span> <span class="n">lattice_sizes</span><span class="p">,</span> <span class="n">monotonicities</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a>            <span class="p">)</span>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span><span class="p">)</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a>            <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span><span class="p">)</span>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a>    <span class="k">def</span> <span class="nf">assert_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Asserts that layer satisfies specified constraints.</span>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">        This checks that weights follow monotonicity and bounds constraints.</span>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a><span class="sd">            eps: the margin of error allowed</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a><span class="sd">            A list of dicts describing violated constraints including indices of</span>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a><span class="sd">            monotonicity violations. If no constraints violated, the list will be empty.</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>        <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>        <span class="n">lattice_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>        <span class="n">monotonicities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a>        <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-179" name="__codelineno-0-179"></a>            <span class="n">lattice_sizes</span> <span class="o">=</span> <span class="n">lattice_sizes</span> <span class="o">+</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<a id="__codelineno-0-180" name="__codelineno-0-180"></a>            <span class="k">if</span> <span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-181" name="__codelineno-0-181"></a>                <span class="n">monotonicities</span> <span class="o">=</span> <span class="n">monotonicities</span> <span class="o">+</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
<a id="__codelineno-0-182" name="__codelineno-0-182"></a>
<a id="__codelineno-0-183" name="__codelineno-0-183"></a>        <span class="c1"># Reshape weights to match lattice sizes</span>
<a id="__codelineno-0-184" name="__codelineno-0-184"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-185" name="__codelineno-0-185"></a>
<a id="__codelineno-0-186" name="__codelineno-0-186"></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">monotonicities</span> <span class="ow">or</span> <span class="p">[])):</span>
<a id="__codelineno-0-187" name="__codelineno-0-187"></a>            <span class="k">if</span> <span class="n">monotonicities</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">:</span>
<a id="__codelineno-0-188" name="__codelineno-0-188"></a>                <span class="k">continue</span>
<a id="__codelineno-0-189" name="__codelineno-0-189"></a>            <span class="n">weights_layers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
<a id="__codelineno-0-190" name="__codelineno-0-190"></a>
<a id="__codelineno-0-191" name="__codelineno-0-191"></a>            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights_layers</span><span class="p">)):</span>
<a id="__codelineno-0-192" name="__codelineno-0-192"></a>                <span class="n">diff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">weights_layers</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">weights_layers</span><span class="p">[</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-193" name="__codelineno-0-193"></a>                <span class="k">if</span> <span class="n">diff</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-194" name="__codelineno-0-194"></a>                    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Monotonicity violated at feature index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-195" name="__codelineno-0-195"></a>
<a id="__codelineno-0-196" name="__codelineno-0-196"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">+</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-197" name="__codelineno-0-197"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Max weight greater than output_max.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-198" name="__codelineno-0-198"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">-</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-199" name="__codelineno-0-199"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Min weight less than output_min.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-200" name="__codelineno-0-200"></a>
<a id="__codelineno-0-201" name="__codelineno-0-201"></a>        <span class="k">return</span> <span class="n">messages</span>
<a id="__codelineno-0-202" name="__codelineno-0-202"></a>
<a id="__codelineno-0-203" name="__codelineno-0-203"></a>    <span class="c1">################################################################################</span>
<a id="__codelineno-0-204" name="__codelineno-0-204"></a>    <span class="c1">############################## PRIVATE METHODS #################################</span>
<a id="__codelineno-0-205" name="__codelineno-0-205"></a>    <span class="c1">################################################################################</span>
<a id="__codelineno-0-206" name="__codelineno-0-206"></a>
<a id="__codelineno-0-207" name="__codelineno-0-207"></a>    <span class="k">def</span> <span class="nf">_linear_initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-208" name="__codelineno-0-208"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates initial weights tensor for linear initialization.</span>
<a id="__codelineno-0-209" name="__codelineno-0-209"></a>
<a id="__codelineno-0-210" name="__codelineno-0-210"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-211" name="__codelineno-0-211"></a><span class="sd">            monotonicities: monotonicity constraints of lattice, enforced in</span>
<a id="__codelineno-0-212" name="__codelineno-0-212"></a><span class="sd">                initialization.</span>
<a id="__codelineno-0-213" name="__codelineno-0-213"></a>
<a id="__codelineno-0-214" name="__codelineno-0-214"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-215" name="__codelineno-0-215"></a><span class="sd">            `torch.Tensor` of shape `(prod(lattice_sizes), units)`</span>
<a id="__codelineno-0-216" name="__codelineno-0-216"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-217" name="__codelineno-0-217"></a>        <span class="n">monotonicities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">[:]</span>
<a id="__codelineno-0-218" name="__codelineno-0-218"></a>
<a id="__codelineno-0-219" name="__codelineno-0-219"></a>        <span class="k">if</span> <span class="n">monotonicities</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-220" name="__codelineno-0-220"></a>            <span class="n">monotonicities</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-221" name="__codelineno-0-221"></a>
<a id="__codelineno-0-222" name="__codelineno-0-222"></a>        <span class="n">num_constraint_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_non_zeros</span><span class="p">(</span><span class="n">monotonicities</span><span class="p">)</span>
<a id="__codelineno-0-223" name="__codelineno-0-223"></a>        <span class="k">if</span> <span class="n">num_constraint_dims</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-224" name="__codelineno-0-224"></a>            <span class="n">monotonicities</span> <span class="o">=</span> <span class="p">[</span><span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-225" name="__codelineno-0-225"></a>            <span class="n">num_constraint_dims</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-226" name="__codelineno-0-226"></a>
<a id="__codelineno-0-227" name="__codelineno-0-227"></a>        <span class="n">dim_range</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-228" name="__codelineno-0-228"></a>            <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_constraint_dims</span>
<a id="__codelineno-0-229" name="__codelineno-0-229"></a>        <span class="p">)</span>
<a id="__codelineno-0-230" name="__codelineno-0-230"></a>        <span class="n">one_d_weights</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-231" name="__codelineno-0-231"></a>
<a id="__codelineno-0-232" name="__codelineno-0-232"></a>        <span class="k">for</span> <span class="n">monotonicity</span><span class="p">,</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">monotonicities</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">):</span>
<a id="__codelineno-0-233" name="__codelineno-0-233"></a>            <span class="k">if</span> <span class="n">monotonicity</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-234" name="__codelineno-0-234"></a>                <span class="n">one_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="n">dim_range</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">dim_size</span><span class="p">)</span>
<a id="__codelineno-0-235" name="__codelineno-0-235"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-236" name="__codelineno-0-236"></a>                <span class="n">one_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dim_size</span><span class="p">)</span>
<a id="__codelineno-0-237" name="__codelineno-0-237"></a>
<a id="__codelineno-0-238" name="__codelineno-0-238"></a>            <span class="n">one_d_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">one_d</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<a id="__codelineno-0-239" name="__codelineno-0-239"></a>
<a id="__codelineno-0-240" name="__codelineno-0-240"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_outer_operation</span><span class="p">(</span><span class="n">one_d_weights</span><span class="p">,</span> <span class="n">operation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">)</span>
<a id="__codelineno-0-241" name="__codelineno-0-241"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-242" name="__codelineno-0-242"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-243" name="__codelineno-0-243"></a>            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)</span>
<a id="__codelineno-0-244" name="__codelineno-0-244"></a>
<a id="__codelineno-0-245" name="__codelineno-0-245"></a>        <span class="k">return</span> <span class="n">weights</span>
<a id="__codelineno-0-246" name="__codelineno-0-246"></a>
<a id="__codelineno-0-247" name="__codelineno-0-247"></a>    <span class="nd">@staticmethod</span>
<a id="__codelineno-0-248" name="__codelineno-0-248"></a>    <span class="k">def</span> <span class="nf">_count_non_zeros</span><span class="p">(</span><span class="o">*</span><span class="n">iterables</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<a id="__codelineno-0-249" name="__codelineno-0-249"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns total number of non 0/None enum elements in given iterables.</span>
<a id="__codelineno-0-250" name="__codelineno-0-250"></a>
<a id="__codelineno-0-251" name="__codelineno-0-251"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-252" name="__codelineno-0-252"></a><span class="sd">            *iterables: Any number of the value `None` or iterables of `None` or</span>
<a id="__codelineno-0-253" name="__codelineno-0-253"></a><span class="sd">                `Monotonicity` enum values.</span>
<a id="__codelineno-0-254" name="__codelineno-0-254"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-255" name="__codelineno-0-255"></a>        <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-0-256" name="__codelineno-0-256"></a>        <span class="k">for</span> <span class="n">iterable</span> <span class="ow">in</span> <span class="n">iterables</span><span class="p">:</span>
<a id="__codelineno-0-257" name="__codelineno-0-257"></a>            <span class="k">if</span> <span class="n">iterable</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-258" name="__codelineno-0-258"></a>                <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">iterable</span><span class="p">:</span>
<a id="__codelineno-0-259" name="__codelineno-0-259"></a>                    <span class="k">if</span> <span class="n">element</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-260" name="__codelineno-0-260"></a>                        <span class="n">result</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-0-261" name="__codelineno-0-261"></a>        <span class="k">return</span> <span class="n">result</span>
<a id="__codelineno-0-262" name="__codelineno-0-262"></a>
<a id="__codelineno-0-263" name="__codelineno-0-263"></a>    <span class="k">def</span> <span class="nf">_compute_simplex_interpolation</span><span class="p">(</span>
<a id="__codelineno-0-264" name="__codelineno-0-264"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<a id="__codelineno-0-265" name="__codelineno-0-265"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-266" name="__codelineno-0-266"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluates a lattice using simplex interpolation.</span>
<a id="__codelineno-0-267" name="__codelineno-0-267"></a>
<a id="__codelineno-0-268" name="__codelineno-0-268"></a><span class="sd">        Each `d`-dimensional unit hypercube of the lattice can be partitioned into `d!`</span>
<a id="__codelineno-0-269" name="__codelineno-0-269"></a><span class="sd">        disjoint simplices with `d+1` vertices. `S` is the unique simplex which contains</span>
<a id="__codelineno-0-270" name="__codelineno-0-270"></a><span class="sd">        input point `P`, and `S` has vertices `ABCD...`. For any vertex such as `A`, a</span>
<a id="__codelineno-0-271" name="__codelineno-0-271"></a><span class="sd">        new simplex `S&#39;` can be created using the vertices `PBCD...`. The weight of `A`</span>
<a id="__codelineno-0-272" name="__codelineno-0-272"></a><span class="sd">        within the interpolation is then `vol(S&#39;)/vol(S)`. This process is repeated</span>
<a id="__codelineno-0-273" name="__codelineno-0-273"></a><span class="sd">        for every vertex in `S`, and the resulting values are summed.</span>
<a id="__codelineno-0-274" name="__codelineno-0-274"></a>
<a id="__codelineno-0-275" name="__codelineno-0-275"></a><span class="sd">        This interpolation can be computed in `O(D log(D))` time because it is only</span>
<a id="__codelineno-0-276" name="__codelineno-0-276"></a><span class="sd">        necessary to compute the volume of the simplex containing input point `P`. For</span>
<a id="__codelineno-0-277" name="__codelineno-0-277"></a><span class="sd">        context, the unit hypercube can be partitioned into `d!` simplices by starting</span>
<a id="__codelineno-0-278" name="__codelineno-0-278"></a><span class="sd">        at `(0,0,...,0)` and incrementing `0` to `1` dimension-by-dimensionuntil one</span>
<a id="__codelineno-0-279" name="__codelineno-0-279"></a><span class="sd">        reaches `(1,1,...,1)`. There are `d!` possible paths from `(0,0,...,0)` to</span>
<a id="__codelineno-0-280" name="__codelineno-0-280"></a><span class="sd">        `(1,1,...,1)`, which account for the number of unique, disjoint simplices</span>
<a id="__codelineno-0-281" name="__codelineno-0-281"></a><span class="sd">        created by the method. There are `d` steps for each possible path where each</span>
<a id="__codelineno-0-282" name="__codelineno-0-282"></a><span class="sd">        step comprises the vertices of one simplex. Thus, one can find the containing</span>
<a id="__codelineno-0-283" name="__codelineno-0-283"></a><span class="sd">        simplex for input `P` by argsorting the coordinates of `P` in descending order</span>
<a id="__codelineno-0-284" name="__codelineno-0-284"></a><span class="sd">        and pathing along said order. To compute the intepolation weights simply take</span>
<a id="__codelineno-0-285" name="__codelineno-0-285"></a><span class="sd">        the deltas from `[1, desc_sort(P_coords), 0]`.</span>
<a id="__codelineno-0-286" name="__codelineno-0-286"></a>
<a id="__codelineno-0-287" name="__codelineno-0-287"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-288" name="__codelineno-0-288"></a><span class="sd">            inputs: input tensor. If `units == 1`, tensor of shape:</span>
<a id="__codelineno-0-289" name="__codelineno-0-289"></a><span class="sd">                `(batch_size, ..., len(lattice_size))` or list of `len(lattice_sizes)`</span>
<a id="__codelineno-0-290" name="__codelineno-0-290"></a><span class="sd">                tensors of same shape: `(batch_size, ..., 1)`. If `units &gt; 1`, tensor of</span>
<a id="__codelineno-0-291" name="__codelineno-0-291"></a><span class="sd">                shape `(batch_size, ..., units, len(lattice_sizes))` or list of</span>
<a id="__codelineno-0-292" name="__codelineno-0-292"></a><span class="sd">                `len(lattice_sizes)` tensors of same shape `(batch_size, ..., units, 1)`</span>
<a id="__codelineno-0-293" name="__codelineno-0-293"></a>
<a id="__codelineno-0-294" name="__codelineno-0-294"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-295" name="__codelineno-0-295"></a><span class="sd">            `torch.Tensor` of shape `(batch_size, ..., units)` containing interpolated</span>
<a id="__codelineno-0-296" name="__codelineno-0-296"></a><span class="sd">            values.</span>
<a id="__codelineno-0-297" name="__codelineno-0-297"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-298" name="__codelineno-0-298"></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<a id="__codelineno-0-299" name="__codelineno-0-299"></a>            <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-300" name="__codelineno-0-300"></a>
<a id="__codelineno-0-301" name="__codelineno-0-301"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_inputs</span><span class="p">:</span>
<a id="__codelineno-0-302" name="__codelineno-0-302"></a>            <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clip_onto_lattice_range</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<a id="__codelineno-0-303" name="__codelineno-0-303"></a>
<a id="__codelineno-0-304" name="__codelineno-0-304"></a>        <span class="n">lattice_rank</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-305" name="__codelineno-0-305"></a>        <span class="n">input_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<a id="__codelineno-0-306" name="__codelineno-0-306"></a>        <span class="n">all_size_2</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="n">size</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-307" name="__codelineno-0-307"></a>
<a id="__codelineno-0-308" name="__codelineno-0-308"></a>        <span class="c1"># Strides are the index shift (with respect to flattened kernel data) of each</span>
<a id="__codelineno-0-309" name="__codelineno-0-309"></a>        <span class="c1"># dimension, which can be used in a dot product with multi-dimensional</span>
<a id="__codelineno-0-310" name="__codelineno-0-310"></a>        <span class="c1"># coordinates to give an index for the flattened lattice weights.</span>
<a id="__codelineno-0-311" name="__codelineno-0-311"></a>        <span class="c1"># Ex): for lattice_sizes = [4, 3, 2], we get strides = [6, 2, 1]: when looking</span>
<a id="__codelineno-0-312" name="__codelineno-0-312"></a>        <span class="c1"># at lattice coords (i, j, k) and kernel data flattened into 1-D, incrementing i</span>
<a id="__codelineno-0-313" name="__codelineno-0-313"></a>        <span class="c1"># corresponds to a shift of 6 in flattened kernel data, j corresponds to a shift</span>
<a id="__codelineno-0-314" name="__codelineno-0-314"></a>        <span class="c1"># of 2, and k corresponds to a shift of 1. Consequently, we can do</span>
<a id="__codelineno-0-315" name="__codelineno-0-315"></a>        <span class="c1"># (coords * strides) for any coordinates to obtain the flattened index.</span>
<a id="__codelineno-0-316" name="__codelineno-0-316"></a>        <span class="n">strides</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-317" name="__codelineno-0-317"></a>            <span class="n">np</span><span class="o">.</span><span class="n">cumprod</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">])[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<a id="__codelineno-0-318" name="__codelineno-0-318"></a>        <span class="p">)</span>
<a id="__codelineno-0-319" name="__codelineno-0-319"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">all_size_2</span><span class="p">:</span>
<a id="__codelineno-0-320" name="__codelineno-0-320"></a>            <span class="n">lower_corner_coordinates</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
<a id="__codelineno-0-321" name="__codelineno-0-321"></a>            <span class="n">lower_corner_coordinates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span>
<a id="__codelineno-0-322" name="__codelineno-0-322"></a>                <span class="n">lower_corner_coordinates</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span>
<a id="__codelineno-0-323" name="__codelineno-0-323"></a>            <span class="p">)</span>
<a id="__codelineno-0-324" name="__codelineno-0-324"></a>            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">-</span> <span class="n">lower_corner_coordinates</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<a id="__codelineno-0-325" name="__codelineno-0-325"></a>
<a id="__codelineno-0-326" name="__codelineno-0-326"></a>        <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<a id="__codelineno-0-327" name="__codelineno-0-327"></a>        <span class="n">sorted_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<a id="__codelineno-0-328" name="__codelineno-0-328"></a>
<a id="__codelineno-0-329" name="__codelineno-0-329"></a>        <span class="c1"># Pad the 1 and 0 onto the ends of sorted coordinates and compute deltas.</span>
<a id="__codelineno-0-330" name="__codelineno-0-330"></a>        <span class="n">no_padding_dims</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-331" name="__codelineno-0-331"></a>        <span class="n">flat_no_padding</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">no_padding_dims</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
<a id="__codelineno-0-332" name="__codelineno-0-332"></a>        <span class="n">sorted_inputs_padded_left</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
<a id="__codelineno-0-333" name="__codelineno-0-333"></a>            <span class="n">sorted_inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">flat_no_padding</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span>
<a id="__codelineno-0-334" name="__codelineno-0-334"></a>        <span class="p">)</span>
<a id="__codelineno-0-335" name="__codelineno-0-335"></a>        <span class="n">sorted_inputs_padded_right</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
<a id="__codelineno-0-336" name="__codelineno-0-336"></a>            <span class="n">sorted_inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">flat_no_padding</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.0</span>
<a id="__codelineno-0-337" name="__codelineno-0-337"></a>        <span class="p">)</span>
<a id="__codelineno-0-338" name="__codelineno-0-338"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">sorted_inputs_padded_left</span> <span class="o">-</span> <span class="n">sorted_inputs_padded_right</span>
<a id="__codelineno-0-339" name="__codelineno-0-339"></a>
<a id="__codelineno-0-340" name="__codelineno-0-340"></a>        <span class="c1"># Use strides to find indices of simplex vertices in flattened form.</span>
<a id="__codelineno-0-341" name="__codelineno-0-341"></a>        <span class="n">sorted_strides</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">strides</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
<a id="__codelineno-0-342" name="__codelineno-0-342"></a>            <span class="n">sorted_indices</span><span class="o">.</span><span class="n">shape</span>
<a id="__codelineno-0-343" name="__codelineno-0-343"></a>        <span class="p">)</span>
<a id="__codelineno-0-344" name="__codelineno-0-344"></a>        <span class="k">if</span> <span class="n">all_size_2</span><span class="p">:</span>
<a id="__codelineno-0-345" name="__codelineno-0-345"></a>            <span class="n">corner_offset_and_sorted_strides</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
<a id="__codelineno-0-346" name="__codelineno-0-346"></a>                <span class="n">sorted_strides</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">flat_no_padding</span>
<a id="__codelineno-0-347" name="__codelineno-0-347"></a>            <span class="p">)</span>
<a id="__codelineno-0-348" name="__codelineno-0-348"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-349" name="__codelineno-0-349"></a>            <span class="n">lower_corner_offset</span> <span class="o">=</span> <span class="p">(</span><span class="n">lower_corner_coordinates</span> <span class="o">*</span> <span class="n">strides</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
<a id="__codelineno-0-350" name="__codelineno-0-350"></a>                <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span>
<a id="__codelineno-0-351" name="__codelineno-0-351"></a>            <span class="p">)</span>
<a id="__codelineno-0-352" name="__codelineno-0-352"></a>            <span class="n">corner_offset_and_sorted_strides</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-353" name="__codelineno-0-353"></a>                <span class="p">[</span><span class="n">lower_corner_offset</span><span class="p">,</span> <span class="n">sorted_strides</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
<a id="__codelineno-0-354" name="__codelineno-0-354"></a>            <span class="p">)</span>
<a id="__codelineno-0-355" name="__codelineno-0-355"></a>        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">corner_offset_and_sorted_strides</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-356" name="__codelineno-0-356"></a>
<a id="__codelineno-0-357" name="__codelineno-0-357"></a>        <span class="c1"># Get kernel data from corresponding simplex vertices.</span>
<a id="__codelineno-0-358" name="__codelineno-0-358"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-359" name="__codelineno-0-359"></a>            <span class="n">gathered_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span>
<a id="__codelineno-0-360" name="__codelineno-0-360"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-361" name="__codelineno-0-361"></a>            <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<a id="__codelineno-0-362" name="__codelineno-0-362"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-363" name="__codelineno-0-363"></a>            <span class="n">unit_offset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-364" name="__codelineno-0-364"></a>                <span class="p">[[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">lattice_rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)]</span>
<a id="__codelineno-0-365" name="__codelineno-0-365"></a>            <span class="p">)</span>
<a id="__codelineno-0-366" name="__codelineno-0-366"></a>            <span class="n">flat_indices</span> <span class="o">=</span> <span class="n">indices</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">+</span> <span class="n">unit_offset</span>
<a id="__codelineno-0-367" name="__codelineno-0-367"></a>            <span class="n">gathered_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span>
<a id="__codelineno-0-368" name="__codelineno-0-368"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">flat_indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-369" name="__codelineno-0-369"></a>            <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<a id="__codelineno-0-370" name="__codelineno-0-370"></a>
<a id="__codelineno-0-371" name="__codelineno-0-371"></a>        <span class="k">return</span> <span class="p">(</span><span class="n">gathered_params</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-372" name="__codelineno-0-372"></a>
<a id="__codelineno-0-373" name="__codelineno-0-373"></a>    <span class="k">def</span> <span class="nf">_compute_hypercube_interpolation</span><span class="p">(</span>
<a id="__codelineno-0-374" name="__codelineno-0-374"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-375" name="__codelineno-0-375"></a>        <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
<a id="__codelineno-0-376" name="__codelineno-0-376"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-377" name="__codelineno-0-377"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Performs hypercube interpolation using the surrounding unit hypercube.</span>
<a id="__codelineno-0-378" name="__codelineno-0-378"></a>
<a id="__codelineno-0-379" name="__codelineno-0-379"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-380" name="__codelineno-0-380"></a><span class="sd">            inputs: input tensor. If `units == 1`, tensor of shape:</span>
<a id="__codelineno-0-381" name="__codelineno-0-381"></a><span class="sd">                `(batch_size, ..., len(lattice_size))` or list of `len(lattice_sizes)`</span>
<a id="__codelineno-0-382" name="__codelineno-0-382"></a><span class="sd">                tensors of same shape: `(batch_size, ..., 1)`. If `units &gt; 1`, tensor of</span>
<a id="__codelineno-0-383" name="__codelineno-0-383"></a><span class="sd">                shape `(batch_size, ..., units, len(lattice_sizes))` or list of</span>
<a id="__codelineno-0-384" name="__codelineno-0-384"></a><span class="sd">                `len(lattice_sizes)` tensors of same shape `(batch_size, ..., units, 1)`</span>
<a id="__codelineno-0-385" name="__codelineno-0-385"></a>
<a id="__codelineno-0-386" name="__codelineno-0-386"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-387" name="__codelineno-0-387"></a><span class="sd">            `torch.Tensor` of shape `(batch_size, ..., units)` containing interpolated</span>
<a id="__codelineno-0-388" name="__codelineno-0-388"></a><span class="sd">            value(s).</span>
<a id="__codelineno-0-389" name="__codelineno-0-389"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-390" name="__codelineno-0-390"></a>        <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_hypercube_interpolation_weights</span><span class="p">(</span>
<a id="__codelineno-0-391" name="__codelineno-0-391"></a>            <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">clip_inputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">clip_inputs</span>
<a id="__codelineno-0-392" name="__codelineno-0-392"></a>        <span class="p">)</span>
<a id="__codelineno-0-393" name="__codelineno-0-393"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-394" name="__codelineno-0-394"></a>            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">interpolation_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
<a id="__codelineno-0-395" name="__codelineno-0-395"></a>
<a id="__codelineno-0-396" name="__codelineno-0-396"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">interpolation_weights</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">t</span><span class="p">(),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-397" name="__codelineno-0-397"></a>
<a id="__codelineno-0-398" name="__codelineno-0-398"></a>    <span class="k">def</span> <span class="nf">_compute_hypercube_interpolation_weights</span><span class="p">(</span>
<a id="__codelineno-0-399" name="__codelineno-0-399"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">clip_inputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<a id="__codelineno-0-400" name="__codelineno-0-400"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-401" name="__codelineno-0-401"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes weights for hypercube lattice interpolation.</span>
<a id="__codelineno-0-402" name="__codelineno-0-402"></a>
<a id="__codelineno-0-403" name="__codelineno-0-403"></a><span class="sd">        For each n-dim unit in &quot;inputs,&quot; the weights matrix will generate the weights</span>
<a id="__codelineno-0-404" name="__codelineno-0-404"></a><span class="sd">        corresponding to the unit&#39;s location within its surrounding hypercube. These</span>
<a id="__codelineno-0-405" name="__codelineno-0-405"></a><span class="sd">        weights can then be multiplied by the lattice layer&#39;s kernel to compute the</span>
<a id="__codelineno-0-406" name="__codelineno-0-406"></a><span class="sd">        actual hypercube interpolation. Specifically, the outer product of the set</span>
<a id="__codelineno-0-407" name="__codelineno-0-407"></a><span class="sd">        `(1-x_i, x_i)` for all x_i in input unit x calculates the weights for each</span>
<a id="__codelineno-0-408" name="__codelineno-0-408"></a><span class="sd">        vertex in the surrounding hypercube, and every other vertex in the lattice is</span>
<a id="__codelineno-0-409" name="__codelineno-0-409"></a><span class="sd">        set to zero since it is not used. In addition, for consecutive dimensions of</span>
<a id="__codelineno-0-410" name="__codelineno-0-410"></a><span class="sd">        equal size in the lattice, broadcasting is used to speed up calculations.</span>
<a id="__codelineno-0-411" name="__codelineno-0-411"></a>
<a id="__codelineno-0-412" name="__codelineno-0-412"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-413" name="__codelineno-0-413"></a><span class="sd">            inputs: torch.Tensor of shape `(batch_size, ..., len(lattice_sizes)` or list</span>
<a id="__codelineno-0-414" name="__codelineno-0-414"></a><span class="sd">                of `len(lattice_sizes)` tensors of same shape `(batch_size, ..., 1)`</span>
<a id="__codelineno-0-415" name="__codelineno-0-415"></a><span class="sd">            clip_inputs: Boolean to determine whether input values outside lattice</span>
<a id="__codelineno-0-416" name="__codelineno-0-416"></a><span class="sd">                bounds should be clipped to the min or max supported values.</span>
<a id="__codelineno-0-417" name="__codelineno-0-417"></a>
<a id="__codelineno-0-418" name="__codelineno-0-418"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-419" name="__codelineno-0-419"></a><span class="sd">            `torch.Tensor` of shape `(batch_size, ..., prod(lattice_sizes))` containing</span>
<a id="__codelineno-0-420" name="__codelineno-0-420"></a><span class="sd">            the weights which can be matrix multiplied with the kernel to perform</span>
<a id="__codelineno-0-421" name="__codelineno-0-421"></a><span class="sd">            hypercube interpolation.</span>
<a id="__codelineno-0-422" name="__codelineno-0-422"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-423" name="__codelineno-0-423"></a>        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<a id="__codelineno-0-424" name="__codelineno-0-424"></a>            <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>
<a id="__codelineno-0-425" name="__codelineno-0-425"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-426" name="__codelineno-0-426"></a>            <span class="n">input_dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
<a id="__codelineno-0-427" name="__codelineno-0-427"></a>
<a id="__codelineno-0-428" name="__codelineno-0-428"></a>        <span class="c1"># Special case: 2^d lattice with input passed in as a single tensor</span>
<a id="__codelineno-0-429" name="__codelineno-0-429"></a>        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span><span class="n">size</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span>
<a id="__codelineno-0-430" name="__codelineno-0-430"></a>            <span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span>
<a id="__codelineno-0-431" name="__codelineno-0-431"></a>        <span class="p">):</span>
<a id="__codelineno-0-432" name="__codelineno-0-432"></a>            <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">),</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-433" name="__codelineno-0-433"></a>            <span class="k">if</span> <span class="n">clip_inputs</span><span class="p">:</span>
<a id="__codelineno-0-434" name="__codelineno-0-434"></a>                <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-435" name="__codelineno-0-435"></a>            <span class="n">one_d_interpolation_weights</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">))</span>
<a id="__codelineno-0-436" name="__codelineno-0-436"></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_outer_operation</span><span class="p">(</span><span class="n">one_d_interpolation_weights</span><span class="p">)</span>
<a id="__codelineno-0-437" name="__codelineno-0-437"></a>
<a id="__codelineno-0-438" name="__codelineno-0-438"></a>        <span class="k">if</span> <span class="n">clip_inputs</span><span class="p">:</span>
<a id="__codelineno-0-439" name="__codelineno-0-439"></a>            <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clip_onto_lattice_range</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<a id="__codelineno-0-440" name="__codelineno-0-440"></a>
<a id="__codelineno-0-441" name="__codelineno-0-441"></a>        <span class="c1"># Set up buckets of consecutive equal dimensions for broadcasting later</span>
<a id="__codelineno-0-442" name="__codelineno-0-442"></a>        <span class="n">dim_keypoints</span> <span class="o">=</span> <span class="p">{}</span>
<a id="__codelineno-0-443" name="__codelineno-0-443"></a>        <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">):</span>
<a id="__codelineno-0-444" name="__codelineno-0-444"></a>            <span class="n">dim_keypoints</span><span class="p">[</span><span class="n">dim_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-445" name="__codelineno-0-445"></a>                <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">dim_size</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">input_dtype</span>
<a id="__codelineno-0-446" name="__codelineno-0-446"></a>            <span class="p">)</span>
<a id="__codelineno-0-447" name="__codelineno-0-447"></a>        <span class="n">bucketized_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bucketize_consecutive_equal_dims</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<a id="__codelineno-0-448" name="__codelineno-0-448"></a>        <span class="n">one_d_interpolation_weights</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-449" name="__codelineno-0-449"></a>
<a id="__codelineno-0-450" name="__codelineno-0-450"></a>        <span class="k">for</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">bucket_size</span><span class="p">,</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="n">bucketized_inputs</span><span class="p">:</span>
<a id="__codelineno-0-451" name="__codelineno-0-451"></a>            <span class="k">if</span> <span class="n">bucket_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-452" name="__codelineno-0-452"></a>                <span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-453" name="__codelineno-0-453"></a>            <span class="n">distance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tensor</span> <span class="o">-</span> <span class="n">dim_keypoints</span><span class="p">[</span><span class="n">dim_size</span><span class="p">])</span>
<a id="__codelineno-0-454" name="__codelineno-0-454"></a>            <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
<a id="__codelineno-0-455" name="__codelineno-0-455"></a>                <span class="n">distance</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">distance</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-456" name="__codelineno-0-456"></a>            <span class="p">)</span>
<a id="__codelineno-0-457" name="__codelineno-0-457"></a>            <span class="k">if</span> <span class="n">bucket_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-458" name="__codelineno-0-458"></a>                <span class="n">one_d_interpolation_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<a id="__codelineno-0-459" name="__codelineno-0-459"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-460" name="__codelineno-0-460"></a>                <span class="n">one_d_interpolation_weights</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">))</span>
<a id="__codelineno-0-461" name="__codelineno-0-461"></a>
<a id="__codelineno-0-462" name="__codelineno-0-462"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_batch_outer_operation</span><span class="p">(</span><span class="n">one_d_interpolation_weights</span><span class="p">)</span>
<a id="__codelineno-0-463" name="__codelineno-0-463"></a>
<a id="__codelineno-0-464" name="__codelineno-0-464"></a>    <span class="nd">@staticmethod</span>
<a id="__codelineno-0-465" name="__codelineno-0-465"></a>    <span class="k">def</span> <span class="nf">_batch_outer_operation</span><span class="p">(</span>
<a id="__codelineno-0-466" name="__codelineno-0-466"></a>        <span class="n">list_of_tensors</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-467" name="__codelineno-0-467"></a>        <span class="n">operation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-468" name="__codelineno-0-468"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-469" name="__codelineno-0-469"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes the flattened outer product of a list of tensors.</span>
<a id="__codelineno-0-470" name="__codelineno-0-470"></a>
<a id="__codelineno-0-471" name="__codelineno-0-471"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-472" name="__codelineno-0-472"></a><span class="sd">            list_of_tensors: List of tensors of same shape `(batch_size, ..., k[i])`</span>
<a id="__codelineno-0-473" name="__codelineno-0-473"></a><span class="sd">                where everything except `k_i` matches.</span>
<a id="__codelineno-0-474" name="__codelineno-0-474"></a><span class="sd">            operation: A torch operation which supports broadcasting to be applied. If</span>
<a id="__codelineno-0-475" name="__codelineno-0-475"></a><span class="sd">                `None` is provided, this will apply `torch.mul` for the first several</span>
<a id="__codelineno-0-476" name="__codelineno-0-476"></a><span class="sd">                tensors and `torch.matmul` for the remaining tensors.</span>
<a id="__codelineno-0-477" name="__codelineno-0-477"></a>
<a id="__codelineno-0-478" name="__codelineno-0-478"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-479" name="__codelineno-0-479"></a><span class="sd">            `torch.Tensor` of shape `(batch_size, ..., k_i * k_j * ...)` containing a</span>
<a id="__codelineno-0-480" name="__codelineno-0-480"></a><span class="sd">            flattened version of the outer product.</span>
<a id="__codelineno-0-481" name="__codelineno-0-481"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-482" name="__codelineno-0-482"></a>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">list_of_tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-483" name="__codelineno-0-483"></a>            <span class="k">return</span> <span class="n">list_of_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-484" name="__codelineno-0-484"></a>
<a id="__codelineno-0-485" name="__codelineno-0-485"></a>        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">list_of_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-486" name="__codelineno-0-486"></a>
<a id="__codelineno-0-487" name="__codelineno-0-487"></a>        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">list_of_tensors</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
<a id="__codelineno-0-488" name="__codelineno-0-488"></a>            <span class="k">if</span> <span class="ow">not</span> <span class="n">operation</span><span class="p">:</span>
<a id="__codelineno-0-489" name="__codelineno-0-489"></a>                <span class="n">op</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span> <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">6</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span>
<a id="__codelineno-0-490" name="__codelineno-0-490"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-491" name="__codelineno-0-491"></a>                <span class="n">op</span> <span class="o">=</span> <span class="n">operation</span>
<a id="__codelineno-0-492" name="__codelineno-0-492"></a>
<a id="__codelineno-0-493" name="__codelineno-0-493"></a>            <span class="n">result</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">))</span>
<a id="__codelineno-0-494" name="__codelineno-0-494"></a>            <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="k">for</span> <span class="n">size</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
<a id="__codelineno-0-495" name="__codelineno-0-495"></a>            <span class="n">new_shape</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<a id="__codelineno-0-496" name="__codelineno-0-496"></a>            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">list_of_tensors</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">:</span>
<a id="__codelineno-0-497" name="__codelineno-0-497"></a>                <span class="n">new_shape</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-498" name="__codelineno-0-498"></a>            <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
<a id="__codelineno-0-499" name="__codelineno-0-499"></a>
<a id="__codelineno-0-500" name="__codelineno-0-500"></a>        <span class="k">return</span> <span class="n">result</span>
<a id="__codelineno-0-501" name="__codelineno-0-501"></a>
<a id="__codelineno-0-502" name="__codelineno-0-502"></a>    <span class="nd">@overload</span>
<a id="__codelineno-0-503" name="__codelineno-0-503"></a>    <span class="k">def</span> <span class="nf">_clip_onto_lattice_range</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-504" name="__codelineno-0-504"></a>        <span class="o">...</span>
<a id="__codelineno-0-505" name="__codelineno-0-505"></a>
<a id="__codelineno-0-506" name="__codelineno-0-506"></a>    <span class="nd">@overload</span>
<a id="__codelineno-0-507" name="__codelineno-0-507"></a>    <span class="k">def</span> <span class="nf">_clip_onto_lattice_range</span><span class="p">(</span>
<a id="__codelineno-0-508" name="__codelineno-0-508"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<a id="__codelineno-0-509" name="__codelineno-0-509"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-0-510" name="__codelineno-0-510"></a>        <span class="o">...</span>
<a id="__codelineno-0-511" name="__codelineno-0-511"></a>
<a id="__codelineno-0-512" name="__codelineno-0-512"></a>    <span class="k">def</span> <span class="nf">_clip_onto_lattice_range</span><span class="p">(</span>
<a id="__codelineno-0-513" name="__codelineno-0-513"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-514" name="__codelineno-0-514"></a>        <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
<a id="__codelineno-0-515" name="__codelineno-0-515"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<a id="__codelineno-0-516" name="__codelineno-0-516"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Clips inputs onto valid input range for given lattice_sizes.</span>
<a id="__codelineno-0-517" name="__codelineno-0-517"></a>
<a id="__codelineno-0-518" name="__codelineno-0-518"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-519" name="__codelineno-0-519"></a><span class="sd">            inputs: `inputs` argument of `_compute_interpolation_weights()`.</span>
<a id="__codelineno-0-520" name="__codelineno-0-520"></a>
<a id="__codelineno-0-521" name="__codelineno-0-521"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-522" name="__codelineno-0-522"></a><span class="sd">            `torch.Tensor` of shape `inputs` with values within range</span>
<a id="__codelineno-0-523" name="__codelineno-0-523"></a><span class="sd">            `[0, dim_size - 1]`.</span>
<a id="__codelineno-0-524" name="__codelineno-0-524"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-525" name="__codelineno-0-525"></a>        <span class="n">clipped_inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<a id="__codelineno-0-526" name="__codelineno-0-526"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<a id="__codelineno-0-527" name="__codelineno-0-527"></a>            <span class="n">upper_bounds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-528" name="__codelineno-0-528"></a>                <span class="p">[</span><span class="n">dim_size</span> <span class="o">-</span> <span class="mf">1.0</span> <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">]</span>
<a id="__codelineno-0-529" name="__codelineno-0-529"></a>            <span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-530" name="__codelineno-0-530"></a>            <span class="n">clipped_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
<a id="__codelineno-0-531" name="__codelineno-0-531"></a>                <span class="n">inputs</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">upper_bounds</span><span class="p">),</span> <span class="nb">max</span><span class="o">=</span><span class="n">upper_bounds</span>
<a id="__codelineno-0-532" name="__codelineno-0-532"></a>            <span class="p">)</span>
<a id="__codelineno-0-533" name="__codelineno-0-533"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-534" name="__codelineno-0-534"></a>            <span class="n">dim_upper_bounds</span> <span class="o">=</span> <span class="p">{}</span>
<a id="__codelineno-0-535" name="__codelineno-0-535"></a>            <span class="k">for</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">):</span>
<a id="__codelineno-0-536" name="__codelineno-0-536"></a>                <span class="n">dim_upper_bounds</span><span class="p">[</span><span class="n">dim_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-537" name="__codelineno-0-537"></a>                    <span class="n">dim_size</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>
<a id="__codelineno-0-538" name="__codelineno-0-538"></a>                <span class="p">)</span>
<a id="__codelineno-0-539" name="__codelineno-0-539"></a>            <span class="n">dim_lower_bound</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<a id="__codelineno-0-540" name="__codelineno-0-540"></a>
<a id="__codelineno-0-541" name="__codelineno-0-541"></a>            <span class="n">clipped_inputs</span> <span class="o">=</span> <span class="p">[</span>
<a id="__codelineno-0-542" name="__codelineno-0-542"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span>
<a id="__codelineno-0-543" name="__codelineno-0-543"></a>                    <span class="n">one_d_input</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">dim_lower_bound</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">dim_upper_bounds</span><span class="p">[</span><span class="n">dim_size</span><span class="p">]</span>
<a id="__codelineno-0-544" name="__codelineno-0-544"></a>                <span class="p">)</span>
<a id="__codelineno-0-545" name="__codelineno-0-545"></a>                <span class="k">for</span> <span class="n">one_d_input</span><span class="p">,</span> <span class="n">dim_size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-546" name="__codelineno-0-546"></a>            <span class="p">]</span>
<a id="__codelineno-0-547" name="__codelineno-0-547"></a>
<a id="__codelineno-0-548" name="__codelineno-0-548"></a>        <span class="k">return</span> <span class="n">clipped_inputs</span>
<a id="__codelineno-0-549" name="__codelineno-0-549"></a>
<a id="__codelineno-0-550" name="__codelineno-0-550"></a>    <span class="k">def</span> <span class="nf">_bucketize_consecutive_equal_dims</span><span class="p">(</span>
<a id="__codelineno-0-551" name="__codelineno-0-551"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-552" name="__codelineno-0-552"></a>        <span class="n">inputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
<a id="__codelineno-0-553" name="__codelineno-0-553"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
<a id="__codelineno-0-554" name="__codelineno-0-554"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates buckets of equal sized dimensions for broadcasting ops.</span>
<a id="__codelineno-0-555" name="__codelineno-0-555"></a>
<a id="__codelineno-0-556" name="__codelineno-0-556"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-557" name="__codelineno-0-557"></a><span class="sd">            inputs: `inputs` argument of `_compute_interpolation_weights()`.</span>
<a id="__codelineno-0-558" name="__codelineno-0-558"></a>
<a id="__codelineno-0-559" name="__codelineno-0-559"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-560" name="__codelineno-0-560"></a><span class="sd">            An `Iterable` containing `(torch.Tensor, int, int)` where the tensor</span>
<a id="__codelineno-0-561" name="__codelineno-0-561"></a><span class="sd">            contains individual values from &quot;inputs&quot; corresponding to its bucket, the</span>
<a id="__codelineno-0-562" name="__codelineno-0-562"></a><span class="sd">            first `int` is bucket size, and the second `int` is size of the dimension of</span>
<a id="__codelineno-0-563" name="__codelineno-0-563"></a><span class="sd">            the bucket.</span>
<a id="__codelineno-0-564" name="__codelineno-0-564"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-565" name="__codelineno-0-565"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
<a id="__codelineno-0-566" name="__codelineno-0-566"></a>            <span class="n">bucket_sizes</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-567" name="__codelineno-0-567"></a>            <span class="n">bucket_dim_sizes</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-568" name="__codelineno-0-568"></a>            <span class="n">current_size</span> <span class="o">=</span> <span class="mi">1</span>
<a id="__codelineno-0-569" name="__codelineno-0-569"></a>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)):</span>
<a id="__codelineno-0-570" name="__codelineno-0-570"></a>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]:</span>
<a id="__codelineno-0-571" name="__codelineno-0-571"></a>                    <span class="n">bucket_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_size</span><span class="p">)</span>
<a id="__codelineno-0-572" name="__codelineno-0-572"></a>                    <span class="n">bucket_dim_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-573" name="__codelineno-0-573"></a>                    <span class="n">current_size</span> <span class="o">=</span> <span class="mi">1</span>
<a id="__codelineno-0-574" name="__codelineno-0-574"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-575" name="__codelineno-0-575"></a>                    <span class="n">current_size</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-0-576" name="__codelineno-0-576"></a>            <span class="n">bucket_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_size</span><span class="p">)</span>
<a id="__codelineno-0-577" name="__codelineno-0-577"></a>            <span class="n">bucket_dim_sizes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-578" name="__codelineno-0-578"></a>            <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">split_size_or_sections</span><span class="o">=</span><span class="n">bucket_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-579" name="__codelineno-0-579"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-580" name="__codelineno-0-580"></a>            <span class="n">bucket_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-581" name="__codelineno-0-581"></a>            <span class="n">bucket_dim_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span>
<a id="__codelineno-0-582" name="__codelineno-0-582"></a>
<a id="__codelineno-0-583" name="__codelineno-0-583"></a>        <span class="k">return</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">bucket_sizes</span><span class="p">,</span> <span class="n">bucket_dim_sizes</span><span class="p">)</span>
<a id="__codelineno-0-584" name="__codelineno-0-584"></a>
<a id="__codelineno-0-585" name="__codelineno-0-585"></a>    <span class="k">def</span> <span class="nf">_approximately_project_monotonicity</span><span class="p">(</span>
<a id="__codelineno-0-586" name="__codelineno-0-586"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-587" name="__codelineno-0-587"></a>        <span class="n">weights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
<a id="__codelineno-0-588" name="__codelineno-0-588"></a>        <span class="n">lattice_sizes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
<a id="__codelineno-0-589" name="__codelineno-0-589"></a>        <span class="n">monotonicities</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Monotonicity</span><span class="p">]],</span>
<a id="__codelineno-0-590" name="__codelineno-0-590"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-591" name="__codelineno-0-591"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Projects weights of lattice to meet monotonicity constraints.</span>
<a id="__codelineno-0-592" name="__codelineno-0-592"></a>
<a id="__codelineno-0-593" name="__codelineno-0-593"></a><span class="sd">        Note that this projection is an approximation which guarantees monotonicity</span>
<a id="__codelineno-0-594" name="__codelineno-0-594"></a><span class="sd">        constraints but is not an exact projection with respect to the L2 norm.</span>
<a id="__codelineno-0-595" name="__codelineno-0-595"></a>
<a id="__codelineno-0-596" name="__codelineno-0-596"></a><span class="sd">        Algorithm:</span>
<a id="__codelineno-0-597" name="__codelineno-0-597"></a><span class="sd">        1. `max_projection`: For each vertex V in the lattice, the weight is adjusted to</span>
<a id="__codelineno-0-598" name="__codelineno-0-598"></a><span class="sd">        be the maximum of all weights of vertices X such that X has all coordinates</span>
<a id="__codelineno-0-599" name="__codelineno-0-599"></a><span class="sd">        less than or equal to V in monotonic dimensions.</span>
<a id="__codelineno-0-600" name="__codelineno-0-600"></a>
<a id="__codelineno-0-601" name="__codelineno-0-601"></a><span class="sd">        2. `half_projection`: We adjust the weights to be the average of the original</span>
<a id="__codelineno-0-602" name="__codelineno-0-602"></a><span class="sd">        weights and the `max_projection` weights.</span>
<a id="__codelineno-0-603" name="__codelineno-0-603"></a>
<a id="__codelineno-0-604" name="__codelineno-0-604"></a><span class="sd">        3. `min_projection`: For each vertex V in the lattice, the weight is adjusted</span>
<a id="__codelineno-0-605" name="__codelineno-0-605"></a><span class="sd">        based on the `half_projection` to be the minimum of all weights of vertices X</span>
<a id="__codelineno-0-606" name="__codelineno-0-606"></a><span class="sd">        such that V has all coordinates less than or equal to X in monotonic dimensions.</span>
<a id="__codelineno-0-607" name="__codelineno-0-607"></a>
<a id="__codelineno-0-608" name="__codelineno-0-608"></a><span class="sd">        This algorithm ensures that weights conform to the monotonicity constraints</span>
<a id="__codelineno-0-609" name="__codelineno-0-609"></a><span class="sd">        while getting closer to a true projection by adjusting both up/downwards.</span>
<a id="__codelineno-0-610" name="__codelineno-0-610"></a>
<a id="__codelineno-0-611" name="__codelineno-0-611"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-612" name="__codelineno-0-612"></a><span class="sd">            weights: `torch.Tensor` of kernel data reshaped into `(lattice_sizes)` if</span>
<a id="__codelineno-0-613" name="__codelineno-0-613"></a><span class="sd">                `units == 1` or `(lattice_sizes, units)` if `units &gt; 1`.</span>
<a id="__codelineno-0-614" name="__codelineno-0-614"></a><span class="sd">            lattice_sizes: List of size of each dimension of lattice, but for</span>
<a id="__codelineno-0-615" name="__codelineno-0-615"></a><span class="sd">                `units &gt; 1`, `units` is appended to the end for computation purposes.</span>
<a id="__codelineno-0-616" name="__codelineno-0-616"></a><span class="sd">            monotonicities: List of `None` or `Monotonicity.INCREASING`</span>
<a id="__codelineno-0-617" name="__codelineno-0-617"></a><span class="sd">                of length `len(lattice_sizes)` for `units == 1` or</span>
<a id="__codelineno-0-618" name="__codelineno-0-618"></a><span class="sd">                `len(lattice_sizes)+1` if `units &gt; 1` specifying monotonicity of each</span>
<a id="__codelineno-0-619" name="__codelineno-0-619"></a><span class="sd">                feature of lattice.</span>
<a id="__codelineno-0-620" name="__codelineno-0-620"></a>
<a id="__codelineno-0-621" name="__codelineno-0-621"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-622" name="__codelineno-0-622"></a><span class="sd">            `torch.Tensor` of shape `self.kernel` with updated weights which meet</span>
<a id="__codelineno-0-623" name="__codelineno-0-623"></a><span class="sd">            monotonicity constraints.</span>
<a id="__codelineno-0-624" name="__codelineno-0-624"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-625" name="__codelineno-0-625"></a>        <span class="n">max_projection</span> <span class="o">=</span> <span class="n">weights</span>
<a id="__codelineno-0-626" name="__codelineno-0-626"></a>        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lattice_sizes</span><span class="p">)):</span>
<a id="__codelineno-0-627" name="__codelineno-0-627"></a>            <span class="k">if</span> <span class="n">monotonicities</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-628" name="__codelineno-0-628"></a>                <span class="k">continue</span>
<a id="__codelineno-0-629" name="__codelineno-0-629"></a>            <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">max_projection</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
<a id="__codelineno-0-630" name="__codelineno-0-630"></a>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)):</span>
<a id="__codelineno-0-631" name="__codelineno-0-631"></a>                <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-632" name="__codelineno-0-632"></a>            <span class="n">max_projection</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
<a id="__codelineno-0-633" name="__codelineno-0-633"></a>
<a id="__codelineno-0-634" name="__codelineno-0-634"></a>        <span class="n">half_projection</span> <span class="o">=</span> <span class="p">(</span><span class="n">weights</span> <span class="o">+</span> <span class="n">max_projection</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
<a id="__codelineno-0-635" name="__codelineno-0-635"></a>
<a id="__codelineno-0-636" name="__codelineno-0-636"></a>        <span class="n">min_projection</span> <span class="o">=</span> <span class="n">half_projection</span>
<a id="__codelineno-0-637" name="__codelineno-0-637"></a>        <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lattice_sizes</span><span class="p">)):</span>
<a id="__codelineno-0-638" name="__codelineno-0-638"></a>            <span class="k">if</span> <span class="n">monotonicities</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-639" name="__codelineno-0-639"></a>                <span class="k">continue</span>
<a id="__codelineno-0-640" name="__codelineno-0-640"></a>            <span class="n">layers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">min_projection</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
<a id="__codelineno-0-641" name="__codelineno-0-641"></a>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
<a id="__codelineno-0-642" name="__codelineno-0-642"></a>                <span class="c1"># Compute cumulative minimum in reverse order</span>
<a id="__codelineno-0-643" name="__codelineno-0-643"></a>                <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-644" name="__codelineno-0-644"></a>            <span class="n">min_projection</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
<a id="__codelineno-0-645" name="__codelineno-0-645"></a>
<a id="__codelineno-0-646" name="__codelineno-0-646"></a>        <span class="k">return</span> <span class="n">min_projection</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.Lattice.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">lattice_sizes</span><span class="p">,</span> <span class="n">output_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_init</span><span class="o">=</span><span class="n">LatticeInit</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">,</span> <span class="n">monotonicities</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clip_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="n">Interpolation</span><span class="o">.</span><span class="n">HYPERCUBE</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initializes an instance of 'Lattice'.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>lattice_sizes</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[list[int], tuple[int]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>List or tuple of size of lattice along each dimension.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>output_min</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Minimum output value for weights at vertices of lattice.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>output_max</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum output value for weights at vertices of lattice.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>kernel_init</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="pytorch_lattice.enums.LatticeInit" href="../enums/#pytorch_lattice.enums.LatticeInit">LatticeInit</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Initialization scheme to use for the kernel.</p>
            </div>
          </td>
          <td>
                <code><span title="pytorch_lattice.enums.LatticeInit.LINEAR">LINEAR</span></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>monotonicities</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[list[<span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="pytorch_lattice.enums.Monotonicity" href="../enums/#pytorch_lattice.enums.Monotonicity">Monotonicity</a>]]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>None</code> or list of <code>NONE</code> or
<code>Monotonicity.INCREASING</code> of length <code>len(lattice_sizes)</code> specifying
monotonicity of each feature of lattice. A monotonically decreasing
 feature should use <code>Monotonicity.INCREASING</code> in the lattice layer but
<code>Monotonicity.DECREASING</code> in the calibrator.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>clip_inputs</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether input points should be clipped to the range of lattice.</p>
            </div>
          </td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>interpolation</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="pytorch_lattice.enums.Interpolation" href="../enums/#pytorch_lattice.enums.Interpolation">Interpolation</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Interpolation scheme for a given input.</p>
            </div>
          </td>
          <td>
                <code><span title="pytorch_lattice.enums.Interpolation.HYPERCUBE">HYPERCUBE</span></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>units</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Dimensionality of weights stored at each vertex of lattice.</p>
            </div>
          </td>
          <td>
                <code>1</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Raises:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>if <code>kernel_init</code> is invalid.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code>NotImplementedError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Random monotonic initialization not yet implemented.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/lattice.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-0-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-0-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-0-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-0-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-0-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-0-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-0-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-0-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-0-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-0-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-0-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-0-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-0-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-0-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-0-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-0-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-0-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-0-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-0-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-0-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-0-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-0-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-0-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-0-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-0-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-0-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-0-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-0-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-0-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-0-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-0-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-0-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-0-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-0-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-0-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-0-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-0-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-0-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-0-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-0-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-0-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-0-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-0-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-0-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-0-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-0-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-0-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-0-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-0-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-43" name="__codelineno-0-43"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>    <span class="n">lattice_sizes</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>    <span class="n">output_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>    <span class="n">output_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>    <span class="n">kernel_init</span><span class="p">:</span> <span class="n">LatticeInit</span> <span class="o">=</span> <span class="n">LatticeInit</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>    <span class="n">monotonicities</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Monotonicity</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>    <span class="n">clip_inputs</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>    <span class="n">interpolation</span><span class="p">:</span> <span class="n">Interpolation</span> <span class="o">=</span> <span class="n">Interpolation</span><span class="o">.</span><span class="n">HYPERCUBE</span><span class="p">,</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a>    <span class="n">units</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Initializes an instance of &#39;Lattice&#39;.</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">        lattice_sizes: List or tuple of size of lattice along each dimension.</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">        output_min: Minimum output value for weights at vertices of lattice.</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">        output_max: Maximum output value for weights at vertices of lattice.</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a><span class="sd">        kernel_init: Initialization scheme to use for the kernel.</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a><span class="sd">        monotonicities: `None` or list of `NONE` or</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a><span class="sd">            `Monotonicity.INCREASING` of length `len(lattice_sizes)` specifying</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a><span class="sd">            monotonicity of each feature of lattice. A monotonically decreasing</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a><span class="sd">             feature should use `Monotonicity.INCREASING` in the lattice layer but</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a><span class="sd">            `Monotonicity.DECREASING` in the calibrator.</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a><span class="sd">        clip_inputs: Whether input points should be clipped to the range of lattice.</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">        interpolation: Interpolation scheme for a given input.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a><span class="sd">        units: Dimensionality of weights stored at each vertex of lattice.</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a><span class="sd">    Raises:</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a><span class="sd">        ValueError: if `kernel_init` is invalid.</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a><span class="sd">        NotImplementedError: Random monotonic initialization not yet implemented.</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">=</span> <span class="n">output_min</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">=</span> <span class="n">output_max</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">=</span> <span class="n">kernel_init</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">clip_inputs</span> <span class="o">=</span> <span class="n">clip_inputs</span>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">=</span> <span class="n">interpolation</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>    <span class="k">if</span> <span class="n">monotonicities</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span> <span class="o">=</span> <span class="n">monotonicities</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>    <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>        <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a>    <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>        <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_min</span> <span class="o">+</span> <span class="mf">4.0</span>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a>    <span class="k">elif</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>        <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_max</span> <span class="o">-</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a>        <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span> <span class="o">=</span> <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>    <span class="k">def</span> <span class="nf">initialize_kernel</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">==</span> <span class="n">LatticeInit</span><span class="o">.</span><span class="n">LINEAR</span><span class="p">:</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear_initializer</span><span class="p">()</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">==</span> <span class="n">LatticeInit</span><span class="o">.</span><span class="n">RANDOM_MONOTONIC</span><span class="p">:</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>                <span class="s2">&quot;Random monotonic initialization not yet implemented.&quot;</span>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a>            <span class="p">)</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown kernel init: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initialize_kernel</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.Lattice.apply_constraints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">apply_constraints</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Aggregate function for enforcing constraints of lattice.</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/lattice.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-135" name="__codelineno-0-135"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a><span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Aggregate function for enforcing constraints of lattice.&quot;&quot;&quot;</span>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>    <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_count_non_zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">):</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a>        <span class="n">lattice_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>        <span class="n">monotonicities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a>            <span class="n">lattice_sizes</span> <span class="o">=</span> <span class="n">lattice_sizes</span> <span class="o">+</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)]</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a>                <span class="n">monotonicities</span> <span class="o">=</span> <span class="n">monotonicities</span> <span class="o">+</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_approximately_project_monotonicity</span><span class="p">(</span>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a>            <span class="n">weights</span><span class="p">,</span> <span class="n">lattice_sizes</span><span class="p">,</span> <span class="n">monotonicities</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a>        <span class="p">)</span>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span><span class="p">)</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp_max</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span><span class="p">)</span>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.Lattice.assert_constraints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">assert_constraints</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Asserts that layer satisfies specified constraints.</p>
<p>This checks that weights follow monotonicity and bounds constraints.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>eps</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>the margin of error allowed</p>
            </div>
          </td>
          <td>
                <code>1e-06</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A list of dicts describing violated constraints including indices of</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>monotonicity violations. If no constraints violated, the list will be empty.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/lattice.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span>
<span class="normal"><a href="#__codelineno-0-179">179</a></span>
<span class="normal"><a href="#__codelineno-0-180">180</a></span>
<span class="normal"><a href="#__codelineno-0-181">181</a></span>
<span class="normal"><a href="#__codelineno-0-182">182</a></span>
<span class="normal"><a href="#__codelineno-0-183">183</a></span>
<span class="normal"><a href="#__codelineno-0-184">184</a></span>
<span class="normal"><a href="#__codelineno-0-185">185</a></span>
<span class="normal"><a href="#__codelineno-0-186">186</a></span>
<span class="normal"><a href="#__codelineno-0-187">187</a></span>
<span class="normal"><a href="#__codelineno-0-188">188</a></span>
<span class="normal"><a href="#__codelineno-0-189">189</a></span>
<span class="normal"><a href="#__codelineno-0-190">190</a></span>
<span class="normal"><a href="#__codelineno-0-191">191</a></span>
<span class="normal"><a href="#__codelineno-0-192">192</a></span>
<span class="normal"><a href="#__codelineno-0-193">193</a></span>
<span class="normal"><a href="#__codelineno-0-194">194</a></span>
<span class="normal"><a href="#__codelineno-0-195">195</a></span>
<span class="normal"><a href="#__codelineno-0-196">196</a></span>
<span class="normal"><a href="#__codelineno-0-197">197</a></span>
<span class="normal"><a href="#__codelineno-0-198">198</a></span>
<span class="normal"><a href="#__codelineno-0-199">199</a></span>
<span class="normal"><a href="#__codelineno-0-200">200</a></span>
<span class="normal"><a href="#__codelineno-0-201">201</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-160" name="__codelineno-0-160"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a><span class="k">def</span> <span class="nf">assert_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Asserts that layer satisfies specified constraints.</span>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a><span class="sd">    This checks that weights follow monotonicity and bounds constraints.</span>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a><span class="sd">        eps: the margin of error allowed</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a><span class="sd">        A list of dicts describing violated constraints including indices of</span>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a><span class="sd">        monotonicity violations. If no constraints violated, the list will be empty.</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>    <span class="n">lattice_sizes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lattice_sizes</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>    <span class="n">monotonicities</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>    <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a>    <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-179" name="__codelineno-0-179"></a>        <span class="n">lattice_sizes</span> <span class="o">=</span> <span class="n">lattice_sizes</span> <span class="o">+</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
<a id="__codelineno-0-180" name="__codelineno-0-180"></a>        <span class="k">if</span> <span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-181" name="__codelineno-0-181"></a>            <span class="n">monotonicities</span> <span class="o">=</span> <span class="n">monotonicities</span> <span class="o">+</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span>
<a id="__codelineno-0-182" name="__codelineno-0-182"></a>
<a id="__codelineno-0-183" name="__codelineno-0-183"></a>    <span class="c1"># Reshape weights to match lattice sizes</span>
<a id="__codelineno-0-184" name="__codelineno-0-184"></a>    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">lattice_sizes</span><span class="p">)</span>
<a id="__codelineno-0-185" name="__codelineno-0-185"></a>
<a id="__codelineno-0-186" name="__codelineno-0-186"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">monotonicities</span> <span class="ow">or</span> <span class="p">[])):</span>
<a id="__codelineno-0-187" name="__codelineno-0-187"></a>        <span class="k">if</span> <span class="n">monotonicities</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">:</span>
<a id="__codelineno-0-188" name="__codelineno-0-188"></a>            <span class="k">continue</span>
<a id="__codelineno-0-189" name="__codelineno-0-189"></a>        <span class="n">weights_layers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
<a id="__codelineno-0-190" name="__codelineno-0-190"></a>
<a id="__codelineno-0-191" name="__codelineno-0-191"></a>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights_layers</span><span class="p">)):</span>
<a id="__codelineno-0-192" name="__codelineno-0-192"></a>            <span class="n">diff</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">weights_layers</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">weights_layers</span><span class="p">[</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-193" name="__codelineno-0-193"></a>            <span class="k">if</span> <span class="n">diff</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-194" name="__codelineno-0-194"></a>                <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Monotonicity violated at feature index </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-195" name="__codelineno-0-195"></a>
<a id="__codelineno-0-196" name="__codelineno-0-196"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">+</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-197" name="__codelineno-0-197"></a>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Max weight greater than output_max.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-198" name="__codelineno-0-198"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">-</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-199" name="__codelineno-0-199"></a>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Min weight less than output_min.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-200" name="__codelineno-0-200"></a>
<a id="__codelineno-0-201" name="__codelineno-0-201"></a>    <span class="k">return</span> <span class="n">messages</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.Lattice.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Calculates interpolation from input, using method of self.interpolation.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>x</code></td>
          <td>
                <code><span title="typing.Union">Union</span>[<span title="torch.Tensor">Tensor</span>, list[<span title="torch.Tensor">Tensor</span>]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>input tensor. If <code>units == 1</code>, tensor of shape:
<code>(batch_size, ..., len(lattice_size))</code> or list of <code>len(lattice_sizes)</code>
tensors of same shape: <code>(batch_size, ..., 1)</code>. If <code>units &gt; 1</code>, tensor of
shape <code>(batch_size, ..., units, len(lattice_sizes))</code> or list of
<code>len(lattice_sizes)</code> tensors OF same shape <code>(batch_size, ..., units, 1)</code></p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>torch.Tensor of shape <code>(batch_size, ..., units)</code> containing interpolated</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>values.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Raises:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If the type of interpolation is unknown.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/lattice.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-111" name="__codelineno-0-111"></a><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculates interpolation from input, using method of self.interpolation.</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a><span class="sd">        x: input tensor. If `units == 1`, tensor of shape:</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a><span class="sd">            `(batch_size, ..., len(lattice_size))` or list of `len(lattice_sizes)`</span>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a><span class="sd">            tensors of same shape: `(batch_size, ..., 1)`. If `units &gt; 1`, tensor of</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a><span class="sd">            shape `(batch_size, ..., units, len(lattice_sizes))` or list of</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a><span class="sd">            `len(lattice_sizes)` tensors OF same shape `(batch_size, ..., units, 1)`</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a><span class="sd">        torch.Tensor of shape `(batch_size, ..., units)` containing interpolated</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a><span class="sd">        values.</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a><span class="sd">    Raises:</span>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a><span class="sd">        ValueError: If the type of interpolation is unknown.</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a>    <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">xi</span><span class="o">.</span><span class="n">double</span><span class="p">()</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">else</span> <span class="n">x</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">==</span> <span class="n">Interpolation</span><span class="o">.</span><span class="n">HYPERCUBE</span><span class="p">:</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_hypercube_interpolation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span> <span class="o">==</span> <span class="n">Interpolation</span><span class="o">.</span><span class="n">SIMPLEX</span><span class="p">:</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_simplex_interpolation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown interpolation type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">interpolation</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_lattice.layers.Linear" class="doc doc-heading">
          <code>pytorch_lattice.layers.Linear</code>


</h2>


  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pytorch_lattice.constrained_module.ConstrainedModule" href="../constrained_module/#pytorch_lattice.constrained_module.ConstrainedModule">ConstrainedModule</a></code></p>

  
      <p>A constrained linear module.</p>
<p>This module takes an input of shape <code>(batch_size, input_dim)</code> and applied a linear
transformation. The output will have the same shape as the input.</p>



  <p><span class="doc-section-title">Attributes:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.Linear.All">All</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>__init__</code> arguments.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.Linear.kernel">kernel</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>torch.nn.Parameter</code> that stores the linear combination weighting.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.Linear.bias">bias</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>torch.nn.Parameter</code> that stores the bias term. Only available is
<code>use_bias</code> is true.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>
      <p>Example:
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">input_dim</span> <span class="o">=</span> <span class="mi">3</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># shape: (batch_size, input_dim)</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="n">linear</span> <span class="o">=</span> <span class="n">Linear</span><span class="p">(</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">input_dim</span><span class="p">,</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">monotonicities</span><span class="o">=</span><span class="p">[</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>        <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>        <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">,</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>        <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="p">],</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="n">weighted_average</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="p">)</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</code></pre></div></p>

            <details class="quote">
              <summary>Source code in <code>pytorch_lattice/layers/linear.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-15"> 15</a></span>
<span class="normal"><a href="#__codelineno-0-16"> 16</a></span>
<span class="normal"><a href="#__codelineno-0-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-0-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-0-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-0-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-0-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-0-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-0-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-0-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-0-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-0-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-0-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-0-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-0-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-0-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-0-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-0-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-0-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-0-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-0-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-0-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-0-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-0-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-0-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-0-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-0-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-0-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-0-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-0-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-0-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-0-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-0-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-0-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-0-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-0-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-0-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-0-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-0-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-0-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-0-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-0-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-0-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-0-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-0-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-0-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-0-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-0-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-0-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-0-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-0-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-0-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-0-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-0-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-0-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-0-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-0-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-0-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-0-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-0-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-0-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-0-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-0-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-0-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-0-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-0-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-0-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-0-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-0-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-0-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-0-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-0-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-0-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-0-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-0-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-0-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-0-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-0-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-15" name="__codelineno-0-15"></a><span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">ConstrainedModule</span><span class="p">):</span>
<a id="__codelineno-0-16" name="__codelineno-0-16"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;A constrained linear module.</span>
<a id="__codelineno-0-17" name="__codelineno-0-17"></a>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a><span class="sd">    This module takes an input of shape `(batch_size, input_dim)` and applied a linear</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a><span class="sd">    transformation. The output will have the same shape as the input.</span>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a><span class="sd">    Attributes:</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a><span class="sd">        All: `__init__` arguments.</span>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a><span class="sd">        kernel: `torch.nn.Parameter` that stores the linear combination weighting.</span>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a><span class="sd">        bias: `torch.nn.Parameter` that stores the bias term. Only available is</span>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a><span class="sd">            `use_bias` is true.</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a><span class="sd">    Example:</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a><span class="sd">    ```python</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a><span class="sd">    input_dim = 3</span>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a><span class="sd">    inputs = torch.tensor(...)  # shape: (batch_size, input_dim)</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a><span class="sd">    linear = Linear(</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a><span class="sd">        input_dim,</span>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="sd">        monotonicities=[</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a><span class="sd">            None,</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a><span class="sd">            Monotonicity.INCREASING,</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a><span class="sd">            Monotonicity.DECREASING</span>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="sd">        ],</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a><span class="sd">        use_bias=False,</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a><span class="sd">        weighted_average=True,</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a><span class="sd">    )</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a><span class="sd">    outputs = linear(inputs)</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a><span class="sd">    ```</span>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>        <span class="n">monotonicities</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Monotonicity</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>        <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>        <span class="n">weighted_average</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an instance of `Linear`.</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a><span class="sd">            input_dim: The number of inputs that will be combined.</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a><span class="sd">            monotonicities: If provided, specifies the monotonicity of each input</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">                dimension.</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">            use_bias: Whether to use a bias term for the linear combination.</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">            weighted_average: Whether to make the output a weighted average i.e. all</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a><span class="sd">                coefficients are positive and add up to a total of 1.0. No bias term</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a><span class="sd">                will be used, and `use_bias` will be set to false regardless of the</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a><span class="sd">                original value. `monotonicities` will also be set to increasing for all</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a><span class="sd">                input dimensions to ensure that all coefficients are positive.</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a><span class="sd">        Raises:</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a><span class="sd">            ValueError: If monotonicities does not have length input_dim (if provided).</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a>        <span class="k">if</span> <span class="n">monotonicities</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">monotonicities</span><span class="p">)</span> <span class="o">!=</span> <span class="n">input_dim</span><span class="p">:</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Monotonicities, if provided, must have length input_dim.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>            <span class="n">monotonicities</span>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>            <span class="k">if</span> <span class="ow">not</span> <span class="n">weighted_average</span>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>            <span class="k">else</span> <span class="p">[</span><span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_dim</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>        <span class="p">)</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">weighted_average</span> <span class="k">else</span> <span class="kc">False</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">weighted_average</span> <span class="o">=</span> <span class="n">weighted_average</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">input_dim</span><span class="p">)</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>        <span class="k">if</span> <span class="n">use_bias</span><span class="p">:</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Transforms inputs using a linear combination.</span>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a><span class="sd">            x: The input tensor of shape `(batch_size, input_dim)`.</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a><span class="sd">            torch.Tensor of shape `(batch_size, 1)` containing transformed input values.</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a>        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>            <span class="n">result</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>        <span class="k">return</span> <span class="n">result</span>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>    <span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Projects kernel into desired constraints.&quot;&quot;&quot;</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>        <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>            <span class="k">if</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>                <span class="n">increasing_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>                    <span class="p">[</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>                        <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span> <span class="k">else</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">]</span>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a>                        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a>                    <span class="p">]</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>                <span class="p">)</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a>                <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a>                    <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="n">projected_kernel_data</span> <span class="o">*</span> <span class="n">increasing_mask</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>                <span class="p">)</span>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a>            <span class="k">if</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a>                <span class="n">decreasing_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a>                    <span class="p">[</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>                        <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span> <span class="k">else</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">]</span>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a>                        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a>                    <span class="p">]</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a>                <span class="p">)</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a>                <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>                    <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="n">projected_kernel_data</span> <span class="o">*</span> <span class="n">decreasing_mask</span>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a>                <span class="p">)</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_average</span><span class="p">:</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>            <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">projected_kernel_data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>            <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">norm</span> <span class="o">&lt;</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>            <span class="n">projected_kernel_data</span> <span class="o">/=</span> <span class="n">norm</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">projected_kernel_data</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>    <span class="k">def</span> <span class="nf">assert_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Asserts that layer satisfies specified constraints.</span>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a><span class="sd">        This checks that decreasing monotonicity corresponds to negative weights,</span>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="sd">        increasing monotonicity corresponds to positive weights, and weights sum to 1</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a><span class="sd">        for weighted_average=True.</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="sd">            eps: the margin of error allowed</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a><span class="sd">            A list of messages describing violated constraints. If no constraints</span>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">            violated, the list will be empty.</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a>        <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_average</span><span class="p">:</span>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>            <span class="n">total_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>            <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">total_weight</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a>                <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Weights do not sum to 1.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a>            <span class="n">monotonicities_constant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>                <span class="p">[</span>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a>                    <span class="mi">1</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a>                    <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a>                    <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a>                    <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a>                    <span class="k">else</span> <span class="mi">0</span>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>                    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a>                <span class="p">],</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a>                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a>                <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a>            <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a>            <span class="n">violated_monotonicities</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">*</span> <span class="n">monotonicities_constant</span><span class="p">)</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">eps</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a>            <span class="n">violation_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">violated_monotonicities</span><span class="p">)</span>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>            <span class="k">if</span> <span class="n">violation_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>                <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>                    <span class="sa">f</span><span class="s2">&quot;Monotonicity violated at: </span><span class="si">{</span><span class="n">violation_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>                <span class="p">)</span>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a>        <span class="k">return</span> <span class="n">messages</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.Linear.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">monotonicities</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weighted_average</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initializes an instance of <code>Linear</code>.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>input_dim</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The number of inputs that will be combined.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>monotonicities</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[list[<span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="pytorch_lattice.enums.Monotonicity" href="../enums/#pytorch_lattice.enums.Monotonicity">Monotonicity</a>]]]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If provided, specifies the monotonicity of each input
dimension.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>use_bias</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether to use a bias term for the linear combination.</p>
            </div>
          </td>
          <td>
                <code>True</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>weighted_average</code></td>
          <td>
                <code>bool</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Whether to make the output a weighted average i.e. all
coefficients are positive and add up to a total of 1.0. No bias term
will be used, and <code>use_bias</code> will be set to false regardless of the
original value. <code>monotonicities</code> will also be set to increasing for all
input dimensions to ensure that all coefficients are positive.</p>
            </div>
          </td>
          <td>
                <code>False</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Raises:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If monotonicities does not have length input_dim (if provided).</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/linear.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-45">45</a></span>
<span class="normal"><a href="#__codelineno-0-46">46</a></span>
<span class="normal"><a href="#__codelineno-0-47">47</a></span>
<span class="normal"><a href="#__codelineno-0-48">48</a></span>
<span class="normal"><a href="#__codelineno-0-49">49</a></span>
<span class="normal"><a href="#__codelineno-0-50">50</a></span>
<span class="normal"><a href="#__codelineno-0-51">51</a></span>
<span class="normal"><a href="#__codelineno-0-52">52</a></span>
<span class="normal"><a href="#__codelineno-0-53">53</a></span>
<span class="normal"><a href="#__codelineno-0-54">54</a></span>
<span class="normal"><a href="#__codelineno-0-55">55</a></span>
<span class="normal"><a href="#__codelineno-0-56">56</a></span>
<span class="normal"><a href="#__codelineno-0-57">57</a></span>
<span class="normal"><a href="#__codelineno-0-58">58</a></span>
<span class="normal"><a href="#__codelineno-0-59">59</a></span>
<span class="normal"><a href="#__codelineno-0-60">60</a></span>
<span class="normal"><a href="#__codelineno-0-61">61</a></span>
<span class="normal"><a href="#__codelineno-0-62">62</a></span>
<span class="normal"><a href="#__codelineno-0-63">63</a></span>
<span class="normal"><a href="#__codelineno-0-64">64</a></span>
<span class="normal"><a href="#__codelineno-0-65">65</a></span>
<span class="normal"><a href="#__codelineno-0-66">66</a></span>
<span class="normal"><a href="#__codelineno-0-67">67</a></span>
<span class="normal"><a href="#__codelineno-0-68">68</a></span>
<span class="normal"><a href="#__codelineno-0-69">69</a></span>
<span class="normal"><a href="#__codelineno-0-70">70</a></span>
<span class="normal"><a href="#__codelineno-0-71">71</a></span>
<span class="normal"><a href="#__codelineno-0-72">72</a></span>
<span class="normal"><a href="#__codelineno-0-73">73</a></span>
<span class="normal"><a href="#__codelineno-0-74">74</a></span>
<span class="normal"><a href="#__codelineno-0-75">75</a></span>
<span class="normal"><a href="#__codelineno-0-76">76</a></span>
<span class="normal"><a href="#__codelineno-0-77">77</a></span>
<span class="normal"><a href="#__codelineno-0-78">78</a></span>
<span class="normal"><a href="#__codelineno-0-79">79</a></span>
<span class="normal"><a href="#__codelineno-0-80">80</a></span>
<span class="normal"><a href="#__codelineno-0-81">81</a></span>
<span class="normal"><a href="#__codelineno-0-82">82</a></span>
<span class="normal"><a href="#__codelineno-0-83">83</a></span>
<span class="normal"><a href="#__codelineno-0-84">84</a></span>
<span class="normal"><a href="#__codelineno-0-85">85</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-45" name="__codelineno-0-45"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>    <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>    <span class="n">monotonicities</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">Monotonicity</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>    <span class="n">use_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>    <span class="n">weighted_average</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Initializes an instance of `Linear`.</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a><span class="sd">        input_dim: The number of inputs that will be combined.</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a><span class="sd">        monotonicities: If provided, specifies the monotonicity of each input</span>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">            dimension.</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">        use_bias: Whether to use a bias term for the linear combination.</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">        weighted_average: Whether to make the output a weighted average i.e. all</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a><span class="sd">            coefficients are positive and add up to a total of 1.0. No bias term</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a><span class="sd">            will be used, and `use_bias` will be set to false regardless of the</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a><span class="sd">            original value. `monotonicities` will also be set to increasing for all</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a><span class="sd">            input dimensions to ensure that all coefficients are positive.</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a><span class="sd">    Raises:</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a><span class="sd">        ValueError: If monotonicities does not have length input_dim (if provided).</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a>    <span class="k">if</span> <span class="n">monotonicities</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">monotonicities</span><span class="p">)</span> <span class="o">!=</span> <span class="n">input_dim</span><span class="p">:</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Monotonicities, if provided, must have length input_dim.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>        <span class="n">monotonicities</span>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a>        <span class="k">if</span> <span class="ow">not</span> <span class="n">weighted_average</span>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a>        <span class="k">else</span> <span class="p">[</span><span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">]</span> <span class="o">*</span> <span class="n">input_dim</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a>    <span class="p">)</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span> <span class="o">=</span> <span class="n">use_bias</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">weighted_average</span> <span class="k">else</span> <span class="kc">False</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">weighted_average</span> <span class="o">=</span> <span class="n">weighted_average</span>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">input_dim</span><span class="p">)</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>    <span class="k">if</span> <span class="n">use_bias</span><span class="p">:</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">())</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.Linear.apply_constraints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">apply_constraints</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Projects kernel into desired constraints.</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/linear.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-101" name="__codelineno-0-101"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a><span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Projects kernel into desired constraints.&quot;&quot;&quot;</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>    <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>        <span class="k">if</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>            <span class="n">increasing_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>                <span class="p">[</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>                    <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span> <span class="k">else</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">]</span>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a>                    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a>                <span class="p">]</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>            <span class="p">)</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a>            <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a>                <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="n">projected_kernel_data</span> <span class="o">*</span> <span class="n">increasing_mask</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>            <span class="p">)</span>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a>        <span class="k">if</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a>            <span class="n">decreasing_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a>                <span class="p">[</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>                    <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span> <span class="k">else</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">]</span>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a>                    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a>                <span class="p">]</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a>            <span class="p">)</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a>            <span class="n">projected_kernel_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>                <span class="n">projected_kernel_data</span><span class="p">,</span> <span class="n">projected_kernel_data</span> <span class="o">*</span> <span class="n">decreasing_mask</span>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a>            <span class="p">)</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_average</span><span class="p">:</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>        <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">projected_kernel_data</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>        <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">norm</span> <span class="o">&lt;</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">norm</span><span class="p">)</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>        <span class="n">projected_kernel_data</span> <span class="o">/=</span> <span class="n">norm</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">projected_kernel_data</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.Linear.assert_constraints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">assert_constraints</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Asserts that layer satisfies specified constraints.</p>
<p>This checks that decreasing monotonicity corresponds to negative weights,
increasing monotonicity corresponds to positive weights, and weights sum to 1
for weighted_average=True.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>eps</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>the margin of error allowed</p>
            </div>
          </td>
          <td>
                <code>1e-06</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A list of messages describing violated constraints. If no constraints</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>violated, the list will be empty.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/linear.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-135" name="__codelineno-0-135"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a><span class="k">def</span> <span class="nf">assert_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Asserts that layer satisfies specified constraints.</span>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a><span class="sd">    This checks that decreasing monotonicity corresponds to negative weights,</span>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a><span class="sd">    increasing monotonicity corresponds to positive weights, and weights sum to 1</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a><span class="sd">    for weighted_average=True.</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="sd">        eps: the margin of error allowed</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a><span class="sd">        A list of messages describing violated constraints. If no constraints</span>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">        violated, the list will be empty.</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weighted_average</span><span class="p">:</span>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>        <span class="n">total_weight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">total_weight</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Weights do not sum to 1.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span><span class="p">:</span>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a>        <span class="n">monotonicities_constant</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>            <span class="p">[</span>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a>                <span class="mi">1</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a>                <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a>                <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a>                <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a>                <span class="k">else</span> <span class="mi">0</span>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>                <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicities</span>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a>            <span class="p">],</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a>            <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a>            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a>        <span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a>        <span class="n">violated_monotonicities</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">*</span> <span class="n">monotonicities_constant</span><span class="p">)</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">eps</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a>        <span class="n">violation_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">violated_monotonicities</span><span class="p">)</span>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>        <span class="k">if</span> <span class="n">violation_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>                <span class="sa">f</span><span class="s2">&quot;Monotonicity violated at: </span><span class="si">{</span><span class="n">violation_indices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>            <span class="p">)</span>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a>    <span class="k">return</span> <span class="n">messages</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.Linear.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Transforms inputs using a linear combination.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>x</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The input tensor of shape <code>(batch_size, input_dim)</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>torch.Tensor of shape <code>(batch_size, 1)</code> containing transformed input values.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/linear.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-87">87</a></span>
<span class="normal"><a href="#__codelineno-0-88">88</a></span>
<span class="normal"><a href="#__codelineno-0-89">89</a></span>
<span class="normal"><a href="#__codelineno-0-90">90</a></span>
<span class="normal"><a href="#__codelineno-0-91">91</a></span>
<span class="normal"><a href="#__codelineno-0-92">92</a></span>
<span class="normal"><a href="#__codelineno-0-93">93</a></span>
<span class="normal"><a href="#__codelineno-0-94">94</a></span>
<span class="normal"><a href="#__codelineno-0-95">95</a></span>
<span class="normal"><a href="#__codelineno-0-96">96</a></span>
<span class="normal"><a href="#__codelineno-0-97">97</a></span>
<span class="normal"><a href="#__codelineno-0-98">98</a></span>
<span class="normal"><a href="#__codelineno-0-99">99</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-87" name="__codelineno-0-87"></a><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Transforms inputs using a linear combination.</span>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a><span class="sd">        x: The input tensor of shape `(batch_size, input_dim)`.</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a><span class="sd">        torch.Tensor of shape `(batch_size, 1)` containing transformed input values.</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a>    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_bias</span><span class="p">:</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>        <span class="n">result</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>

<div class="doc doc-object doc-class">



<h2 id="pytorch_lattice.layers.NumericalCalibrator" class="doc doc-heading">
          <code>pytorch_lattice.layers.NumericalCalibrator</code>


</h2>


  <div class="doc doc-contents first">
          <p class="doc doc-class-bases">
            Bases: <code><a class="autorefs autorefs-internal" title="pytorch_lattice.constrained_module.ConstrainedModule" href="../constrained_module/#pytorch_lattice.constrained_module.ConstrainedModule">ConstrainedModule</a></code></p>

  
      <p>A numerical calibrator.</p>
<p>This module takes an input of shape <code>(batch_size, 1)</code> and calibrates it using a
piece-wise linear function that conforms to any provided constraints. The output
will have the same shape as the input.</p>



  <p><span class="doc-section-title">Attributes:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.NumericalCalibrator.All">All</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>__init__</code> arguments.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.NumericalCalibrator.kernel">kernel</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>torch.nn.Parameter</code> that stores the piece-wise linear function weights.</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code><span title="pytorch_lattice.layers.NumericalCalibrator.missing_output">missing_output</span></code></td>
          <td>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>torch.nn.Parameter</code> that stores the output learned for any
missing inputs. Only available if <code>missing_input_value</code> is provided.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>
      <p>Example:
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>  <span class="c1"># shape: (batch_size, 1)</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="n">calibrator</span> <span class="o">=</span> <span class="n">NumericalCalibrator</span><span class="p">(</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">input_keypoints</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">5</span><span class="p">),</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">output_min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">output_max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">monotonicity</span><span class="o">=</span><span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">,</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">kernel_init</span><span class="o">=</span><span class="n">NumericalCalibratorInit</span><span class="o">.</span><span class="n">EQUAL_HEIGHTS</span><span class="p">,</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="p">)</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="n">outputs</span> <span class="o">=</span> <span class="n">calibrator</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</code></pre></div></p>

            <details class="quote">
              <summary>Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-17"> 17</a></span>
<span class="normal"><a href="#__codelineno-0-18"> 18</a></span>
<span class="normal"><a href="#__codelineno-0-19"> 19</a></span>
<span class="normal"><a href="#__codelineno-0-20"> 20</a></span>
<span class="normal"><a href="#__codelineno-0-21"> 21</a></span>
<span class="normal"><a href="#__codelineno-0-22"> 22</a></span>
<span class="normal"><a href="#__codelineno-0-23"> 23</a></span>
<span class="normal"><a href="#__codelineno-0-24"> 24</a></span>
<span class="normal"><a href="#__codelineno-0-25"> 25</a></span>
<span class="normal"><a href="#__codelineno-0-26"> 26</a></span>
<span class="normal"><a href="#__codelineno-0-27"> 27</a></span>
<span class="normal"><a href="#__codelineno-0-28"> 28</a></span>
<span class="normal"><a href="#__codelineno-0-29"> 29</a></span>
<span class="normal"><a href="#__codelineno-0-30"> 30</a></span>
<span class="normal"><a href="#__codelineno-0-31"> 31</a></span>
<span class="normal"><a href="#__codelineno-0-32"> 32</a></span>
<span class="normal"><a href="#__codelineno-0-33"> 33</a></span>
<span class="normal"><a href="#__codelineno-0-34"> 34</a></span>
<span class="normal"><a href="#__codelineno-0-35"> 35</a></span>
<span class="normal"><a href="#__codelineno-0-36"> 36</a></span>
<span class="normal"><a href="#__codelineno-0-37"> 37</a></span>
<span class="normal"><a href="#__codelineno-0-38"> 38</a></span>
<span class="normal"><a href="#__codelineno-0-39"> 39</a></span>
<span class="normal"><a href="#__codelineno-0-40"> 40</a></span>
<span class="normal"><a href="#__codelineno-0-41"> 41</a></span>
<span class="normal"><a href="#__codelineno-0-42"> 42</a></span>
<span class="normal"><a href="#__codelineno-0-43"> 43</a></span>
<span class="normal"><a href="#__codelineno-0-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-0-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-0-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-0-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-0-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-0-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-0-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-0-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-0-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-0-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-0-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-0-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-0-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-0-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-0-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-0-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-0-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-0-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-0-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-0-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-0-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-0-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-0-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-0-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-0-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-0-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-0-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-0-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-0-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-0-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-0-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-0-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-0-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-0-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-0-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-0-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-0-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-0-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-0-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-0-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-0-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-0-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-0-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-0-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-0-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-0-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-0-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-0-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-0-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span>
<span class="normal"><a href="#__codelineno-0-143">143</a></span>
<span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span>
<span class="normal"><a href="#__codelineno-0-178">178</a></span>
<span class="normal"><a href="#__codelineno-0-179">179</a></span>
<span class="normal"><a href="#__codelineno-0-180">180</a></span>
<span class="normal"><a href="#__codelineno-0-181">181</a></span>
<span class="normal"><a href="#__codelineno-0-182">182</a></span>
<span class="normal"><a href="#__codelineno-0-183">183</a></span>
<span class="normal"><a href="#__codelineno-0-184">184</a></span>
<span class="normal"><a href="#__codelineno-0-185">185</a></span>
<span class="normal"><a href="#__codelineno-0-186">186</a></span>
<span class="normal"><a href="#__codelineno-0-187">187</a></span>
<span class="normal"><a href="#__codelineno-0-188">188</a></span>
<span class="normal"><a href="#__codelineno-0-189">189</a></span>
<span class="normal"><a href="#__codelineno-0-190">190</a></span>
<span class="normal"><a href="#__codelineno-0-191">191</a></span>
<span class="normal"><a href="#__codelineno-0-192">192</a></span>
<span class="normal"><a href="#__codelineno-0-193">193</a></span>
<span class="normal"><a href="#__codelineno-0-194">194</a></span>
<span class="normal"><a href="#__codelineno-0-195">195</a></span>
<span class="normal"><a href="#__codelineno-0-196">196</a></span>
<span class="normal"><a href="#__codelineno-0-197">197</a></span>
<span class="normal"><a href="#__codelineno-0-198">198</a></span>
<span class="normal"><a href="#__codelineno-0-199">199</a></span>
<span class="normal"><a href="#__codelineno-0-200">200</a></span>
<span class="normal"><a href="#__codelineno-0-201">201</a></span>
<span class="normal"><a href="#__codelineno-0-202">202</a></span>
<span class="normal"><a href="#__codelineno-0-203">203</a></span>
<span class="normal"><a href="#__codelineno-0-204">204</a></span>
<span class="normal"><a href="#__codelineno-0-205">205</a></span>
<span class="normal"><a href="#__codelineno-0-206">206</a></span>
<span class="normal"><a href="#__codelineno-0-207">207</a></span>
<span class="normal"><a href="#__codelineno-0-208">208</a></span>
<span class="normal"><a href="#__codelineno-0-209">209</a></span>
<span class="normal"><a href="#__codelineno-0-210">210</a></span>
<span class="normal"><a href="#__codelineno-0-211">211</a></span>
<span class="normal"><a href="#__codelineno-0-212">212</a></span>
<span class="normal"><a href="#__codelineno-0-213">213</a></span>
<span class="normal"><a href="#__codelineno-0-214">214</a></span>
<span class="normal"><a href="#__codelineno-0-215">215</a></span>
<span class="normal"><a href="#__codelineno-0-216">216</a></span>
<span class="normal"><a href="#__codelineno-0-217">217</a></span>
<span class="normal"><a href="#__codelineno-0-218">218</a></span>
<span class="normal"><a href="#__codelineno-0-219">219</a></span>
<span class="normal"><a href="#__codelineno-0-220">220</a></span>
<span class="normal"><a href="#__codelineno-0-221">221</a></span>
<span class="normal"><a href="#__codelineno-0-222">222</a></span>
<span class="normal"><a href="#__codelineno-0-223">223</a></span>
<span class="normal"><a href="#__codelineno-0-224">224</a></span>
<span class="normal"><a href="#__codelineno-0-225">225</a></span>
<span class="normal"><a href="#__codelineno-0-226">226</a></span>
<span class="normal"><a href="#__codelineno-0-227">227</a></span>
<span class="normal"><a href="#__codelineno-0-228">228</a></span>
<span class="normal"><a href="#__codelineno-0-229">229</a></span>
<span class="normal"><a href="#__codelineno-0-230">230</a></span>
<span class="normal"><a href="#__codelineno-0-231">231</a></span>
<span class="normal"><a href="#__codelineno-0-232">232</a></span>
<span class="normal"><a href="#__codelineno-0-233">233</a></span>
<span class="normal"><a href="#__codelineno-0-234">234</a></span>
<span class="normal"><a href="#__codelineno-0-235">235</a></span>
<span class="normal"><a href="#__codelineno-0-236">236</a></span>
<span class="normal"><a href="#__codelineno-0-237">237</a></span>
<span class="normal"><a href="#__codelineno-0-238">238</a></span>
<span class="normal"><a href="#__codelineno-0-239">239</a></span>
<span class="normal"><a href="#__codelineno-0-240">240</a></span>
<span class="normal"><a href="#__codelineno-0-241">241</a></span>
<span class="normal"><a href="#__codelineno-0-242">242</a></span>
<span class="normal"><a href="#__codelineno-0-243">243</a></span>
<span class="normal"><a href="#__codelineno-0-244">244</a></span>
<span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span>
<span class="normal"><a href="#__codelineno-0-259">259</a></span>
<span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span>
<span class="normal"><a href="#__codelineno-0-265">265</a></span>
<span class="normal"><a href="#__codelineno-0-266">266</a></span>
<span class="normal"><a href="#__codelineno-0-267">267</a></span>
<span class="normal"><a href="#__codelineno-0-268">268</a></span>
<span class="normal"><a href="#__codelineno-0-269">269</a></span>
<span class="normal"><a href="#__codelineno-0-270">270</a></span>
<span class="normal"><a href="#__codelineno-0-271">271</a></span>
<span class="normal"><a href="#__codelineno-0-272">272</a></span>
<span class="normal"><a href="#__codelineno-0-273">273</a></span>
<span class="normal"><a href="#__codelineno-0-274">274</a></span>
<span class="normal"><a href="#__codelineno-0-275">275</a></span>
<span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span>
<span class="normal"><a href="#__codelineno-0-301">301</a></span>
<span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span>
<span class="normal"><a href="#__codelineno-0-312">312</a></span>
<span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span>
<span class="normal"><a href="#__codelineno-0-317">317</a></span>
<span class="normal"><a href="#__codelineno-0-318">318</a></span>
<span class="normal"><a href="#__codelineno-0-319">319</a></span>
<span class="normal"><a href="#__codelineno-0-320">320</a></span>
<span class="normal"><a href="#__codelineno-0-321">321</a></span>
<span class="normal"><a href="#__codelineno-0-322">322</a></span>
<span class="normal"><a href="#__codelineno-0-323">323</a></span>
<span class="normal"><a href="#__codelineno-0-324">324</a></span>
<span class="normal"><a href="#__codelineno-0-325">325</a></span>
<span class="normal"><a href="#__codelineno-0-326">326</a></span>
<span class="normal"><a href="#__codelineno-0-327">327</a></span>
<span class="normal"><a href="#__codelineno-0-328">328</a></span>
<span class="normal"><a href="#__codelineno-0-329">329</a></span>
<span class="normal"><a href="#__codelineno-0-330">330</a></span>
<span class="normal"><a href="#__codelineno-0-331">331</a></span>
<span class="normal"><a href="#__codelineno-0-332">332</a></span>
<span class="normal"><a href="#__codelineno-0-333">333</a></span>
<span class="normal"><a href="#__codelineno-0-334">334</a></span>
<span class="normal"><a href="#__codelineno-0-335">335</a></span>
<span class="normal"><a href="#__codelineno-0-336">336</a></span>
<span class="normal"><a href="#__codelineno-0-337">337</a></span>
<span class="normal"><a href="#__codelineno-0-338">338</a></span>
<span class="normal"><a href="#__codelineno-0-339">339</a></span>
<span class="normal"><a href="#__codelineno-0-340">340</a></span>
<span class="normal"><a href="#__codelineno-0-341">341</a></span>
<span class="normal"><a href="#__codelineno-0-342">342</a></span>
<span class="normal"><a href="#__codelineno-0-343">343</a></span>
<span class="normal"><a href="#__codelineno-0-344">344</a></span>
<span class="normal"><a href="#__codelineno-0-345">345</a></span>
<span class="normal"><a href="#__codelineno-0-346">346</a></span>
<span class="normal"><a href="#__codelineno-0-347">347</a></span>
<span class="normal"><a href="#__codelineno-0-348">348</a></span>
<span class="normal"><a href="#__codelineno-0-349">349</a></span>
<span class="normal"><a href="#__codelineno-0-350">350</a></span>
<span class="normal"><a href="#__codelineno-0-351">351</a></span>
<span class="normal"><a href="#__codelineno-0-352">352</a></span>
<span class="normal"><a href="#__codelineno-0-353">353</a></span>
<span class="normal"><a href="#__codelineno-0-354">354</a></span>
<span class="normal"><a href="#__codelineno-0-355">355</a></span>
<span class="normal"><a href="#__codelineno-0-356">356</a></span>
<span class="normal"><a href="#__codelineno-0-357">357</a></span>
<span class="normal"><a href="#__codelineno-0-358">358</a></span>
<span class="normal"><a href="#__codelineno-0-359">359</a></span>
<span class="normal"><a href="#__codelineno-0-360">360</a></span>
<span class="normal"><a href="#__codelineno-0-361">361</a></span>
<span class="normal"><a href="#__codelineno-0-362">362</a></span>
<span class="normal"><a href="#__codelineno-0-363">363</a></span>
<span class="normal"><a href="#__codelineno-0-364">364</a></span>
<span class="normal"><a href="#__codelineno-0-365">365</a></span>
<span class="normal"><a href="#__codelineno-0-366">366</a></span>
<span class="normal"><a href="#__codelineno-0-367">367</a></span>
<span class="normal"><a href="#__codelineno-0-368">368</a></span>
<span class="normal"><a href="#__codelineno-0-369">369</a></span>
<span class="normal"><a href="#__codelineno-0-370">370</a></span>
<span class="normal"><a href="#__codelineno-0-371">371</a></span>
<span class="normal"><a href="#__codelineno-0-372">372</a></span>
<span class="normal"><a href="#__codelineno-0-373">373</a></span>
<span class="normal"><a href="#__codelineno-0-374">374</a></span>
<span class="normal"><a href="#__codelineno-0-375">375</a></span>
<span class="normal"><a href="#__codelineno-0-376">376</a></span>
<span class="normal"><a href="#__codelineno-0-377">377</a></span>
<span class="normal"><a href="#__codelineno-0-378">378</a></span>
<span class="normal"><a href="#__codelineno-0-379">379</a></span>
<span class="normal"><a href="#__codelineno-0-380">380</a></span>
<span class="normal"><a href="#__codelineno-0-381">381</a></span>
<span class="normal"><a href="#__codelineno-0-382">382</a></span>
<span class="normal"><a href="#__codelineno-0-383">383</a></span>
<span class="normal"><a href="#__codelineno-0-384">384</a></span>
<span class="normal"><a href="#__codelineno-0-385">385</a></span>
<span class="normal"><a href="#__codelineno-0-386">386</a></span>
<span class="normal"><a href="#__codelineno-0-387">387</a></span>
<span class="normal"><a href="#__codelineno-0-388">388</a></span>
<span class="normal"><a href="#__codelineno-0-389">389</a></span>
<span class="normal"><a href="#__codelineno-0-390">390</a></span>
<span class="normal"><a href="#__codelineno-0-391">391</a></span>
<span class="normal"><a href="#__codelineno-0-392">392</a></span>
<span class="normal"><a href="#__codelineno-0-393">393</a></span>
<span class="normal"><a href="#__codelineno-0-394">394</a></span>
<span class="normal"><a href="#__codelineno-0-395">395</a></span>
<span class="normal"><a href="#__codelineno-0-396">396</a></span>
<span class="normal"><a href="#__codelineno-0-397">397</a></span>
<span class="normal"><a href="#__codelineno-0-398">398</a></span>
<span class="normal"><a href="#__codelineno-0-399">399</a></span>
<span class="normal"><a href="#__codelineno-0-400">400</a></span>
<span class="normal"><a href="#__codelineno-0-401">401</a></span>
<span class="normal"><a href="#__codelineno-0-402">402</a></span>
<span class="normal"><a href="#__codelineno-0-403">403</a></span>
<span class="normal"><a href="#__codelineno-0-404">404</a></span>
<span class="normal"><a href="#__codelineno-0-405">405</a></span>
<span class="normal"><a href="#__codelineno-0-406">406</a></span>
<span class="normal"><a href="#__codelineno-0-407">407</a></span>
<span class="normal"><a href="#__codelineno-0-408">408</a></span>
<span class="normal"><a href="#__codelineno-0-409">409</a></span>
<span class="normal"><a href="#__codelineno-0-410">410</a></span>
<span class="normal"><a href="#__codelineno-0-411">411</a></span>
<span class="normal"><a href="#__codelineno-0-412">412</a></span>
<span class="normal"><a href="#__codelineno-0-413">413</a></span>
<span class="normal"><a href="#__codelineno-0-414">414</a></span>
<span class="normal"><a href="#__codelineno-0-415">415</a></span>
<span class="normal"><a href="#__codelineno-0-416">416</a></span>
<span class="normal"><a href="#__codelineno-0-417">417</a></span>
<span class="normal"><a href="#__codelineno-0-418">418</a></span>
<span class="normal"><a href="#__codelineno-0-419">419</a></span>
<span class="normal"><a href="#__codelineno-0-420">420</a></span>
<span class="normal"><a href="#__codelineno-0-421">421</a></span>
<span class="normal"><a href="#__codelineno-0-422">422</a></span>
<span class="normal"><a href="#__codelineno-0-423">423</a></span>
<span class="normal"><a href="#__codelineno-0-424">424</a></span>
<span class="normal"><a href="#__codelineno-0-425">425</a></span>
<span class="normal"><a href="#__codelineno-0-426">426</a></span>
<span class="normal"><a href="#__codelineno-0-427">427</a></span>
<span class="normal"><a href="#__codelineno-0-428">428</a></span>
<span class="normal"><a href="#__codelineno-0-429">429</a></span>
<span class="normal"><a href="#__codelineno-0-430">430</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-17" name="__codelineno-0-17"></a><span class="k">class</span> <span class="nc">NumericalCalibrator</span><span class="p">(</span><span class="n">ConstrainedModule</span><span class="p">):</span>
<a id="__codelineno-0-18" name="__codelineno-0-18"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;A numerical calibrator.</span>
<a id="__codelineno-0-19" name="__codelineno-0-19"></a>
<a id="__codelineno-0-20" name="__codelineno-0-20"></a><span class="sd">    This module takes an input of shape `(batch_size, 1)` and calibrates it using a</span>
<a id="__codelineno-0-21" name="__codelineno-0-21"></a><span class="sd">    piece-wise linear function that conforms to any provided constraints. The output</span>
<a id="__codelineno-0-22" name="__codelineno-0-22"></a><span class="sd">    will have the same shape as the input.</span>
<a id="__codelineno-0-23" name="__codelineno-0-23"></a>
<a id="__codelineno-0-24" name="__codelineno-0-24"></a><span class="sd">    Attributes:</span>
<a id="__codelineno-0-25" name="__codelineno-0-25"></a><span class="sd">        All: `__init__` arguments.</span>
<a id="__codelineno-0-26" name="__codelineno-0-26"></a><span class="sd">        kernel: `torch.nn.Parameter` that stores the piece-wise linear function weights.</span>
<a id="__codelineno-0-27" name="__codelineno-0-27"></a><span class="sd">        missing_output: `torch.nn.Parameter` that stores the output learned for any</span>
<a id="__codelineno-0-28" name="__codelineno-0-28"></a><span class="sd">            missing inputs. Only available if `missing_input_value` is provided.</span>
<a id="__codelineno-0-29" name="__codelineno-0-29"></a>
<a id="__codelineno-0-30" name="__codelineno-0-30"></a><span class="sd">    Example:</span>
<a id="__codelineno-0-31" name="__codelineno-0-31"></a><span class="sd">    ```python</span>
<a id="__codelineno-0-32" name="__codelineno-0-32"></a><span class="sd">    inputs = torch.tensor(...)  # shape: (batch_size, 1)</span>
<a id="__codelineno-0-33" name="__codelineno-0-33"></a><span class="sd">    calibrator = NumericalCalibrator(</span>
<a id="__codelineno-0-34" name="__codelineno-0-34"></a><span class="sd">        input_keypoints=np.linspace(1., 5., num=5),</span>
<a id="__codelineno-0-35" name="__codelineno-0-35"></a><span class="sd">        output_min=0.0,</span>
<a id="__codelineno-0-36" name="__codelineno-0-36"></a><span class="sd">        output_max=1.0,</span>
<a id="__codelineno-0-37" name="__codelineno-0-37"></a><span class="sd">        monotonicity=Monotonicity.INCREASING,</span>
<a id="__codelineno-0-38" name="__codelineno-0-38"></a><span class="sd">        kernel_init=NumericalCalibratorInit.EQUAL_HEIGHTS,</span>
<a id="__codelineno-0-39" name="__codelineno-0-39"></a><span class="sd">    )</span>
<a id="__codelineno-0-40" name="__codelineno-0-40"></a><span class="sd">    outputs = calibrator(inputs)</span>
<a id="__codelineno-0-41" name="__codelineno-0-41"></a><span class="sd">    ```</span>
<a id="__codelineno-0-42" name="__codelineno-0-42"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-43" name="__codelineno-0-43"></a>
<a id="__codelineno-0-44" name="__codelineno-0-44"></a>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>        <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>        <span class="n">input_keypoints</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>        <span class="n">missing_input_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>        <span class="n">output_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>        <span class="n">output_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>        <span class="n">monotonicity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Monotonicity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>        <span class="n">kernel_init</span><span class="p">:</span> <span class="n">NumericalCalibratorInit</span> <span class="o">=</span> <span class="n">NumericalCalibratorInit</span><span class="o">.</span><span class="n">EQUAL_HEIGHTS</span><span class="p">,</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a>        <span class="n">projection_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>        <span class="n">input_keypoints_type</span><span class="p">:</span> <span class="n">InputKeypointsType</span> <span class="o">=</span> <span class="n">InputKeypointsType</span><span class="o">.</span><span class="n">FIXED</span><span class="p">,</span>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an instance of `NumericalCalibrator`.</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">            input_keypoints: Ordered list of float-valued keypoints for the underlying</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">                piece-wise linear function.</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a><span class="sd">            missing_input_value: If provided, the calibrator will learn to map all</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a><span class="sd">                instances of this missing input value to a learned output value.</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a><span class="sd">            output_min: Minimum output value. If `None`, the minimum output value will</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a><span class="sd">                be unbounded.</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a><span class="sd">            output_max: Maximum output value. If `None`, the maximum output value will</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a><span class="sd">                be unbounded.</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a><span class="sd">            monotonicity: Monotonicity constraint for the underlying piece-wise linear</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">                function.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a><span class="sd">            kernel_init: Initialization scheme to use for the kernel.</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a><span class="sd">            projection_iterations: Number of times to run Dykstra&#39;s projection</span>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a><span class="sd">                algorithm when applying constraints.</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a><span class="sd">            input_keypoints_type: `InputKeypointType` of either `FIXED` or `LEARNED`. If</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a><span class="sd">                `LEARNED`, keypoints other than the first or last will follow</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a><span class="sd">                `input_keypoints` for initialization but adapt during training.</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a><span class="sd">        Raises:</span>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a><span class="sd">            ValueError: If `kernel_init` is invalid.</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input_keypoints</span> <span class="o">=</span> <span class="n">input_keypoints</span>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="o">=</span> <span class="n">missing_input_value</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">=</span> <span class="n">output_min</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">=</span> <span class="n">output_max</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">=</span> <span class="n">monotonicity</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">=</span> <span class="n">kernel_init</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">projection_iterations</span> <span class="o">=</span> <span class="n">projection_iterations</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">input_keypoints_type</span> <span class="o">=</span> <span class="n">input_keypoints_type</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>        <span class="c1"># Determine default output initialization values if bounds are not fully set.</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>        <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a>            <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>        <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a>            <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_min</span> <span class="o">+</span> <span class="mf">4.0</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>        <span class="k">elif</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>            <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_max</span> <span class="o">-</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>            <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span> <span class="o">=</span> <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_keypoints</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_keypoints</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">input_keypoints</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_keypoints_type</span> <span class="o">==</span> <span class="n">InputKeypointsType</span><span class="o">.</span><span class="n">LEARNED</span><span class="p">:</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_min</span> <span class="o">=</span> <span class="n">input_keypoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_range</span> <span class="o">=</span> <span class="n">input_keypoints</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">input_keypoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>            <span class="n">initial_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a>                <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>                    <span class="p">(</span><span class="n">input_keypoints</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">input_keypoints</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_range</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>                <span class="p">)</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>            <span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initial_logits</span><span class="p">)</span>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a>        <span class="c1"># First row of the kernel represents the bias. The remaining rows represent</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>        <span class="c1"># the y-value delta compared to the previous point i.e. the segment heights.</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a>        <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a>        <span class="k">def</span> <span class="nf">initialize_kernel</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>            <span class="n">output_init_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a>            <span class="k">if</span> <span class="n">kernel_init</span> <span class="o">==</span> <span class="n">NumericalCalibratorInit</span><span class="o">.</span><span class="n">EQUAL_HEIGHTS</span><span class="p">:</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a>                <span class="n">num_segments</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a>                <span class="n">segment_height</span> <span class="o">=</span> <span class="n">output_init_range</span> <span class="o">/</span> <span class="n">num_segments</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>                <span class="n">heights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">segment_height</span><span class="p">]]</span> <span class="o">*</span> <span class="n">num_segments</span><span class="p">)</span>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a>            <span class="k">elif</span> <span class="n">kernel_init</span> <span class="o">==</span> <span class="n">NumericalCalibratorInit</span><span class="o">.</span><span class="n">EQUAL_SLOPES</span><span class="p">:</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a>                <span class="n">heights</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a>                    <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span> <span class="o">*</span> <span class="n">output_init_range</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span><span class="p">)</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a>                <span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a>                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown kernel init: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a>            <span class="k">if</span> <span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span><span class="p">:</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>                <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span><span class="p">]])</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>                <span class="n">heights</span> <span class="o">=</span> <span class="o">-</span><span class="n">heights</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>                <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span><span class="p">]])</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initialize_kernel</span><span class="p">())</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a>        <span class="k">if</span> <span class="n">missing_input_value</span><span class="p">:</span>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">missing_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">missing_output</span><span class="p">,</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a>                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>            <span class="p">)</span>
<a id="__codelineno-0-143" name="__codelineno-0-143"></a>
<a id="__codelineno-0-144" name="__codelineno-0-144"></a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Calibrates numerical inputs through piece-wise linear interpolation.</span>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">            x: The input tensor of shape `(batch_size, 1)`.</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a><span class="sd">            torch.Tensor of shape `(batch_size, 1)` containing calibrated input values.</span>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_keypoints_type</span> <span class="o">==</span> <span class="n">InputKeypointsType</span><span class="o">.</span><span class="n">LEARNED</span><span class="p">:</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>            <span class="n">softmaxed_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a>            <span class="p">)</span>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span> <span class="o">=</span> <span class="n">softmaxed_logits</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_range</span>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a>            <span class="n">interior_keypoints</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>                <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_min</span>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a>            <span class="p">)</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a>                <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_min</span><span class="p">]),</span> <span class="n">interior_keypoints</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a>            <span class="p">)</span>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>        <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a>        <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">interpolation_weights</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a>        <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">interpolation_weights</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a>        <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a>            <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">interpolation_weights</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a>        <span class="p">)</span>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a>        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">interpolation_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>            <span class="n">missing_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>            <span class="n">result</span> <span class="o">=</span> <span class="n">missing_mask</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_output</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">missing_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">result</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>        <span class="k">return</span> <span class="n">result</span>
<a id="__codelineno-0-178" name="__codelineno-0-178"></a>
<a id="__codelineno-0-179" name="__codelineno-0-179"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-180" name="__codelineno-0-180"></a>    <span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-181" name="__codelineno-0-181"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Jointly projects kernel into desired constraints.</span>
<a id="__codelineno-0-182" name="__codelineno-0-182"></a>
<a id="__codelineno-0-183" name="__codelineno-0-183"></a><span class="sd">        Uses Dykstra&#39;s alternating projection algorithm to jointly project onto all</span>
<a id="__codelineno-0-184" name="__codelineno-0-184"></a><span class="sd">        given constraints. This algorithm projects with respect to the L2 norm, but it</span>
<a id="__codelineno-0-185" name="__codelineno-0-185"></a><span class="sd">        approached the norm from the &quot;wrong&quot; side. To ensure that all constraints are</span>
<a id="__codelineno-0-186" name="__codelineno-0-186"></a><span class="sd">        strictly met, we do final approximate projections that project strictly into the</span>
<a id="__codelineno-0-187" name="__codelineno-0-187"></a><span class="sd">        feasible space, but this is not an exact projection with respect to the L2 norm.</span>
<a id="__codelineno-0-188" name="__codelineno-0-188"></a><span class="sd">        Enough iterations make the impact of this approximation negligible.</span>
<a id="__codelineno-0-189" name="__codelineno-0-189"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-190" name="__codelineno-0-190"></a>        <span class="n">constrain_bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-191" name="__codelineno-0-191"></a>        <span class="n">constrain_monotonicity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-192" name="__codelineno-0-192"></a>        <span class="n">num_constraints</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">constrain_bounds</span><span class="p">,</span> <span class="n">constrain_monotonicity</span><span class="p">])</span>
<a id="__codelineno-0-193" name="__codelineno-0-193"></a>
<a id="__codelineno-0-194" name="__codelineno-0-194"></a>        <span class="c1"># We do nothing to the weights in this case</span>
<a id="__codelineno-0-195" name="__codelineno-0-195"></a>        <span class="k">if</span> <span class="n">num_constraints</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-196" name="__codelineno-0-196"></a>            <span class="k">return</span>
<a id="__codelineno-0-197" name="__codelineno-0-197"></a>
<a id="__codelineno-0-198" name="__codelineno-0-198"></a>        <span class="n">original_bias</span><span class="p">,</span> <span class="n">original_heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<a id="__codelineno-0-199" name="__codelineno-0-199"></a>        <span class="n">previous_bias_delta</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
<a id="__codelineno-0-200" name="__codelineno-0-200"></a>            <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">original_bias</span><span class="p">)</span>
<a id="__codelineno-0-201" name="__codelineno-0-201"></a>        <span class="p">)</span>
<a id="__codelineno-0-202" name="__codelineno-0-202"></a>        <span class="n">previous_heights_delta</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
<a id="__codelineno-0-203" name="__codelineno-0-203"></a>            <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">original_heights</span><span class="p">)</span>
<a id="__codelineno-0-204" name="__codelineno-0-204"></a>        <span class="p">)</span>
<a id="__codelineno-0-205" name="__codelineno-0-205"></a>
<a id="__codelineno-0-206" name="__codelineno-0-206"></a>        <span class="k">def</span> <span class="nf">apply_bound_constraints</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">):</span>
<a id="__codelineno-0-207" name="__codelineno-0-207"></a>            <span class="n">previous_bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">previous_bias_delta</span><span class="p">[</span><span class="s2">&quot;BOUNDS&quot;</span><span class="p">]</span>
<a id="__codelineno-0-208" name="__codelineno-0-208"></a>            <span class="n">previous_heights</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">-</span> <span class="n">previous_heights_delta</span><span class="p">[</span><span class="s2">&quot;BOUNDS&quot;</span><span class="p">]</span>
<a id="__codelineno-0-209" name="__codelineno-0-209"></a>            <span class="k">if</span> <span class="n">constrain_monotonicity</span><span class="p">:</span>
<a id="__codelineno-0-210" name="__codelineno-0-210"></a>                <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_project_monotonic_bounds</span><span class="p">(</span>
<a id="__codelineno-0-211" name="__codelineno-0-211"></a>                    <span class="n">previous_bias</span><span class="p">,</span> <span class="n">previous_heights</span>
<a id="__codelineno-0-212" name="__codelineno-0-212"></a>                <span class="p">)</span>
<a id="__codelineno-0-213" name="__codelineno-0-213"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-214" name="__codelineno-0-214"></a>                <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_approximately_project_bounds_only</span><span class="p">(</span>
<a id="__codelineno-0-215" name="__codelineno-0-215"></a>                    <span class="n">previous_bias</span><span class="p">,</span> <span class="n">previous_heights</span>
<a id="__codelineno-0-216" name="__codelineno-0-216"></a>                <span class="p">)</span>
<a id="__codelineno-0-217" name="__codelineno-0-217"></a>            <span class="n">previous_bias_delta</span><span class="p">[</span><span class="s2">&quot;BOUNDS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">previous_bias</span>
<a id="__codelineno-0-218" name="__codelineno-0-218"></a>            <span class="n">previous_heights_delta</span><span class="p">[</span><span class="s2">&quot;BOUNDS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">-</span> <span class="n">previous_heights</span>
<a id="__codelineno-0-219" name="__codelineno-0-219"></a>            <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-220" name="__codelineno-0-220"></a>
<a id="__codelineno-0-221" name="__codelineno-0-221"></a>        <span class="k">def</span> <span class="nf">apply_monotonicity_constraints</span><span class="p">(</span><span class="n">heights</span><span class="p">):</span>
<a id="__codelineno-0-222" name="__codelineno-0-222"></a>            <span class="n">previous_heights</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">-</span> <span class="n">previous_bias_delta</span><span class="p">[</span><span class="s2">&quot;MONOTONICITY&quot;</span><span class="p">]</span>
<a id="__codelineno-0-223" name="__codelineno-0-223"></a>            <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_project_monotonicity</span><span class="p">(</span><span class="n">previous_heights</span><span class="p">)</span>
<a id="__codelineno-0-224" name="__codelineno-0-224"></a>            <span class="n">previous_heights_delta</span><span class="p">[</span><span class="s2">&quot;MONOTONICITY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">-</span> <span class="n">previous_heights</span>
<a id="__codelineno-0-225" name="__codelineno-0-225"></a>            <span class="k">return</span> <span class="n">heights</span>
<a id="__codelineno-0-226" name="__codelineno-0-226"></a>
<a id="__codelineno-0-227" name="__codelineno-0-227"></a>        <span class="k">def</span> <span class="nf">apply_dykstras_projection</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">):</span>
<a id="__codelineno-0-228" name="__codelineno-0-228"></a>            <span class="k">if</span> <span class="n">constrain_bounds</span><span class="p">:</span>
<a id="__codelineno-0-229" name="__codelineno-0-229"></a>                <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="n">apply_bound_constraints</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">)</span>
<a id="__codelineno-0-230" name="__codelineno-0-230"></a>            <span class="k">if</span> <span class="n">constrain_monotonicity</span><span class="p">:</span>
<a id="__codelineno-0-231" name="__codelineno-0-231"></a>                <span class="n">heights</span> <span class="o">=</span> <span class="n">apply_monotonicity_constraints</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<a id="__codelineno-0-232" name="__codelineno-0-232"></a>            <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-233" name="__codelineno-0-233"></a>
<a id="__codelineno-0-234" name="__codelineno-0-234"></a>        <span class="k">def</span> <span class="nf">finalize_constraints</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">):</span>
<a id="__codelineno-0-235" name="__codelineno-0-235"></a>            <span class="k">if</span> <span class="n">constrain_monotonicity</span><span class="p">:</span>
<a id="__codelineno-0-236" name="__codelineno-0-236"></a>                <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_project_monotonicity</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<a id="__codelineno-0-237" name="__codelineno-0-237"></a>            <span class="k">if</span> <span class="n">constrain_bounds</span><span class="p">:</span>
<a id="__codelineno-0-238" name="__codelineno-0-238"></a>                <span class="k">if</span> <span class="n">constrain_monotonicity</span><span class="p">:</span>
<a id="__codelineno-0-239" name="__codelineno-0-239"></a>                    <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_squeeze_by_scaling</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">)</span>
<a id="__codelineno-0-240" name="__codelineno-0-240"></a>                <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-241" name="__codelineno-0-241"></a>                    <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_approximately_project_bounds_only</span><span class="p">(</span>
<a id="__codelineno-0-242" name="__codelineno-0-242"></a>                        <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-243" name="__codelineno-0-243"></a>                    <span class="p">)</span>
<a id="__codelineno-0-244" name="__codelineno-0-244"></a>            <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-245" name="__codelineno-0-245"></a>
<a id="__codelineno-0-246" name="__codelineno-0-246"></a>        <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span> <span class="o">=</span> <span class="n">apply_dykstras_projection</span><span class="p">(</span>
<a id="__codelineno-0-247" name="__codelineno-0-247"></a>            <span class="n">original_bias</span><span class="p">,</span> <span class="n">original_heights</span>
<a id="__codelineno-0-248" name="__codelineno-0-248"></a>        <span class="p">)</span>
<a id="__codelineno-0-249" name="__codelineno-0-249"></a>        <span class="k">if</span> <span class="n">num_constraints</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-250" name="__codelineno-0-250"></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
<a id="__codelineno-0-251" name="__codelineno-0-251"></a>                <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span> <span class="o">=</span> <span class="n">apply_dykstras_projection</span><span class="p">(</span>
<a id="__codelineno-0-252" name="__codelineno-0-252"></a>                    <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span>
<a id="__codelineno-0-253" name="__codelineno-0-253"></a>                <span class="p">)</span>
<a id="__codelineno-0-254" name="__codelineno-0-254"></a>            <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span> <span class="o">=</span> <span class="n">finalize_constraints</span><span class="p">(</span>
<a id="__codelineno-0-255" name="__codelineno-0-255"></a>                <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span>
<a id="__codelineno-0-256" name="__codelineno-0-256"></a>            <span class="p">)</span>
<a id="__codelineno-0-257" name="__codelineno-0-257"></a>
<a id="__codelineno-0-258" name="__codelineno-0-258"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-0-259" name="__codelineno-0-259"></a>
<a id="__codelineno-0-260" name="__codelineno-0-260"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-261" name="__codelineno-0-261"></a>    <span class="k">def</span> <span class="nf">assert_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-262" name="__codelineno-0-262"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Asserts that layer satisfies specified constraints.</span>
<a id="__codelineno-0-263" name="__codelineno-0-263"></a>
<a id="__codelineno-0-264" name="__codelineno-0-264"></a><span class="sd">        This checks that weights follow monotonicity constraints and that the output is</span>
<a id="__codelineno-0-265" name="__codelineno-0-265"></a><span class="sd">        within bounds.</span>
<a id="__codelineno-0-266" name="__codelineno-0-266"></a>
<a id="__codelineno-0-267" name="__codelineno-0-267"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-268" name="__codelineno-0-268"></a><span class="sd">            eps: the margin of error allowed</span>
<a id="__codelineno-0-269" name="__codelineno-0-269"></a>
<a id="__codelineno-0-270" name="__codelineno-0-270"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-271" name="__codelineno-0-271"></a><span class="sd">            A list of messages describing violated constraints including indices of</span>
<a id="__codelineno-0-272" name="__codelineno-0-272"></a><span class="sd">            monotonicity violations. If no constraints violated, the list will be empty.</span>
<a id="__codelineno-0-273" name="__codelineno-0-273"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-274" name="__codelineno-0-274"></a>        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-275" name="__codelineno-0-275"></a>        <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-276" name="__codelineno-0-276"></a>
<a id="__codelineno-0-277" name="__codelineno-0-277"></a>        <span class="k">if</span> <span class="p">(</span>
<a id="__codelineno-0-278" name="__codelineno-0-278"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-279" name="__codelineno-0-279"></a>            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keypoints_outputs</span><span class="p">())</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">+</span> <span class="n">eps</span>
<a id="__codelineno-0-280" name="__codelineno-0-280"></a>        <span class="p">):</span>
<a id="__codelineno-0-281" name="__codelineno-0-281"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Max weight greater than output_max.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-282" name="__codelineno-0-282"></a>        <span class="k">if</span> <span class="p">(</span>
<a id="__codelineno-0-283" name="__codelineno-0-283"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-284" name="__codelineno-0-284"></a>            <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keypoints_outputs</span><span class="p">())</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">-</span> <span class="n">eps</span>
<a id="__codelineno-0-285" name="__codelineno-0-285"></a>        <span class="p">):</span>
<a id="__codelineno-0-286" name="__codelineno-0-286"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Min weight less than output_min.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-287" name="__codelineno-0-287"></a>
<a id="__codelineno-0-288" name="__codelineno-0-288"></a>        <span class="n">diffs</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<a id="__codelineno-0-289" name="__codelineno-0-289"></a>        <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-290" name="__codelineno-0-290"></a>
<a id="__codelineno-0-291" name="__codelineno-0-291"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">:</span>
<a id="__codelineno-0-292" name="__codelineno-0-292"></a>            <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">diffs</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<a id="__codelineno-0-293" name="__codelineno-0-293"></a>        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span><span class="p">:</span>
<a id="__codelineno-0-294" name="__codelineno-0-294"></a>            <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">diffs</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<a id="__codelineno-0-295" name="__codelineno-0-295"></a>
<a id="__codelineno-0-296" name="__codelineno-0-296"></a>        <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">violation_indices</span><span class="p">]</span>
<a id="__codelineno-0-297" name="__codelineno-0-297"></a>        <span class="k">if</span> <span class="n">violation_indices</span><span class="p">:</span>
<a id="__codelineno-0-298" name="__codelineno-0-298"></a>            <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Monotonicity violated at: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">violation_indices</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-299" name="__codelineno-0-299"></a>
<a id="__codelineno-0-300" name="__codelineno-0-300"></a>        <span class="k">return</span> <span class="n">messages</span>
<a id="__codelineno-0-301" name="__codelineno-0-301"></a>
<a id="__codelineno-0-302" name="__codelineno-0-302"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-303" name="__codelineno-0-303"></a>    <span class="k">def</span> <span class="nf">keypoints_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns tensor of keypoint inputs.&quot;&quot;&quot;</span>
<a id="__codelineno-0-305" name="__codelineno-0-305"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-306" name="__codelineno-0-306"></a>            <span class="p">(</span>
<a id="__codelineno-0-307" name="__codelineno-0-307"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span><span class="p">,</span>
<a id="__codelineno-0-308" name="__codelineno-0-308"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span>
<a id="__codelineno-0-309" name="__codelineno-0-309"></a>            <span class="p">),</span>
<a id="__codelineno-0-310" name="__codelineno-0-310"></a>            <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-311" name="__codelineno-0-311"></a>        <span class="p">)</span>
<a id="__codelineno-0-312" name="__codelineno-0-312"></a>
<a id="__codelineno-0-313" name="__codelineno-0-313"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-314" name="__codelineno-0-314"></a>    <span class="k">def</span> <span class="nf">keypoints_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-315" name="__codelineno-0-315"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns tensor of keypoint outputs.&quot;&quot;&quot;</span>
<a id="__codelineno-0-316" name="__codelineno-0-316"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-317" name="__codelineno-0-317"></a>
<a id="__codelineno-0-318" name="__codelineno-0-318"></a>    <span class="c1">################################################################################</span>
<a id="__codelineno-0-319" name="__codelineno-0-319"></a>    <span class="c1">############################## PRIVATE METHODS #################################</span>
<a id="__codelineno-0-320" name="__codelineno-0-320"></a>    <span class="c1">################################################################################</span>
<a id="__codelineno-0-321" name="__codelineno-0-321"></a>
<a id="__codelineno-0-322" name="__codelineno-0-322"></a>    <span class="k">def</span> <span class="nf">_project_monotonic_bounds</span><span class="p">(</span>
<a id="__codelineno-0-323" name="__codelineno-0-323"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">heights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<a id="__codelineno-0-324" name="__codelineno-0-324"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-0-325" name="__codelineno-0-325"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Projects bias and heights into bounds considering monotonicity.</span>
<a id="__codelineno-0-326" name="__codelineno-0-326"></a>
<a id="__codelineno-0-327" name="__codelineno-0-327"></a><span class="sd">        For computation simplification in the case of decreasing monotonicity, we mirror</span>
<a id="__codelineno-0-328" name="__codelineno-0-328"></a><span class="sd">        bias and heights and swap-mirror the output bounds. After doing the standard</span>
<a id="__codelineno-0-329" name="__codelineno-0-329"></a><span class="sd">        projection with resepct to increasing monotonicity, we then mirror everything</span>
<a id="__codelineno-0-330" name="__codelineno-0-330"></a><span class="sd">        back to get the correct projection.</span>
<a id="__codelineno-0-331" name="__codelineno-0-331"></a>
<a id="__codelineno-0-332" name="__codelineno-0-332"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-333" name="__codelineno-0-333"></a><span class="sd">            bias: The bias of the underlying piece-wise linear function.</span>
<a id="__codelineno-0-334" name="__codelineno-0-334"></a><span class="sd">            heights: The heights of each segment of the underlying piece-wise linear</span>
<a id="__codelineno-0-335" name="__codelineno-0-335"></a><span class="sd">                function.</span>
<a id="__codelineno-0-336" name="__codelineno-0-336"></a>
<a id="__codelineno-0-337" name="__codelineno-0-337"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-338" name="__codelineno-0-338"></a><span class="sd">            A tuple containing the projected bias and projected heights.</span>
<a id="__codelineno-0-339" name="__codelineno-0-339"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-340" name="__codelineno-0-340"></a>        <span class="n">output_min</span><span class="p">,</span> <span class="n">output_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span>
<a id="__codelineno-0-341" name="__codelineno-0-341"></a>        <span class="n">decreasing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span>
<a id="__codelineno-0-342" name="__codelineno-0-342"></a>        <span class="k">if</span> <span class="n">decreasing</span><span class="p">:</span>
<a id="__codelineno-0-343" name="__codelineno-0-343"></a>            <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="o">-</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">heights</span>
<a id="__codelineno-0-344" name="__codelineno-0-344"></a>            <span class="n">output_min</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span>
<a id="__codelineno-0-345" name="__codelineno-0-345"></a>            <span class="n">output_max</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span>
<a id="__codelineno-0-346" name="__codelineno-0-346"></a>        <span class="k">if</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-347" name="__codelineno-0-347"></a>            <span class="n">num_heights</span> <span class="o">=</span> <span class="n">heights</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-348" name="__codelineno-0-348"></a>            <span class="n">output_max_diffs</span> <span class="o">=</span> <span class="n">output_max</span> <span class="o">-</span> <span class="p">(</span><span class="n">bias</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<a id="__codelineno-0-349" name="__codelineno-0-349"></a>            <span class="n">bias_delta</span> <span class="o">=</span> <span class="n">output_max_diffs</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_heights</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-0-350" name="__codelineno-0-350"></a>            <span class="n">bias_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">bias_delta</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<a id="__codelineno-0-351" name="__codelineno-0-351"></a>            <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-352" name="__codelineno-0-352"></a>                <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">bias</span> <span class="o">+</span> <span class="n">bias_delta</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">output_min</span><span class="p">))</span>
<a id="__codelineno-0-353" name="__codelineno-0-353"></a>                <span class="n">heights_delta</span> <span class="o">=</span> <span class="n">output_max_diffs</span> <span class="o">/</span> <span class="n">num_heights</span>
<a id="__codelineno-0-354" name="__codelineno-0-354"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-355" name="__codelineno-0-355"></a>                <span class="n">bias</span> <span class="o">+=</span> <span class="n">bias_delta</span>
<a id="__codelineno-0-356" name="__codelineno-0-356"></a>                <span class="n">heights_delta</span> <span class="o">=</span> <span class="n">bias_delta</span>
<a id="__codelineno-0-357" name="__codelineno-0-357"></a>            <span class="n">heights</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">heights_delta</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<a id="__codelineno-0-358" name="__codelineno-0-358"></a>        <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-359" name="__codelineno-0-359"></a>            <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">output_min</span><span class="p">))</span>
<a id="__codelineno-0-360" name="__codelineno-0-360"></a>        <span class="k">if</span> <span class="n">decreasing</span><span class="p">:</span>
<a id="__codelineno-0-361" name="__codelineno-0-361"></a>            <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="o">-</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">heights</span>
<a id="__codelineno-0-362" name="__codelineno-0-362"></a>        <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-363" name="__codelineno-0-363"></a>
<a id="__codelineno-0-364" name="__codelineno-0-364"></a>    <span class="k">def</span> <span class="nf">_approximately_project_bounds_only</span><span class="p">(</span>
<a id="__codelineno-0-365" name="__codelineno-0-365"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">heights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<a id="__codelineno-0-366" name="__codelineno-0-366"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-0-367" name="__codelineno-0-367"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Projects bias and heights without considering monotonicity.</span>
<a id="__codelineno-0-368" name="__codelineno-0-368"></a>
<a id="__codelineno-0-369" name="__codelineno-0-369"></a><span class="sd">        It is worth noting that this projection is an approximation and is not an exact</span>
<a id="__codelineno-0-370" name="__codelineno-0-370"></a><span class="sd">        projection with respect to the L2 norm; however, it is sufficiently accurate and</span>
<a id="__codelineno-0-371" name="__codelineno-0-371"></a><span class="sd">        efficient in practice for non-monotonic functions.</span>
<a id="__codelineno-0-372" name="__codelineno-0-372"></a>
<a id="__codelineno-0-373" name="__codelineno-0-373"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-374" name="__codelineno-0-374"></a><span class="sd">            bias: The bias of the underlying piece-wise linear function.</span>
<a id="__codelineno-0-375" name="__codelineno-0-375"></a><span class="sd">            heights: The heights of each segment of the underlying piece-wise linear</span>
<a id="__codelineno-0-376" name="__codelineno-0-376"></a><span class="sd">                function.</span>
<a id="__codelineno-0-377" name="__codelineno-0-377"></a>
<a id="__codelineno-0-378" name="__codelineno-0-378"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-379" name="__codelineno-0-379"></a><span class="sd">            A tuple containing the projected bias and projected heights.</span>
<a id="__codelineno-0-380" name="__codelineno-0-380"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-381" name="__codelineno-0-381"></a>        <span class="n">sums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-0-382" name="__codelineno-0-382"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-383" name="__codelineno-0-383"></a>            <span class="n">sums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">sums</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_min</span><span class="p">))</span>
<a id="__codelineno-0-384" name="__codelineno-0-384"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-385" name="__codelineno-0-385"></a>            <span class="n">sums</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">sums</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_max</span><span class="p">))</span>
<a id="__codelineno-0-386" name="__codelineno-0-386"></a>        <span class="n">bias</span> <span class="o">=</span> <span class="n">sums</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-0-387" name="__codelineno-0-387"></a>        <span class="n">heights</span> <span class="o">=</span> <span class="n">sums</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">sums</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<a id="__codelineno-0-388" name="__codelineno-0-388"></a>        <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-389" name="__codelineno-0-389"></a>
<a id="__codelineno-0-390" name="__codelineno-0-390"></a>    <span class="k">def</span> <span class="nf">_project_monotonicity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">heights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-391" name="__codelineno-0-391"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns bias and heights projected into desired monotonicity constraints.&quot;&quot;&quot;</span>
<a id="__codelineno-0-392" name="__codelineno-0-392"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">:</span>
<a id="__codelineno-0-393" name="__codelineno-0-393"></a>            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<a id="__codelineno-0-394" name="__codelineno-0-394"></a>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span><span class="p">:</span>
<a id="__codelineno-0-395" name="__codelineno-0-395"></a>            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<a id="__codelineno-0-396" name="__codelineno-0-396"></a>        <span class="k">return</span> <span class="n">heights</span>
<a id="__codelineno-0-397" name="__codelineno-0-397"></a>
<a id="__codelineno-0-398" name="__codelineno-0-398"></a>    <span class="k">def</span> <span class="nf">_squeeze_by_scaling</span><span class="p">(</span>
<a id="__codelineno-0-399" name="__codelineno-0-399"></a>        <span class="bp">self</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">heights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<a id="__codelineno-0-400" name="__codelineno-0-400"></a>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-0-401" name="__codelineno-0-401"></a><span class="w">        </span><span class="sd">&quot;&quot;&quot;Squeezes monotonic calibrators by scaling them into bound constraints.</span>
<a id="__codelineno-0-402" name="__codelineno-0-402"></a>
<a id="__codelineno-0-403" name="__codelineno-0-403"></a><span class="sd">        It is worth noting that this is not an exact projection with respect to the L2</span>
<a id="__codelineno-0-404" name="__codelineno-0-404"></a><span class="sd">        norm; however, it maintains convexity, which projection by shift does not.</span>
<a id="__codelineno-0-405" name="__codelineno-0-405"></a>
<a id="__codelineno-0-406" name="__codelineno-0-406"></a><span class="sd">        Args:</span>
<a id="__codelineno-0-407" name="__codelineno-0-407"></a><span class="sd">            bias: The bias of the underlying piece-wise linear function.</span>
<a id="__codelineno-0-408" name="__codelineno-0-408"></a><span class="sd">            heights: The heights of each segment of the underlying piece-wise linear</span>
<a id="__codelineno-0-409" name="__codelineno-0-409"></a><span class="sd">                function.</span>
<a id="__codelineno-0-410" name="__codelineno-0-410"></a>
<a id="__codelineno-0-411" name="__codelineno-0-411"></a><span class="sd">        Returns:</span>
<a id="__codelineno-0-412" name="__codelineno-0-412"></a><span class="sd">            A tuple containing the projected bias and projected heights.</span>
<a id="__codelineno-0-413" name="__codelineno-0-413"></a><span class="sd">        &quot;&quot;&quot;</span>
<a id="__codelineno-0-414" name="__codelineno-0-414"></a>        <span class="n">decreasing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span>
<a id="__codelineno-0-415" name="__codelineno-0-415"></a>        <span class="n">output_max</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span>
<a id="__codelineno-0-416" name="__codelineno-0-416"></a>        <span class="k">if</span> <span class="n">decreasing</span><span class="p">:</span>
<a id="__codelineno-0-417" name="__codelineno-0-417"></a>            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-418" name="__codelineno-0-418"></a>                <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-419" name="__codelineno-0-419"></a>            <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="o">-</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">heights</span>
<a id="__codelineno-0-420" name="__codelineno-0-420"></a>            <span class="n">output_max</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span>
<a id="__codelineno-0-421" name="__codelineno-0-421"></a>        <span class="k">if</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-422" name="__codelineno-0-422"></a>            <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-423" name="__codelineno-0-423"></a>        <span class="n">delta</span> <span class="o">=</span> <span class="n">output_max</span> <span class="o">-</span> <span class="n">bias</span>
<a id="__codelineno-0-424" name="__codelineno-0-424"></a>        <span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
<a id="__codelineno-0-425" name="__codelineno-0-425"></a>            <span class="n">delta</span> <span class="o">&gt;</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">delta</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
<a id="__codelineno-0-426" name="__codelineno-0-426"></a>        <span class="p">)</span>
<a id="__codelineno-0-427" name="__codelineno-0-427"></a>        <span class="n">heights</span> <span class="o">/=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">scaling_factor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
<a id="__codelineno-0-428" name="__codelineno-0-428"></a>        <span class="k">if</span> <span class="n">decreasing</span><span class="p">:</span>
<a id="__codelineno-0-429" name="__codelineno-0-429"></a>            <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="o">-</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">heights</span>
<a id="__codelineno-0-430" name="__codelineno-0-430"></a>        <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.NumericalCalibrator.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">input_keypoints</span><span class="p">,</span> <span class="n">missing_input_value</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_min</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_max</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">monotonicity</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel_init</span><span class="o">=</span><span class="n">NumericalCalibratorInit</span><span class="o">.</span><span class="n">EQUAL_HEIGHTS</span><span class="p">,</span> <span class="n">projection_iterations</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">input_keypoints_type</span><span class="o">=</span><span class="n">InputKeypointsType</span><span class="o">.</span><span class="n">FIXED</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initializes an instance of <code>NumericalCalibrator</code>.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>input_keypoints</code></td>
          <td>
                <code><span title="numpy.ndarray">ndarray</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Ordered list of float-valued keypoints for the underlying
piece-wise linear function.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>missing_input_value</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If provided, the calibrator will learn to map all
instances of this missing input value to a learned output value.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>output_min</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Minimum output value. If <code>None</code>, the minimum output value will
be unbounded.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>output_max</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[float]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Maximum output value. If <code>None</code>, the maximum output value will
be unbounded.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>monotonicity</code></td>
          <td>
                <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="pytorch_lattice.enums.Monotonicity" href="../enums/#pytorch_lattice.enums.Monotonicity">Monotonicity</a>]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Monotonicity constraint for the underlying piece-wise linear
function.</p>
            </div>
          </td>
          <td>
                <code>None</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>kernel_init</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="pytorch_lattice.enums.NumericalCalibratorInit" href="../enums/#pytorch_lattice.enums.NumericalCalibratorInit">NumericalCalibratorInit</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Initialization scheme to use for the kernel.</p>
            </div>
          </td>
          <td>
                <code><span title="pytorch_lattice.enums.NumericalCalibratorInit.EQUAL_HEIGHTS">EQUAL_HEIGHTS</span></code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>projection_iterations</code></td>
          <td>
                <code>int</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>Number of times to run Dykstra's projection
algorithm when applying constraints.</p>
            </div>
          </td>
          <td>
                <code>8</code>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td><code>input_keypoints_type</code></td>
          <td>
                <code><a class="autorefs autorefs-internal" title="pytorch_lattice.enums.InputKeypointsType" href="../enums/#pytorch_lattice.enums.InputKeypointsType">InputKeypointsType</a></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p><code>InputKeypointType</code> of either <code>FIXED</code> or <code>LEARNED</code>. If
<code>LEARNED</code>, keypoints other than the first or last will follow
<code>input_keypoints</code> for initialization but adapt during training.</p>
            </div>
          </td>
          <td>
                <code><span title="pytorch_lattice.enums.InputKeypointsType.FIXED">FIXED</span></code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Raises:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>ValueError</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>If <code>kernel_init</code> is invalid.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-44"> 44</a></span>
<span class="normal"><a href="#__codelineno-0-45"> 45</a></span>
<span class="normal"><a href="#__codelineno-0-46"> 46</a></span>
<span class="normal"><a href="#__codelineno-0-47"> 47</a></span>
<span class="normal"><a href="#__codelineno-0-48"> 48</a></span>
<span class="normal"><a href="#__codelineno-0-49"> 49</a></span>
<span class="normal"><a href="#__codelineno-0-50"> 50</a></span>
<span class="normal"><a href="#__codelineno-0-51"> 51</a></span>
<span class="normal"><a href="#__codelineno-0-52"> 52</a></span>
<span class="normal"><a href="#__codelineno-0-53"> 53</a></span>
<span class="normal"><a href="#__codelineno-0-54"> 54</a></span>
<span class="normal"><a href="#__codelineno-0-55"> 55</a></span>
<span class="normal"><a href="#__codelineno-0-56"> 56</a></span>
<span class="normal"><a href="#__codelineno-0-57"> 57</a></span>
<span class="normal"><a href="#__codelineno-0-58"> 58</a></span>
<span class="normal"><a href="#__codelineno-0-59"> 59</a></span>
<span class="normal"><a href="#__codelineno-0-60"> 60</a></span>
<span class="normal"><a href="#__codelineno-0-61"> 61</a></span>
<span class="normal"><a href="#__codelineno-0-62"> 62</a></span>
<span class="normal"><a href="#__codelineno-0-63"> 63</a></span>
<span class="normal"><a href="#__codelineno-0-64"> 64</a></span>
<span class="normal"><a href="#__codelineno-0-65"> 65</a></span>
<span class="normal"><a href="#__codelineno-0-66"> 66</a></span>
<span class="normal"><a href="#__codelineno-0-67"> 67</a></span>
<span class="normal"><a href="#__codelineno-0-68"> 68</a></span>
<span class="normal"><a href="#__codelineno-0-69"> 69</a></span>
<span class="normal"><a href="#__codelineno-0-70"> 70</a></span>
<span class="normal"><a href="#__codelineno-0-71"> 71</a></span>
<span class="normal"><a href="#__codelineno-0-72"> 72</a></span>
<span class="normal"><a href="#__codelineno-0-73"> 73</a></span>
<span class="normal"><a href="#__codelineno-0-74"> 74</a></span>
<span class="normal"><a href="#__codelineno-0-75"> 75</a></span>
<span class="normal"><a href="#__codelineno-0-76"> 76</a></span>
<span class="normal"><a href="#__codelineno-0-77"> 77</a></span>
<span class="normal"><a href="#__codelineno-0-78"> 78</a></span>
<span class="normal"><a href="#__codelineno-0-79"> 79</a></span>
<span class="normal"><a href="#__codelineno-0-80"> 80</a></span>
<span class="normal"><a href="#__codelineno-0-81"> 81</a></span>
<span class="normal"><a href="#__codelineno-0-82"> 82</a></span>
<span class="normal"><a href="#__codelineno-0-83"> 83</a></span>
<span class="normal"><a href="#__codelineno-0-84"> 84</a></span>
<span class="normal"><a href="#__codelineno-0-85"> 85</a></span>
<span class="normal"><a href="#__codelineno-0-86"> 86</a></span>
<span class="normal"><a href="#__codelineno-0-87"> 87</a></span>
<span class="normal"><a href="#__codelineno-0-88"> 88</a></span>
<span class="normal"><a href="#__codelineno-0-89"> 89</a></span>
<span class="normal"><a href="#__codelineno-0-90"> 90</a></span>
<span class="normal"><a href="#__codelineno-0-91"> 91</a></span>
<span class="normal"><a href="#__codelineno-0-92"> 92</a></span>
<span class="normal"><a href="#__codelineno-0-93"> 93</a></span>
<span class="normal"><a href="#__codelineno-0-94"> 94</a></span>
<span class="normal"><a href="#__codelineno-0-95"> 95</a></span>
<span class="normal"><a href="#__codelineno-0-96"> 96</a></span>
<span class="normal"><a href="#__codelineno-0-97"> 97</a></span>
<span class="normal"><a href="#__codelineno-0-98"> 98</a></span>
<span class="normal"><a href="#__codelineno-0-99"> 99</a></span>
<span class="normal"><a href="#__codelineno-0-100">100</a></span>
<span class="normal"><a href="#__codelineno-0-101">101</a></span>
<span class="normal"><a href="#__codelineno-0-102">102</a></span>
<span class="normal"><a href="#__codelineno-0-103">103</a></span>
<span class="normal"><a href="#__codelineno-0-104">104</a></span>
<span class="normal"><a href="#__codelineno-0-105">105</a></span>
<span class="normal"><a href="#__codelineno-0-106">106</a></span>
<span class="normal"><a href="#__codelineno-0-107">107</a></span>
<span class="normal"><a href="#__codelineno-0-108">108</a></span>
<span class="normal"><a href="#__codelineno-0-109">109</a></span>
<span class="normal"><a href="#__codelineno-0-110">110</a></span>
<span class="normal"><a href="#__codelineno-0-111">111</a></span>
<span class="normal"><a href="#__codelineno-0-112">112</a></span>
<span class="normal"><a href="#__codelineno-0-113">113</a></span>
<span class="normal"><a href="#__codelineno-0-114">114</a></span>
<span class="normal"><a href="#__codelineno-0-115">115</a></span>
<span class="normal"><a href="#__codelineno-0-116">116</a></span>
<span class="normal"><a href="#__codelineno-0-117">117</a></span>
<span class="normal"><a href="#__codelineno-0-118">118</a></span>
<span class="normal"><a href="#__codelineno-0-119">119</a></span>
<span class="normal"><a href="#__codelineno-0-120">120</a></span>
<span class="normal"><a href="#__codelineno-0-121">121</a></span>
<span class="normal"><a href="#__codelineno-0-122">122</a></span>
<span class="normal"><a href="#__codelineno-0-123">123</a></span>
<span class="normal"><a href="#__codelineno-0-124">124</a></span>
<span class="normal"><a href="#__codelineno-0-125">125</a></span>
<span class="normal"><a href="#__codelineno-0-126">126</a></span>
<span class="normal"><a href="#__codelineno-0-127">127</a></span>
<span class="normal"><a href="#__codelineno-0-128">128</a></span>
<span class="normal"><a href="#__codelineno-0-129">129</a></span>
<span class="normal"><a href="#__codelineno-0-130">130</a></span>
<span class="normal"><a href="#__codelineno-0-131">131</a></span>
<span class="normal"><a href="#__codelineno-0-132">132</a></span>
<span class="normal"><a href="#__codelineno-0-133">133</a></span>
<span class="normal"><a href="#__codelineno-0-134">134</a></span>
<span class="normal"><a href="#__codelineno-0-135">135</a></span>
<span class="normal"><a href="#__codelineno-0-136">136</a></span>
<span class="normal"><a href="#__codelineno-0-137">137</a></span>
<span class="normal"><a href="#__codelineno-0-138">138</a></span>
<span class="normal"><a href="#__codelineno-0-139">139</a></span>
<span class="normal"><a href="#__codelineno-0-140">140</a></span>
<span class="normal"><a href="#__codelineno-0-141">141</a></span>
<span class="normal"><a href="#__codelineno-0-142">142</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-44" name="__codelineno-0-44"></a><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
<a id="__codelineno-0-45" name="__codelineno-0-45"></a>    <span class="bp">self</span><span class="p">,</span>
<a id="__codelineno-0-46" name="__codelineno-0-46"></a>    <span class="n">input_keypoints</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
<a id="__codelineno-0-47" name="__codelineno-0-47"></a>    <span class="n">missing_input_value</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-48" name="__codelineno-0-48"></a>    <span class="n">output_min</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-49" name="__codelineno-0-49"></a>    <span class="n">output_max</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-50" name="__codelineno-0-50"></a>    <span class="n">monotonicity</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Monotonicity</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-51" name="__codelineno-0-51"></a>    <span class="n">kernel_init</span><span class="p">:</span> <span class="n">NumericalCalibratorInit</span> <span class="o">=</span> <span class="n">NumericalCalibratorInit</span><span class="o">.</span><span class="n">EQUAL_HEIGHTS</span><span class="p">,</span>
<a id="__codelineno-0-52" name="__codelineno-0-52"></a>    <span class="n">projection_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
<a id="__codelineno-0-53" name="__codelineno-0-53"></a>    <span class="n">input_keypoints_type</span><span class="p">:</span> <span class="n">InputKeypointsType</span> <span class="o">=</span> <span class="n">InputKeypointsType</span><span class="o">.</span><span class="n">FIXED</span><span class="p">,</span>
<a id="__codelineno-0-54" name="__codelineno-0-54"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Initializes an instance of `NumericalCalibrator`.</span>
<a id="__codelineno-0-56" name="__codelineno-0-56"></a>
<a id="__codelineno-0-57" name="__codelineno-0-57"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-58" name="__codelineno-0-58"></a><span class="sd">        input_keypoints: Ordered list of float-valued keypoints for the underlying</span>
<a id="__codelineno-0-59" name="__codelineno-0-59"></a><span class="sd">            piece-wise linear function.</span>
<a id="__codelineno-0-60" name="__codelineno-0-60"></a><span class="sd">        missing_input_value: If provided, the calibrator will learn to map all</span>
<a id="__codelineno-0-61" name="__codelineno-0-61"></a><span class="sd">            instances of this missing input value to a learned output value.</span>
<a id="__codelineno-0-62" name="__codelineno-0-62"></a><span class="sd">        output_min: Minimum output value. If `None`, the minimum output value will</span>
<a id="__codelineno-0-63" name="__codelineno-0-63"></a><span class="sd">            be unbounded.</span>
<a id="__codelineno-0-64" name="__codelineno-0-64"></a><span class="sd">        output_max: Maximum output value. If `None`, the maximum output value will</span>
<a id="__codelineno-0-65" name="__codelineno-0-65"></a><span class="sd">            be unbounded.</span>
<a id="__codelineno-0-66" name="__codelineno-0-66"></a><span class="sd">        monotonicity: Monotonicity constraint for the underlying piece-wise linear</span>
<a id="__codelineno-0-67" name="__codelineno-0-67"></a><span class="sd">            function.</span>
<a id="__codelineno-0-68" name="__codelineno-0-68"></a><span class="sd">        kernel_init: Initialization scheme to use for the kernel.</span>
<a id="__codelineno-0-69" name="__codelineno-0-69"></a><span class="sd">        projection_iterations: Number of times to run Dykstra&#39;s projection</span>
<a id="__codelineno-0-70" name="__codelineno-0-70"></a><span class="sd">            algorithm when applying constraints.</span>
<a id="__codelineno-0-71" name="__codelineno-0-71"></a><span class="sd">        input_keypoints_type: `InputKeypointType` of either `FIXED` or `LEARNED`. If</span>
<a id="__codelineno-0-72" name="__codelineno-0-72"></a><span class="sd">            `LEARNED`, keypoints other than the first or last will follow</span>
<a id="__codelineno-0-73" name="__codelineno-0-73"></a><span class="sd">            `input_keypoints` for initialization but adapt during training.</span>
<a id="__codelineno-0-74" name="__codelineno-0-74"></a>
<a id="__codelineno-0-75" name="__codelineno-0-75"></a><span class="sd">    Raises:</span>
<a id="__codelineno-0-76" name="__codelineno-0-76"></a><span class="sd">        ValueError: If `kernel_init` is invalid.</span>
<a id="__codelineno-0-77" name="__codelineno-0-77"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-78" name="__codelineno-0-78"></a>    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-0-79" name="__codelineno-0-79"></a>
<a id="__codelineno-0-80" name="__codelineno-0-80"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">input_keypoints</span> <span class="o">=</span> <span class="n">input_keypoints</span>
<a id="__codelineno-0-81" name="__codelineno-0-81"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="o">=</span> <span class="n">missing_input_value</span>
<a id="__codelineno-0-82" name="__codelineno-0-82"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">=</span> <span class="n">output_min</span>
<a id="__codelineno-0-83" name="__codelineno-0-83"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">=</span> <span class="n">output_max</span>
<a id="__codelineno-0-84" name="__codelineno-0-84"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">=</span> <span class="n">monotonicity</span>
<a id="__codelineno-0-85" name="__codelineno-0-85"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span> <span class="o">=</span> <span class="n">kernel_init</span>
<a id="__codelineno-0-86" name="__codelineno-0-86"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">projection_iterations</span> <span class="o">=</span> <span class="n">projection_iterations</span>
<a id="__codelineno-0-87" name="__codelineno-0-87"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">input_keypoints_type</span> <span class="o">=</span> <span class="n">input_keypoints_type</span>
<a id="__codelineno-0-88" name="__codelineno-0-88"></a>
<a id="__codelineno-0-89" name="__codelineno-0-89"></a>    <span class="c1"># Determine default output initialization values if bounds are not fully set.</span>
<a id="__codelineno-0-90" name="__codelineno-0-90"></a>    <span class="k">if</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-91" name="__codelineno-0-91"></a>        <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-92" name="__codelineno-0-92"></a>    <span class="k">elif</span> <span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-93" name="__codelineno-0-93"></a>        <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_min</span><span class="p">,</span> <span class="n">output_min</span> <span class="o">+</span> <span class="mf">4.0</span>
<a id="__codelineno-0-94" name="__codelineno-0-94"></a>    <span class="k">elif</span> <span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-95" name="__codelineno-0-95"></a>        <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="n">output_max</span> <span class="o">-</span> <span class="mf">4.0</span><span class="p">,</span> <span class="n">output_max</span>
<a id="__codelineno-0-96" name="__codelineno-0-96"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-97" name="__codelineno-0-97"></a>        <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span>
<a id="__codelineno-0-98" name="__codelineno-0-98"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span> <span class="o">=</span> <span class="n">output_init_min</span><span class="p">,</span> <span class="n">output_init_max</span>
<a id="__codelineno-0-99" name="__codelineno-0-99"></a>
<a id="__codelineno-0-100" name="__codelineno-0-100"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_keypoints</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-101" name="__codelineno-0-101"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">input_keypoints</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">input_keypoints</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<a id="__codelineno-0-102" name="__codelineno-0-102"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_keypoints_type</span> <span class="o">==</span> <span class="n">InputKeypointsType</span><span class="o">.</span><span class="n">LEARNED</span><span class="p">:</span>
<a id="__codelineno-0-103" name="__codelineno-0-103"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_min</span> <span class="o">=</span> <span class="n">input_keypoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-104" name="__codelineno-0-104"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_range</span> <span class="o">=</span> <span class="n">input_keypoints</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">input_keypoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-105" name="__codelineno-0-105"></a>        <span class="n">initial_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span>
<a id="__codelineno-0-106" name="__codelineno-0-106"></a>            <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
<a id="__codelineno-0-107" name="__codelineno-0-107"></a>                <span class="p">(</span><span class="n">input_keypoints</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">input_keypoints</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_range</span>
<a id="__codelineno-0-108" name="__codelineno-0-108"></a>            <span class="p">)</span>
<a id="__codelineno-0-109" name="__codelineno-0-109"></a>        <span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-110" name="__codelineno-0-110"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initial_logits</span><span class="p">)</span>
<a id="__codelineno-0-111" name="__codelineno-0-111"></a>
<a id="__codelineno-0-112" name="__codelineno-0-112"></a>    <span class="c1"># First row of the kernel represents the bias. The remaining rows represent</span>
<a id="__codelineno-0-113" name="__codelineno-0-113"></a>    <span class="c1"># the y-value delta compared to the previous point i.e. the segment heights.</span>
<a id="__codelineno-0-114" name="__codelineno-0-114"></a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-115" name="__codelineno-0-115"></a>    <span class="k">def</span> <span class="nf">initialize_kernel</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-116" name="__codelineno-0-116"></a>        <span class="n">output_init_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span>
<a id="__codelineno-0-117" name="__codelineno-0-117"></a>        <span class="k">if</span> <span class="n">kernel_init</span> <span class="o">==</span> <span class="n">NumericalCalibratorInit</span><span class="o">.</span><span class="n">EQUAL_HEIGHTS</span><span class="p">:</span>
<a id="__codelineno-0-118" name="__codelineno-0-118"></a>            <span class="n">num_segments</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-119" name="__codelineno-0-119"></a>            <span class="n">segment_height</span> <span class="o">=</span> <span class="n">output_init_range</span> <span class="o">/</span> <span class="n">num_segments</span>
<a id="__codelineno-0-120" name="__codelineno-0-120"></a>            <span class="n">heights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="n">segment_height</span><span class="p">]]</span> <span class="o">*</span> <span class="n">num_segments</span><span class="p">)</span>
<a id="__codelineno-0-121" name="__codelineno-0-121"></a>        <span class="k">elif</span> <span class="n">kernel_init</span> <span class="o">==</span> <span class="n">NumericalCalibratorInit</span><span class="o">.</span><span class="n">EQUAL_SLOPES</span><span class="p">:</span>
<a id="__codelineno-0-122" name="__codelineno-0-122"></a>            <span class="n">heights</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-123" name="__codelineno-0-123"></a>                <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span> <span class="o">*</span> <span class="n">output_init_range</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span><span class="p">)</span>
<a id="__codelineno-0-124" name="__codelineno-0-124"></a>            <span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<a id="__codelineno-0-125" name="__codelineno-0-125"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-126" name="__codelineno-0-126"></a>            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown kernel init: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_init</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-0-127" name="__codelineno-0-127"></a>
<a id="__codelineno-0-128" name="__codelineno-0-128"></a>        <span class="k">if</span> <span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span><span class="p">:</span>
<a id="__codelineno-0-129" name="__codelineno-0-129"></a>            <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span><span class="p">]])</span>
<a id="__codelineno-0-130" name="__codelineno-0-130"></a>            <span class="n">heights</span> <span class="o">=</span> <span class="o">-</span><span class="n">heights</span>
<a id="__codelineno-0-131" name="__codelineno-0-131"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-132" name="__codelineno-0-132"></a>            <span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span><span class="p">]])</span>
<a id="__codelineno-0-133" name="__codelineno-0-133"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<a id="__codelineno-0-134" name="__codelineno-0-134"></a>
<a id="__codelineno-0-135" name="__codelineno-0-135"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">initialize_kernel</span><span class="p">())</span>
<a id="__codelineno-0-136" name="__codelineno-0-136"></a>
<a id="__codelineno-0-137" name="__codelineno-0-137"></a>    <span class="k">if</span> <span class="n">missing_input_value</span><span class="p">:</span>
<a id="__codelineno-0-138" name="__codelineno-0-138"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">missing_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<a id="__codelineno-0-139" name="__codelineno-0-139"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span>
<a id="__codelineno-0-140" name="__codelineno-0-140"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">missing_output</span><span class="p">,</span>
<a id="__codelineno-0-141" name="__codelineno-0-141"></a>            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_init_min</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_init_max</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span>
<a id="__codelineno-0-142" name="__codelineno-0-142"></a>        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.NumericalCalibrator.apply_constraints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">apply_constraints</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Jointly projects kernel into desired constraints.</p>
<p>Uses Dykstra's alternating projection algorithm to jointly project onto all
given constraints. This algorithm projects with respect to the L2 norm, but it
approached the norm from the "wrong" side. To ensure that all constraints are
strictly met, we do final approximate projections that project strictly into the
feasible space, but this is not an exact projection with respect to the L2 norm.
Enough iterations make the impact of this approximation negligible.</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-179">179</a></span>
<span class="normal"><a href="#__codelineno-0-180">180</a></span>
<span class="normal"><a href="#__codelineno-0-181">181</a></span>
<span class="normal"><a href="#__codelineno-0-182">182</a></span>
<span class="normal"><a href="#__codelineno-0-183">183</a></span>
<span class="normal"><a href="#__codelineno-0-184">184</a></span>
<span class="normal"><a href="#__codelineno-0-185">185</a></span>
<span class="normal"><a href="#__codelineno-0-186">186</a></span>
<span class="normal"><a href="#__codelineno-0-187">187</a></span>
<span class="normal"><a href="#__codelineno-0-188">188</a></span>
<span class="normal"><a href="#__codelineno-0-189">189</a></span>
<span class="normal"><a href="#__codelineno-0-190">190</a></span>
<span class="normal"><a href="#__codelineno-0-191">191</a></span>
<span class="normal"><a href="#__codelineno-0-192">192</a></span>
<span class="normal"><a href="#__codelineno-0-193">193</a></span>
<span class="normal"><a href="#__codelineno-0-194">194</a></span>
<span class="normal"><a href="#__codelineno-0-195">195</a></span>
<span class="normal"><a href="#__codelineno-0-196">196</a></span>
<span class="normal"><a href="#__codelineno-0-197">197</a></span>
<span class="normal"><a href="#__codelineno-0-198">198</a></span>
<span class="normal"><a href="#__codelineno-0-199">199</a></span>
<span class="normal"><a href="#__codelineno-0-200">200</a></span>
<span class="normal"><a href="#__codelineno-0-201">201</a></span>
<span class="normal"><a href="#__codelineno-0-202">202</a></span>
<span class="normal"><a href="#__codelineno-0-203">203</a></span>
<span class="normal"><a href="#__codelineno-0-204">204</a></span>
<span class="normal"><a href="#__codelineno-0-205">205</a></span>
<span class="normal"><a href="#__codelineno-0-206">206</a></span>
<span class="normal"><a href="#__codelineno-0-207">207</a></span>
<span class="normal"><a href="#__codelineno-0-208">208</a></span>
<span class="normal"><a href="#__codelineno-0-209">209</a></span>
<span class="normal"><a href="#__codelineno-0-210">210</a></span>
<span class="normal"><a href="#__codelineno-0-211">211</a></span>
<span class="normal"><a href="#__codelineno-0-212">212</a></span>
<span class="normal"><a href="#__codelineno-0-213">213</a></span>
<span class="normal"><a href="#__codelineno-0-214">214</a></span>
<span class="normal"><a href="#__codelineno-0-215">215</a></span>
<span class="normal"><a href="#__codelineno-0-216">216</a></span>
<span class="normal"><a href="#__codelineno-0-217">217</a></span>
<span class="normal"><a href="#__codelineno-0-218">218</a></span>
<span class="normal"><a href="#__codelineno-0-219">219</a></span>
<span class="normal"><a href="#__codelineno-0-220">220</a></span>
<span class="normal"><a href="#__codelineno-0-221">221</a></span>
<span class="normal"><a href="#__codelineno-0-222">222</a></span>
<span class="normal"><a href="#__codelineno-0-223">223</a></span>
<span class="normal"><a href="#__codelineno-0-224">224</a></span>
<span class="normal"><a href="#__codelineno-0-225">225</a></span>
<span class="normal"><a href="#__codelineno-0-226">226</a></span>
<span class="normal"><a href="#__codelineno-0-227">227</a></span>
<span class="normal"><a href="#__codelineno-0-228">228</a></span>
<span class="normal"><a href="#__codelineno-0-229">229</a></span>
<span class="normal"><a href="#__codelineno-0-230">230</a></span>
<span class="normal"><a href="#__codelineno-0-231">231</a></span>
<span class="normal"><a href="#__codelineno-0-232">232</a></span>
<span class="normal"><a href="#__codelineno-0-233">233</a></span>
<span class="normal"><a href="#__codelineno-0-234">234</a></span>
<span class="normal"><a href="#__codelineno-0-235">235</a></span>
<span class="normal"><a href="#__codelineno-0-236">236</a></span>
<span class="normal"><a href="#__codelineno-0-237">237</a></span>
<span class="normal"><a href="#__codelineno-0-238">238</a></span>
<span class="normal"><a href="#__codelineno-0-239">239</a></span>
<span class="normal"><a href="#__codelineno-0-240">240</a></span>
<span class="normal"><a href="#__codelineno-0-241">241</a></span>
<span class="normal"><a href="#__codelineno-0-242">242</a></span>
<span class="normal"><a href="#__codelineno-0-243">243</a></span>
<span class="normal"><a href="#__codelineno-0-244">244</a></span>
<span class="normal"><a href="#__codelineno-0-245">245</a></span>
<span class="normal"><a href="#__codelineno-0-246">246</a></span>
<span class="normal"><a href="#__codelineno-0-247">247</a></span>
<span class="normal"><a href="#__codelineno-0-248">248</a></span>
<span class="normal"><a href="#__codelineno-0-249">249</a></span>
<span class="normal"><a href="#__codelineno-0-250">250</a></span>
<span class="normal"><a href="#__codelineno-0-251">251</a></span>
<span class="normal"><a href="#__codelineno-0-252">252</a></span>
<span class="normal"><a href="#__codelineno-0-253">253</a></span>
<span class="normal"><a href="#__codelineno-0-254">254</a></span>
<span class="normal"><a href="#__codelineno-0-255">255</a></span>
<span class="normal"><a href="#__codelineno-0-256">256</a></span>
<span class="normal"><a href="#__codelineno-0-257">257</a></span>
<span class="normal"><a href="#__codelineno-0-258">258</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-179" name="__codelineno-0-179"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-180" name="__codelineno-0-180"></a><span class="k">def</span> <span class="nf">apply_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-181" name="__codelineno-0-181"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Jointly projects kernel into desired constraints.</span>
<a id="__codelineno-0-182" name="__codelineno-0-182"></a>
<a id="__codelineno-0-183" name="__codelineno-0-183"></a><span class="sd">    Uses Dykstra&#39;s alternating projection algorithm to jointly project onto all</span>
<a id="__codelineno-0-184" name="__codelineno-0-184"></a><span class="sd">    given constraints. This algorithm projects with respect to the L2 norm, but it</span>
<a id="__codelineno-0-185" name="__codelineno-0-185"></a><span class="sd">    approached the norm from the &quot;wrong&quot; side. To ensure that all constraints are</span>
<a id="__codelineno-0-186" name="__codelineno-0-186"></a><span class="sd">    strictly met, we do final approximate projections that project strictly into the</span>
<a id="__codelineno-0-187" name="__codelineno-0-187"></a><span class="sd">    feasible space, but this is not an exact projection with respect to the L2 norm.</span>
<a id="__codelineno-0-188" name="__codelineno-0-188"></a><span class="sd">    Enough iterations make the impact of this approximation negligible.</span>
<a id="__codelineno-0-189" name="__codelineno-0-189"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-190" name="__codelineno-0-190"></a>    <span class="n">constrain_bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-191" name="__codelineno-0-191"></a>    <span class="n">constrain_monotonicity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-192" name="__codelineno-0-192"></a>    <span class="n">num_constraints</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">constrain_bounds</span><span class="p">,</span> <span class="n">constrain_monotonicity</span><span class="p">])</span>
<a id="__codelineno-0-193" name="__codelineno-0-193"></a>
<a id="__codelineno-0-194" name="__codelineno-0-194"></a>    <span class="c1"># We do nothing to the weights in this case</span>
<a id="__codelineno-0-195" name="__codelineno-0-195"></a>    <span class="k">if</span> <span class="n">num_constraints</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-196" name="__codelineno-0-196"></a>        <span class="k">return</span>
<a id="__codelineno-0-197" name="__codelineno-0-197"></a>
<a id="__codelineno-0-198" name="__codelineno-0-198"></a>    <span class="n">original_bias</span><span class="p">,</span> <span class="n">original_heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<a id="__codelineno-0-199" name="__codelineno-0-199"></a>    <span class="n">previous_bias_delta</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
<a id="__codelineno-0-200" name="__codelineno-0-200"></a>        <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">original_bias</span><span class="p">)</span>
<a id="__codelineno-0-201" name="__codelineno-0-201"></a>    <span class="p">)</span>
<a id="__codelineno-0-202" name="__codelineno-0-202"></a>    <span class="n">previous_heights_delta</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span>
<a id="__codelineno-0-203" name="__codelineno-0-203"></a>        <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">original_heights</span><span class="p">)</span>
<a id="__codelineno-0-204" name="__codelineno-0-204"></a>    <span class="p">)</span>
<a id="__codelineno-0-205" name="__codelineno-0-205"></a>
<a id="__codelineno-0-206" name="__codelineno-0-206"></a>    <span class="k">def</span> <span class="nf">apply_bound_constraints</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">):</span>
<a id="__codelineno-0-207" name="__codelineno-0-207"></a>        <span class="n">previous_bias</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">previous_bias_delta</span><span class="p">[</span><span class="s2">&quot;BOUNDS&quot;</span><span class="p">]</span>
<a id="__codelineno-0-208" name="__codelineno-0-208"></a>        <span class="n">previous_heights</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">-</span> <span class="n">previous_heights_delta</span><span class="p">[</span><span class="s2">&quot;BOUNDS&quot;</span><span class="p">]</span>
<a id="__codelineno-0-209" name="__codelineno-0-209"></a>        <span class="k">if</span> <span class="n">constrain_monotonicity</span><span class="p">:</span>
<a id="__codelineno-0-210" name="__codelineno-0-210"></a>            <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_project_monotonic_bounds</span><span class="p">(</span>
<a id="__codelineno-0-211" name="__codelineno-0-211"></a>                <span class="n">previous_bias</span><span class="p">,</span> <span class="n">previous_heights</span>
<a id="__codelineno-0-212" name="__codelineno-0-212"></a>            <span class="p">)</span>
<a id="__codelineno-0-213" name="__codelineno-0-213"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-214" name="__codelineno-0-214"></a>            <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_approximately_project_bounds_only</span><span class="p">(</span>
<a id="__codelineno-0-215" name="__codelineno-0-215"></a>                <span class="n">previous_bias</span><span class="p">,</span> <span class="n">previous_heights</span>
<a id="__codelineno-0-216" name="__codelineno-0-216"></a>            <span class="p">)</span>
<a id="__codelineno-0-217" name="__codelineno-0-217"></a>        <span class="n">previous_bias_delta</span><span class="p">[</span><span class="s2">&quot;BOUNDS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bias</span> <span class="o">-</span> <span class="n">previous_bias</span>
<a id="__codelineno-0-218" name="__codelineno-0-218"></a>        <span class="n">previous_heights_delta</span><span class="p">[</span><span class="s2">&quot;BOUNDS&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">-</span> <span class="n">previous_heights</span>
<a id="__codelineno-0-219" name="__codelineno-0-219"></a>        <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-220" name="__codelineno-0-220"></a>
<a id="__codelineno-0-221" name="__codelineno-0-221"></a>    <span class="k">def</span> <span class="nf">apply_monotonicity_constraints</span><span class="p">(</span><span class="n">heights</span><span class="p">):</span>
<a id="__codelineno-0-222" name="__codelineno-0-222"></a>        <span class="n">previous_heights</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">-</span> <span class="n">previous_bias_delta</span><span class="p">[</span><span class="s2">&quot;MONOTONICITY&quot;</span><span class="p">]</span>
<a id="__codelineno-0-223" name="__codelineno-0-223"></a>        <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_project_monotonicity</span><span class="p">(</span><span class="n">previous_heights</span><span class="p">)</span>
<a id="__codelineno-0-224" name="__codelineno-0-224"></a>        <span class="n">previous_heights_delta</span><span class="p">[</span><span class="s2">&quot;MONOTONICITY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">-</span> <span class="n">previous_heights</span>
<a id="__codelineno-0-225" name="__codelineno-0-225"></a>        <span class="k">return</span> <span class="n">heights</span>
<a id="__codelineno-0-226" name="__codelineno-0-226"></a>
<a id="__codelineno-0-227" name="__codelineno-0-227"></a>    <span class="k">def</span> <span class="nf">apply_dykstras_projection</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">):</span>
<a id="__codelineno-0-228" name="__codelineno-0-228"></a>        <span class="k">if</span> <span class="n">constrain_bounds</span><span class="p">:</span>
<a id="__codelineno-0-229" name="__codelineno-0-229"></a>            <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="n">apply_bound_constraints</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">)</span>
<a id="__codelineno-0-230" name="__codelineno-0-230"></a>        <span class="k">if</span> <span class="n">constrain_monotonicity</span><span class="p">:</span>
<a id="__codelineno-0-231" name="__codelineno-0-231"></a>            <span class="n">heights</span> <span class="o">=</span> <span class="n">apply_monotonicity_constraints</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<a id="__codelineno-0-232" name="__codelineno-0-232"></a>        <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-233" name="__codelineno-0-233"></a>
<a id="__codelineno-0-234" name="__codelineno-0-234"></a>    <span class="k">def</span> <span class="nf">finalize_constraints</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">):</span>
<a id="__codelineno-0-235" name="__codelineno-0-235"></a>        <span class="k">if</span> <span class="n">constrain_monotonicity</span><span class="p">:</span>
<a id="__codelineno-0-236" name="__codelineno-0-236"></a>            <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_project_monotonicity</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<a id="__codelineno-0-237" name="__codelineno-0-237"></a>        <span class="k">if</span> <span class="n">constrain_bounds</span><span class="p">:</span>
<a id="__codelineno-0-238" name="__codelineno-0-238"></a>            <span class="k">if</span> <span class="n">constrain_monotonicity</span><span class="p">:</span>
<a id="__codelineno-0-239" name="__codelineno-0-239"></a>                <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_squeeze_by_scaling</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="n">heights</span><span class="p">)</span>
<a id="__codelineno-0-240" name="__codelineno-0-240"></a>            <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-241" name="__codelineno-0-241"></a>                <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_approximately_project_bounds_only</span><span class="p">(</span>
<a id="__codelineno-0-242" name="__codelineno-0-242"></a>                    <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-243" name="__codelineno-0-243"></a>                <span class="p">)</span>
<a id="__codelineno-0-244" name="__codelineno-0-244"></a>        <span class="k">return</span> <span class="n">bias</span><span class="p">,</span> <span class="n">heights</span>
<a id="__codelineno-0-245" name="__codelineno-0-245"></a>
<a id="__codelineno-0-246" name="__codelineno-0-246"></a>    <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span> <span class="o">=</span> <span class="n">apply_dykstras_projection</span><span class="p">(</span>
<a id="__codelineno-0-247" name="__codelineno-0-247"></a>        <span class="n">original_bias</span><span class="p">,</span> <span class="n">original_heights</span>
<a id="__codelineno-0-248" name="__codelineno-0-248"></a>    <span class="p">)</span>
<a id="__codelineno-0-249" name="__codelineno-0-249"></a>    <span class="k">if</span> <span class="n">num_constraints</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-250" name="__codelineno-0-250"></a>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projection_iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
<a id="__codelineno-0-251" name="__codelineno-0-251"></a>            <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span> <span class="o">=</span> <span class="n">apply_dykstras_projection</span><span class="p">(</span>
<a id="__codelineno-0-252" name="__codelineno-0-252"></a>                <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span>
<a id="__codelineno-0-253" name="__codelineno-0-253"></a>            <span class="p">)</span>
<a id="__codelineno-0-254" name="__codelineno-0-254"></a>        <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span> <span class="o">=</span> <span class="n">finalize_constraints</span><span class="p">(</span>
<a id="__codelineno-0-255" name="__codelineno-0-255"></a>            <span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span>
<a id="__codelineno-0-256" name="__codelineno-0-256"></a>        <span class="p">)</span>
<a id="__codelineno-0-257" name="__codelineno-0-257"></a>
<a id="__codelineno-0-258" name="__codelineno-0-258"></a>    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">projected_bias</span><span class="p">,</span> <span class="n">projected_heights</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.NumericalCalibrator.assert_constraints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">assert_constraints</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Asserts that layer satisfies specified constraints.</p>
<p>This checks that weights follow monotonicity constraints and that the output is
within bounds.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>eps</code></td>
          <td>
                <code>float</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>the margin of error allowed</p>
            </div>
          </td>
          <td>
                <code>1e-06</code>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>A list of messages describing violated constraints including indices of</p>
            </div>
          </td>
        </tr>
        <tr class="doc-section-item">
          <td>
                <code>list[str]</code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>monotonicity violations. If no constraints violated, the list will be empty.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-260">260</a></span>
<span class="normal"><a href="#__codelineno-0-261">261</a></span>
<span class="normal"><a href="#__codelineno-0-262">262</a></span>
<span class="normal"><a href="#__codelineno-0-263">263</a></span>
<span class="normal"><a href="#__codelineno-0-264">264</a></span>
<span class="normal"><a href="#__codelineno-0-265">265</a></span>
<span class="normal"><a href="#__codelineno-0-266">266</a></span>
<span class="normal"><a href="#__codelineno-0-267">267</a></span>
<span class="normal"><a href="#__codelineno-0-268">268</a></span>
<span class="normal"><a href="#__codelineno-0-269">269</a></span>
<span class="normal"><a href="#__codelineno-0-270">270</a></span>
<span class="normal"><a href="#__codelineno-0-271">271</a></span>
<span class="normal"><a href="#__codelineno-0-272">272</a></span>
<span class="normal"><a href="#__codelineno-0-273">273</a></span>
<span class="normal"><a href="#__codelineno-0-274">274</a></span>
<span class="normal"><a href="#__codelineno-0-275">275</a></span>
<span class="normal"><a href="#__codelineno-0-276">276</a></span>
<span class="normal"><a href="#__codelineno-0-277">277</a></span>
<span class="normal"><a href="#__codelineno-0-278">278</a></span>
<span class="normal"><a href="#__codelineno-0-279">279</a></span>
<span class="normal"><a href="#__codelineno-0-280">280</a></span>
<span class="normal"><a href="#__codelineno-0-281">281</a></span>
<span class="normal"><a href="#__codelineno-0-282">282</a></span>
<span class="normal"><a href="#__codelineno-0-283">283</a></span>
<span class="normal"><a href="#__codelineno-0-284">284</a></span>
<span class="normal"><a href="#__codelineno-0-285">285</a></span>
<span class="normal"><a href="#__codelineno-0-286">286</a></span>
<span class="normal"><a href="#__codelineno-0-287">287</a></span>
<span class="normal"><a href="#__codelineno-0-288">288</a></span>
<span class="normal"><a href="#__codelineno-0-289">289</a></span>
<span class="normal"><a href="#__codelineno-0-290">290</a></span>
<span class="normal"><a href="#__codelineno-0-291">291</a></span>
<span class="normal"><a href="#__codelineno-0-292">292</a></span>
<span class="normal"><a href="#__codelineno-0-293">293</a></span>
<span class="normal"><a href="#__codelineno-0-294">294</a></span>
<span class="normal"><a href="#__codelineno-0-295">295</a></span>
<span class="normal"><a href="#__codelineno-0-296">296</a></span>
<span class="normal"><a href="#__codelineno-0-297">297</a></span>
<span class="normal"><a href="#__codelineno-0-298">298</a></span>
<span class="normal"><a href="#__codelineno-0-299">299</a></span>
<span class="normal"><a href="#__codelineno-0-300">300</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-260" name="__codelineno-0-260"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-261" name="__codelineno-0-261"></a><span class="k">def</span> <span class="nf">assert_constraints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-262" name="__codelineno-0-262"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Asserts that layer satisfies specified constraints.</span>
<a id="__codelineno-0-263" name="__codelineno-0-263"></a>
<a id="__codelineno-0-264" name="__codelineno-0-264"></a><span class="sd">    This checks that weights follow monotonicity constraints and that the output is</span>
<a id="__codelineno-0-265" name="__codelineno-0-265"></a><span class="sd">    within bounds.</span>
<a id="__codelineno-0-266" name="__codelineno-0-266"></a>
<a id="__codelineno-0-267" name="__codelineno-0-267"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-268" name="__codelineno-0-268"></a><span class="sd">        eps: the margin of error allowed</span>
<a id="__codelineno-0-269" name="__codelineno-0-269"></a>
<a id="__codelineno-0-270" name="__codelineno-0-270"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-271" name="__codelineno-0-271"></a><span class="sd">        A list of messages describing violated constraints including indices of</span>
<a id="__codelineno-0-272" name="__codelineno-0-272"></a><span class="sd">        monotonicity violations. If no constraints violated, the list will be empty.</span>
<a id="__codelineno-0-273" name="__codelineno-0-273"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-274" name="__codelineno-0-274"></a>    <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<a id="__codelineno-0-275" name="__codelineno-0-275"></a>    <span class="n">messages</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-276" name="__codelineno-0-276"></a>
<a id="__codelineno-0-277" name="__codelineno-0-277"></a>    <span class="k">if</span> <span class="p">(</span>
<a id="__codelineno-0-278" name="__codelineno-0-278"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-279" name="__codelineno-0-279"></a>        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keypoints_outputs</span><span class="p">())</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_max</span> <span class="o">+</span> <span class="n">eps</span>
<a id="__codelineno-0-280" name="__codelineno-0-280"></a>    <span class="p">):</span>
<a id="__codelineno-0-281" name="__codelineno-0-281"></a>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Max weight greater than output_max.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-282" name="__codelineno-0-282"></a>    <span class="k">if</span> <span class="p">(</span>
<a id="__codelineno-0-283" name="__codelineno-0-283"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a id="__codelineno-0-284" name="__codelineno-0-284"></a>        <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">keypoints_outputs</span><span class="p">())</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_min</span> <span class="o">-</span> <span class="n">eps</span>
<a id="__codelineno-0-285" name="__codelineno-0-285"></a>    <span class="p">):</span>
<a id="__codelineno-0-286" name="__codelineno-0-286"></a>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;Min weight less than output_min.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-287" name="__codelineno-0-287"></a>
<a id="__codelineno-0-288" name="__codelineno-0-288"></a>    <span class="n">diffs</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
<a id="__codelineno-0-289" name="__codelineno-0-289"></a>    <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-290" name="__codelineno-0-290"></a>
<a id="__codelineno-0-291" name="__codelineno-0-291"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">INCREASING</span><span class="p">:</span>
<a id="__codelineno-0-292" name="__codelineno-0-292"></a>        <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">diffs</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<a id="__codelineno-0-293" name="__codelineno-0-293"></a>    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">monotonicity</span> <span class="o">==</span> <span class="n">Monotonicity</span><span class="o">.</span><span class="n">DECREASING</span><span class="p">:</span>
<a id="__codelineno-0-294" name="__codelineno-0-294"></a>        <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">(</span><span class="n">diffs</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<a id="__codelineno-0-295" name="__codelineno-0-295"></a>
<a id="__codelineno-0-296" name="__codelineno-0-296"></a>    <span class="n">violation_indices</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">violation_indices</span><span class="p">]</span>
<a id="__codelineno-0-297" name="__codelineno-0-297"></a>    <span class="k">if</span> <span class="n">violation_indices</span><span class="p">:</span>
<a id="__codelineno-0-298" name="__codelineno-0-298"></a>        <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Monotonicity violated at: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">violation_indices</span><span class="p">)</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<a id="__codelineno-0-299" name="__codelineno-0-299"></a>
<a id="__codelineno-0-300" name="__codelineno-0-300"></a>    <span class="k">return</span> <span class="n">messages</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.NumericalCalibrator.forward" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Calibrates numerical inputs through piece-wise linear interpolation.</p>



  <p><span class="doc-section-title">Parameters:</span></p>
  <table>
    <thead>
      <tr>
        <th>Name</th>
        <th>Type</th>
        <th>Description</th>
        <th>Default</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td><code>x</code></td>
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>The input tensor of shape <code>(batch_size, 1)</code>.</p>
            </div>
          </td>
          <td>
              <em>required</em>
          </td>
        </tr>
    </tbody>
  </table>



  <p><span class="doc-section-title">Returns:</span></p>
  <table>
    <thead>
      <tr>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
        <tr class="doc-section-item">
          <td>
                <code><span title="torch.Tensor">Tensor</span></code>
          </td>
          <td>
            <div class="doc-md-description">
              <p>torch.Tensor of shape <code>(batch_size, 1)</code> containing calibrated input values.</p>
            </div>
          </td>
        </tr>
    </tbody>
  </table>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-144">144</a></span>
<span class="normal"><a href="#__codelineno-0-145">145</a></span>
<span class="normal"><a href="#__codelineno-0-146">146</a></span>
<span class="normal"><a href="#__codelineno-0-147">147</a></span>
<span class="normal"><a href="#__codelineno-0-148">148</a></span>
<span class="normal"><a href="#__codelineno-0-149">149</a></span>
<span class="normal"><a href="#__codelineno-0-150">150</a></span>
<span class="normal"><a href="#__codelineno-0-151">151</a></span>
<span class="normal"><a href="#__codelineno-0-152">152</a></span>
<span class="normal"><a href="#__codelineno-0-153">153</a></span>
<span class="normal"><a href="#__codelineno-0-154">154</a></span>
<span class="normal"><a href="#__codelineno-0-155">155</a></span>
<span class="normal"><a href="#__codelineno-0-156">156</a></span>
<span class="normal"><a href="#__codelineno-0-157">157</a></span>
<span class="normal"><a href="#__codelineno-0-158">158</a></span>
<span class="normal"><a href="#__codelineno-0-159">159</a></span>
<span class="normal"><a href="#__codelineno-0-160">160</a></span>
<span class="normal"><a href="#__codelineno-0-161">161</a></span>
<span class="normal"><a href="#__codelineno-0-162">162</a></span>
<span class="normal"><a href="#__codelineno-0-163">163</a></span>
<span class="normal"><a href="#__codelineno-0-164">164</a></span>
<span class="normal"><a href="#__codelineno-0-165">165</a></span>
<span class="normal"><a href="#__codelineno-0-166">166</a></span>
<span class="normal"><a href="#__codelineno-0-167">167</a></span>
<span class="normal"><a href="#__codelineno-0-168">168</a></span>
<span class="normal"><a href="#__codelineno-0-169">169</a></span>
<span class="normal"><a href="#__codelineno-0-170">170</a></span>
<span class="normal"><a href="#__codelineno-0-171">171</a></span>
<span class="normal"><a href="#__codelineno-0-172">172</a></span>
<span class="normal"><a href="#__codelineno-0-173">173</a></span>
<span class="normal"><a href="#__codelineno-0-174">174</a></span>
<span class="normal"><a href="#__codelineno-0-175">175</a></span>
<span class="normal"><a href="#__codelineno-0-176">176</a></span>
<span class="normal"><a href="#__codelineno-0-177">177</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-144" name="__codelineno-0-144"></a><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-145" name="__codelineno-0-145"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Calibrates numerical inputs through piece-wise linear interpolation.</span>
<a id="__codelineno-0-146" name="__codelineno-0-146"></a>
<a id="__codelineno-0-147" name="__codelineno-0-147"></a><span class="sd">    Args:</span>
<a id="__codelineno-0-148" name="__codelineno-0-148"></a><span class="sd">        x: The input tensor of shape `(batch_size, 1)`.</span>
<a id="__codelineno-0-149" name="__codelineno-0-149"></a>
<a id="__codelineno-0-150" name="__codelineno-0-150"></a><span class="sd">    Returns:</span>
<a id="__codelineno-0-151" name="__codelineno-0-151"></a><span class="sd">        torch.Tensor of shape `(batch_size, 1)` containing calibrated input values.</span>
<a id="__codelineno-0-152" name="__codelineno-0-152"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-153" name="__codelineno-0-153"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_keypoints_type</span> <span class="o">==</span> <span class="n">InputKeypointsType</span><span class="o">.</span><span class="n">LEARNED</span><span class="p">:</span>
<a id="__codelineno-0-154" name="__codelineno-0-154"></a>        <span class="n">softmaxed_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
<a id="__codelineno-0-155" name="__codelineno-0-155"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
<a id="__codelineno-0-156" name="__codelineno-0-156"></a>        <span class="p">)</span>
<a id="__codelineno-0-157" name="__codelineno-0-157"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span> <span class="o">=</span> <span class="n">softmaxed_logits</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_range</span>
<a id="__codelineno-0-158" name="__codelineno-0-158"></a>        <span class="n">interior_keypoints</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-159" name="__codelineno-0-159"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_min</span>
<a id="__codelineno-0-160" name="__codelineno-0-160"></a>        <span class="p">)</span>
<a id="__codelineno-0-161" name="__codelineno-0-161"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-162" name="__codelineno-0-162"></a>            <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">_keypoint_min</span><span class="p">]),</span> <span class="n">interior_keypoints</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<a id="__codelineno-0-163" name="__codelineno-0-163"></a>        <span class="p">)</span>
<a id="__codelineno-0-164" name="__codelineno-0-164"></a>
<a id="__codelineno-0-165" name="__codelineno-0-165"></a>    <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span>
<a id="__codelineno-0-166" name="__codelineno-0-166"></a>    <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">interpolation_weights</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span>
<a id="__codelineno-0-167" name="__codelineno-0-167"></a>    <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">interpolation_weights</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">))</span>
<a id="__codelineno-0-168" name="__codelineno-0-168"></a>    <span class="n">interpolation_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-169" name="__codelineno-0-169"></a>        <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">interpolation_weights</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span>
<a id="__codelineno-0-170" name="__codelineno-0-170"></a>    <span class="p">)</span>
<a id="__codelineno-0-171" name="__codelineno-0-171"></a>    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">interpolation_weights</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
<a id="__codelineno-0-172" name="__codelineno-0-172"></a>
<a id="__codelineno-0-173" name="__codelineno-0-173"></a>    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-174" name="__codelineno-0-174"></a>        <span class="n">missing_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_input_value</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
<a id="__codelineno-0-175" name="__codelineno-0-175"></a>        <span class="n">result</span> <span class="o">=</span> <span class="n">missing_mask</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">missing_output</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">missing_mask</span><span class="p">)</span> <span class="o">*</span> <span class="n">result</span>
<a id="__codelineno-0-176" name="__codelineno-0-176"></a>
<a id="__codelineno-0-177" name="__codelineno-0-177"></a>    <span class="k">return</span> <span class="n">result</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.NumericalCalibrator.keypoints_inputs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">keypoints_inputs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns tensor of keypoint inputs.</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-302">302</a></span>
<span class="normal"><a href="#__codelineno-0-303">303</a></span>
<span class="normal"><a href="#__codelineno-0-304">304</a></span>
<span class="normal"><a href="#__codelineno-0-305">305</a></span>
<span class="normal"><a href="#__codelineno-0-306">306</a></span>
<span class="normal"><a href="#__codelineno-0-307">307</a></span>
<span class="normal"><a href="#__codelineno-0-308">308</a></span>
<span class="normal"><a href="#__codelineno-0-309">309</a></span>
<span class="normal"><a href="#__codelineno-0-310">310</a></span>
<span class="normal"><a href="#__codelineno-0-311">311</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-302" name="__codelineno-0-302"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-303" name="__codelineno-0-303"></a><span class="k">def</span> <span class="nf">keypoints_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-304" name="__codelineno-0-304"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns tensor of keypoint inputs.&quot;&quot;&quot;</span>
<a id="__codelineno-0-305" name="__codelineno-0-305"></a>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
<a id="__codelineno-0-306" name="__codelineno-0-306"></a>        <span class="p">(</span>
<a id="__codelineno-0-307" name="__codelineno-0-307"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span><span class="p">,</span>
<a id="__codelineno-0-308" name="__codelineno-0-308"></a>            <span class="bp">self</span><span class="o">.</span><span class="n">_interpolation_keypoints</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lengths</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:],</span>
<a id="__codelineno-0-309" name="__codelineno-0-309"></a>        <span class="p">),</span>
<a id="__codelineno-0-310" name="__codelineno-0-310"></a>        <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-311" name="__codelineno-0-311"></a>    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="pytorch_lattice.layers.NumericalCalibrator.keypoints_outputs" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">keypoints_outputs</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Returns tensor of keypoint outputs.</p>

          <details class="quote">
            <summary>Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-313">313</a></span>
<span class="normal"><a href="#__codelineno-0-314">314</a></span>
<span class="normal"><a href="#__codelineno-0-315">315</a></span>
<span class="normal"><a href="#__codelineno-0-316">316</a></span></pre></div></td><td class="code"><div><pre><span></span><code><a id="__codelineno-0-313" name="__codelineno-0-313"></a><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<a id="__codelineno-0-314" name="__codelineno-0-314"></a><span class="k">def</span> <span class="nf">keypoints_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-315" name="__codelineno-0-315"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns tensor of keypoint outputs.&quot;&quot;&quot;</span>
<a id="__codelineno-0-316" name="__codelineno-0-316"></a>    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>









  




                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../../walkthroughs/uci_adult_income/" class="md-footer__link md-footer__link--prev" aria-label="Previous: UCI Adult Income">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                UCI Adult Income
              </div>
            </div>
          </a>
        
        
          
          <a href="../models/" class="md-footer__link md-footer__link--next" aria-label="Next: models">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                models
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 William Bakst.
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/willbakst" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/WilliamBakst" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/wbakst/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.annotation", "content.code.copy", "content.code.link", "navigation.footer", "navigation.sections", "navigation.tabs", "navigation.top", "search.highlight", "search.suggest", "toc.follow"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.a7c05c9e.min.js"></script>
      
    
  </body>
</html>