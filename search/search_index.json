{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started with PyTorch Lattice","text":"<p>A PyTorch implementation of constrained optimization and modeling techniques</p> <ul> <li>Transparent Models: Glassbox models to provide increased interpretability and insights into your ML models.</li> <li>Shape Constraints: Embed domain knowledge directly into the model through feature constraints.</li> <li>Rate Constraints (Coming soon...): Optimize any PyTorch model under a set of constraints on rates (e.g. FPR &lt; 1%). Rates can be calculated both for the entire dataset as well as specific slices.</li> </ul> <p> </p>"},{"location":"#installation","title":"Installation","text":"<p>Install PyTorch Lattice and start training and analyzing calibrated models in minutes.</p> <pre><code>$ pip install pytorch-lattice\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#step-1-import-the-package","title":"Step 1. Import the package","text":"<p>First, import the PyTorch Lattice library:</p> <pre><code>import pytorch_lattice as pyl\n</code></pre>"},{"location":"#step-2-load-data-and-fit-a-classifier","title":"Step 2. Load data and fit a classifier","text":"<p>Load the UCI Statlog (Heart) dataset. Then create a base classifier and fit it to the data. Creating the base classifier requires only the feature names.</p> <pre><code>X, y = pyl.datasets.heart()\nclf = pyl.Classifier(X.columns).fit(X, y)\n</code></pre>"},{"location":"#step-3-plot-a-feature-calibrator","title":"Step 3. Plot a feature calibrator","text":"<p>Now that you've trained a classifier, you can plot the feature calibrators to better understand how the model is understanding each feature.</p> <pre><code>pyl.plots.calibrator(clf.model, \"thal\")\n</code></pre> <p></p>"},{"location":"#step-4-whats-next","title":"Step 4. What's Next?","text":"<ul> <li>Check out the Concepts section to dive deeper into the library and the core features that make it powerful, such as calibrators and shape constraints.</li> <li>You can follow along with more detailed walkthroughs to get a better understanding of how to utilize the library to effectively model your data. You can also take a look at code examples in the repo.</li> <li>The API Reference contains full details on all classes, methods, functions, etc.</li> </ul>"},{"location":"#related-research","title":"Related Research","text":"<ul> <li>Monotonic Kronecker-Factored Lattice, William Taylor Bakst, Nobuyuki Morioka, Erez Louidor, International Conference on Learning Representations (ICLR), 2021</li> <li>Multidimensional Shape Constraints, Maya Gupta, Erez Louidor, Oleksandr Mangylov, Nobu Morioka, Taman Narayan, Sen Zhao, Proceedings of the 37th International Conference on Machine Learning (PMLR), 2020</li> <li>Deontological Ethics By Monotonicity Shape Constraints, Serena Wang, Maya Gupta, International Conference on Artificial Intelligence and Statistics (AISTATS), 2020</li> <li>Shape Constraints for Set Functions, Andrew Cotter, Maya Gupta, H. Jiang, Erez Louidor, Jim Muller, Taman Narayan, Serena Wang, Tao Zhu. International Conference on Machine Learning (ICML), 2019</li> <li>Diminishing Returns Shape Constraints for Interpretability and Regularization, Maya Gupta, Dara Bahri, Andrew Cotter, Kevin Canini, Advances in Neural Information Processing Systems (NeurIPS), 2018</li> <li>Deep Lattice Networks and Partial Monotonic Functions, Seungil You, Kevin Canini, David Ding, Jan Pfeifer, Maya R. Gupta, Advances in Neural Information Processing Systems (NeurIPS), 2017</li> <li>Fast and Flexible Monotonic Functions with Ensembles of Lattices, Mahdi Milani Fard, Kevin Canini, Andrew Cotter, Jan Pfeifer, Maya Gupta, Advances in Neural Information Processing Systems (NeurIPS), 2016</li> <li>Monotonic Calibrated Interpolated Look-Up Tables, Maya Gupta, Andrew Cotter, Jan Pfeifer, Konstantin Voevodski, Kevin Canini, Alexander Mangylov, Wojciech Moczydlowski, Alexander van Esbroeck, Journal of Machine Learning Research (JMLR), 2016</li> <li>Optimized Regression for Efficient Function Evaluation, Eric Garcia, Raman Arora, Maya R. Gupta, IEEE Transactions on Image Processing, 2012</li> <li>Lattice Regression, Eric Garcia, Maya Gupta, Advances in Neural Information Processing Systems (NeurIPS), 2009</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>PyTorch Lattice welcomes contributions from the community! See the contribution guide for more information on the development workflow. For bugs and feature requests, visit our GitHub Issues and check out our templates.</p>"},{"location":"#how-to-help","title":"How To Help","text":"<p>Any and all help is greatly appreciated! Check out our page on how you can help.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<p>Check out the our roadmap to see what's planned. If there's an item that you really want that isn't assigned or in progress, take a stab at it!</p>"},{"location":"#versioning","title":"Versioning","text":"<p>PyTorch Lattice uses Semantic Versioning.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>First, install pyenv so you can run the code under all of the supported environments. Also make sure to install pyenv-virtualenv so you can create python environments with the correct versions.</p> <p>To install a specific version of python, you can run e.g. <code>pyenv install 3.10.9</code>. You can then create a virtual environment to run and test code locally during development by running the following code from the base directory:</p> <pre><code>pyenv virtualenv {python_version} env-name\npyenv activate env-name\npip install poetry\npoetry install\n</code></pre> <p>If you'd prefer, you can also use conda to manage your python versions and environments. For installing conda, see their installation guide.</p> <p>The following code is an example of how to set up such an environment:</p> <pre><code>conda create -n env-name pip poetry python={python_version}\nconda activate env-name\npoetry install\n</code></pre> <p>Make sure to replace <code>{python_version}</code> in the above snippets with the version you want the environment to use (e.g. 3.10.9) and name the environment accordingly (e.g. env-name-3.10).</p>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Search through existing GitHub Issues to see if what you want to work on has already been added.</p> <ul> <li>If not, please create a new issue. This will help to reduce duplicated work.</li> </ul> </li> <li> <p>For first-time contributors, visit https://github.com/ControlAI/pytorch-lattice and \"Fork\" the repository (see the button in the top right corner).</p> <ul> <li> <p>You'll need to set up SSH authentication.</p> </li> <li> <p>Clone the forked project and point it to the main project:</p> </li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/pytorch-lattice.git\ngit remote add upstream https://github.com/ControlAI/pytorch-lattice.git\n</code></pre> </li> <li> <p>Development.</p> <ul> <li>Make sure you are in sync with the main repo:</li> </ul> <pre><code>git checkout dev\ngit pull upstream dev\n</code></pre> <ul> <li>Create a <code>git</code> feature branch with a meaningful name where you will add your contributions.</li> </ul> <pre><code>git checkout -b meaningful-branch-name\n</code></pre> <ul> <li>Start coding! commit your changes locally as you work:</li> </ul> <pre><code>git add pytorch-lattice/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n</code></pre> <ul> <li>Format your code!</li> </ul> <pre><code>poetry run ruff format .\n</code></pre> <ul> <li>Lint and test your code! From the base directory, run:</li> </ul> <pre><code>poetry run ruff check .\npoetry run mypy .\n</code></pre> </li> <li> <p>Contributions are submitted through GitHub Pull Requests</p> <ul> <li>When you are ready to submit your contribution for review, push your branch:</li> </ul> <pre><code>git push origin meaningful-branch-name\n</code></pre> <ul> <li> <p>Open the printed URL to open a PR. Make sure to fill in a detailed title and description. Submit your PR for review.</p> </li> <li> <p>Link the issue you selected or created under \"Development\"</p> </li> <li> <p>We will review your contribution and add any comments to the PR. Commit any updates you make in response to comments and push them to the branch (they will be automatically included in the PR)</p> </li> </ul> </li> </ol>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>Please conform to the Conventional Commits specification for all PR titles and commits.</p>"},{"location":"contributing/#formatting-linting","title":"Formatting &amp; Linting","text":"<p>In an effort to keep the codebase clean and easy to work with, we use <code>ruff</code> for formatting and both <code>ruff</code> and <code>mypy</code> for linting. Before sending any PR for review, make sure to run both <code>ruff</code> and <code>mypy</code>.</p> <p>If you are using VS Code, then install the extensions in <code>.vscode/extensions.json</code> and the workspace settings should automatically run <code>ruff</code> formatting on save and show <code>ruff</code> and <code>mypy</code> errors.</p>"},{"location":"help/","title":"How to help PyTorch Lattice","text":""},{"location":"help/#star-pytorch-lattice-on-github","title":"Star PyTorch Lattice on GitHub","text":"<p>\u2b50\ufe0f You can \"star\" PyTorch Lattice on GitHub \u2b50\ufe0f</p>"},{"location":"help/#connect-with-the-author","title":"Connect with the author","text":"<ul> <li> <p>Follow me on GitHub</p> <ul> <li>See other related Open Source projects that might help you with machine learning</li> </ul> </li> <li> <p>Follow me on Twitter/X</p> <ul> <li>Tell me how you use lattice models</li> <li>Hear about new announcements or releases</li> </ul> </li> <li> <p>Connect with me on LinkedIn</p> <ul> <li>Give me any feedback about packages or suggestions</li> </ul> </li> </ul>"},{"location":"help/#post-about-pytorch-lattice","title":"Post about PyTorch Lattice","text":"<ul> <li> <p>Twitter, Reddit, Hackernews, LinkedIn, and others.</p> </li> <li> <p>We love to hear about how PyTorch Lattice has helped you and in which project/company you are using it.</p> </li> </ul>"},{"location":"help/#help-others","title":"Help Others","text":"<p>We are a kind and welcoming community that encourages you to help others with their questions on GitHub Issues / Discussions.</p> <ul> <li>Guide for asking questions<ul> <li>First, search through issues and discussions to see if others have faced similar issues</li> <li>Be as specific as possible, add minimal reproducible example</li> <li>List out things you have tried, errors, etc</li> <li>Close the issue if your question has been successfully answered</li> </ul> </li> <li>Guide for answering questions<ul> <li>Understand the question, ask clarifying questions</li> <li>If there is sample code, reproduce the issue with code given by original poster</li> <li>Give them solution or possibly an alternative that might be better than what original poster is trying to do</li> <li>Ask original poster to close the issue</li> </ul> </li> </ul>"},{"location":"help/#review-pull-requests","title":"Review Pull Requests","text":"<p>You are encouraged to review any pull requests. Here is a guideline on how to review a pull request:</p> <ul> <li>Understand the problem the pull request is trying to solve</li> <li>Ask clarification questions to determine whether the pull request belongs in the package</li> <li>Check the code, run it locally, see if it solves the problem described by the pull request</li> <li>Add a comment with screenshots or accompanying code to verify that you have tested it</li> <li>Check for tests<ul> <li>Request the original poster to add tests if they do not exist</li> <li>Check that tests fail before the PR and succeed after</li> </ul> </li> <li>This will greatly speed up the review process for a PR and will ultimately make SOTAI a better package</li> </ul>"},{"location":"why/","title":"Why use PyTorch Lattice?","text":"<p>Many current state-of-the-art machine learning models are built using a black-box modeling approach, which means training an opaque but flexible model like a deep neural net (DNN) on a dataset of training examples. While we know the structure of DNNs, it is precisely this structure that makes them black-box models.</p> <p></p> <p>Every feature goes through a series of fully-connected layers, meaning every node is a function of every feature. Each node becomes a function through training, but the purpose of any individual node is hidden from the user -- only the model knows. How are we supposed to understand or trust a model's predictions if we don't know what any function within the larger system is doing?</p> <p>Furthermore, black-box models are 100% reliant on the training data. This means that if a model is producing funky predictions, the solution is to either (1) find more training data and re-train the model, or (2) discover a new model structure tailored to the given task. Neither option is a great choice for the majority of data scientists and machine learning practitioners -- unless they work at a large tech company with the resources dedicated to making such solutions possible -- since gathering and cleaning data and discovering new model structures are not only inherently difficult tasks but also time and cost intensive.</p> <p>But every data scientist and machine learning practitioner, even those at large tech companies, has run into issues where their model behaves unexpectedly in the wild because the training data is too different from live examples, especially since real-world data distributions change frequently.</p> <p>So, what can we do to reduce the risk of unknown outcomes?</p>"},{"location":"why/#understanding-the-why-of-a-models-predictions","title":"Understanding The Why Of A Model's Predictions","text":"<p>Without the why, a model's prediction is opaque and difficult to trust, even if it's correct. That's why understanding the why is such an active area of research. It's worth noting that there is a distinction between the two approaches in this field that have seen success: Explainability vs. Interpretability.</p> <p>Explainability focuses on explaining a black-box model's predictions, which is a top-down approach. The benefit of this approach is that resulting methods apply to black-box models, meaning that they apply to any machine learning model. The current state-of-the-art explainability technology is Shapley values, which we can use to determine the importance of each feature for any machine learning model. Perhaps we train a model to predict the price of a house and learn that zip code is the most important feature. The downside of this approach is the limitations inherent to a black-box structure. While this knowledge of importance provides general insight into how the model is making predictions, does it really explain anything? How a particular zip code impacts a model's predictions is still a mystery.</p> <p>The sad truth is that Explainability often only points to common sense results -- not illuminating insights.</p> <p>Interpretability is instead a bottom-up approach focused on providing transparency through calibrated models structured specifically with illuminating insights and control in mind. The downside to this approach is that it requires more input from the user; however, this input is invaluable for the model to understand the system in the way we expect. The benefit of this approach is that the resulting models are much easier to understand. For example, we can analyze the way a calibrated model handles individual features by charting the corresponding feature calibration layer -- the layer specific to calibrated models that calibrates the initial input for later layers. For a categorical feature like zip code, the result will be a bar chart that shows us the calibrated values for each zip code. So now we know not only that zip code is the most important feature, but also the relative impact each zip code has on the predicted price. This is a far more granular understanding.</p>"},{"location":"why/#consistently-predicting-how-a-model-will-behave-on-unseen-examples","title":"Consistently Predicting How A Model Will Behave On Unseen Examples","text":"<p>Okay, so we have a way to dig deeper and understand the why. That's great. But we have to remember that why is an afterthought -- for example, something went wrong and we want to know why. Of course, the why is incredibly useful and plays a big part in understanding how a model will behave, but it does not provide any guarantees on future behavior. Trust comes from the ability to predict behavior, so the more consistently one can predict a model's behavior, the more one can trust that model.</p> <p>Consider using a machine learning model to predict credit score where one of the input features is how late someone is on their payments. The behavior we want and expect is for the model to produce a better credit score for someone who pays their bills sooner, all else being equal. We can imagine that it would be unfair to penalize someone for paying their bills sooner. Even if we can understand the why, with black-box modeling we have no such guarantee.</p> <p>With calibrated modeling, we can constrain the shape of the model's function to provide certain guarantees. We call these shape constraints, and they come in many different flavors. The feature for payment lateness is a perfect fit for a decreasing monotonicity shape constraint. A decreasing monotonic functions's output always increases if the input decreases, and vice-versa. We want the model (function) to produce a higher credit score (output) if payment lateness (input) decreases, all else being equal. With PyTorch Lattice, just configure this behavior before training and it will be guaranteed. Pretty cool, right?</p> <p>Now, if you're not here to predict credit scores, you might be wondering how shape constraints can help you. What about the age of a house when predicting its price? Time since last repair for predictive maintenance? Number of similar items purchased when trying to predict a sale?</p> <p>Hopefully it's clear that many real-world features operate under these or similar constraints because they are part of real-world systems with certain fundamental rules. While we can hope that black-box models learn what we would expect from data, the ability to guarantee the behaviors we expect enables a higher level of trust in a model and eliminates toil.</p>"},{"location":"api/classifier/","title":"classifier","text":""},{"location":"api/classifier/#pytorch_lattice.classifier.Classifier","title":"<code>pytorch_lattice.classifier.Classifier</code>","text":"<p>A classifier for tabular data using calibrated models.</p> <p>Note: currently only handles binary classification targets.</p> <p>Example: <pre><code>X, y = pyl.datasets.heart()\nclf = pyl.Classifier(X.columns)\nclf.configure(\"age\").num_keypoints(10).monotonicity(\"increasing\")\nclf.fit(X, y)\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>features</code> <p>A dict mapping feature names to their corresponding <code>FeatureConfig</code> instances.</p> <code>model_config</code> <p>The model configuration to use for fitting the classifier.</p> <code>self.model</code> <p>The fitted model. This will be <code>None</code> until <code>fit</code> is called.</p> Source code in <code>pytorch_lattice/classifier.py</code> <pre><code>class Classifier:\n    \"\"\"A classifier for tabular data using calibrated models.\n\n    Note: currently only handles binary classification targets.\n\n    Example:\n    ```python\n    X, y = pyl.datasets.heart()\n    clf = pyl.Classifier(X.columns)\n    clf.configure(\"age\").num_keypoints(10).monotonicity(\"increasing\")\n    clf.fit(X, y)\n    ```\n\n    Attributes:\n        features: A dict mapping feature names to their corresponding `FeatureConfig`\n            instances.\n        model_config: The model configuration to use for fitting the classifier.\n        self.model: The fitted model. This will be `None` until `fit` is called.\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_names: list[str],\n        model_config: Optional[Union[LinearConfig, LatticeConfig]] = None,\n    ):\n        \"\"\"Initializes an instance of `Classifier`.\"\"\"\n        self.features = {\n            feature_name: FeatureConfig(name=feature_name)\n            for feature_name in feature_names\n        }\n        self.model_config = model_config if model_config is not None else LinearConfig()\n        self.model: Optional[Union[CalibratedLinear, CalibratedLattice]] = None\n\n    def configure(self, feature_name: str):\n        \"\"\"Returns a `FeatureConfig` object for the given feature name.\"\"\"\n        return self.features[feature_name]\n\n    def fit(\n        self,\n        X: pd.DataFrame,\n        y: np.ndarray,\n        epochs: int = 50,\n        batch_size: int = 64,\n        learning_rate: float = 1e-3,\n        shuffle: bool = False,\n    ) -&gt; Classifier:\n        \"\"\"Returns this classifier after fitting a model to the given data.\n\n        Note that calling this function will overwrite any existing model and train a\n        new model from scratch.\n\n        Args:\n            X: A `pd.DataFrame` containing the features for the training data.\n            y: A `np.ndarray` containing the labels for the training data.\n            epochs: The number of epochs for which to fit the classifier.\n            batch_size: The batch size to use for fitting.\n            learning_rate: The learning rate to use for fitting the model.\n            shuffle: Whether to shuffle the data before fitting.\n        \"\"\"\n        model = self._create_model(X)\n        optimizer = torch.optim.Adam(model.parameters(recurse=True), lr=learning_rate)\n        loss_fn = torch.nn.BCEWithLogitsLoss()\n\n        dataset = Dataset(X, y, model.features)\n        dataloader = torch.utils.data.DataLoader(\n            dataset, batch_size=batch_size, shuffle=shuffle\n        )\n        for _ in trange(epochs, desc=\"Training Progress\"):\n            for inputs, labels in dataloader:\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = loss_fn(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                model.apply_constraints()\n\n        self.model = model\n        return self\n\n    def predict(self, X: pd.DataFrame, logits: bool = False) -&gt; np.ndarray:\n        \"\"\"Returns predictions for the given data.\n\n        Args:\n            X: a `pd.DataFrame` containing to data for which to generate predictions.\n            logits: If `True`, returns the logits of the predictions. Otherwise, returns\n                probabilities.\n        \"\"\"\n        if self.model is None:\n            raise RuntimeError(\"Cannot predict before fitting the model.\")\n\n        self.model.eval()\n        X_copy = X[[feature.feature_name for feature in self.model.features]].copy()\n        prepare_features(X_copy, self.model.features)\n        X_tensor = torch.tensor(X_copy.values).double()\n        with torch.no_grad():\n            preds = self.model(X_tensor).numpy()\n\n        if logits:\n            return preds\n        else:\n            return 1.0 / (1.0 + np.exp(-preds))\n\n    def save(self, filepath: str):\n        \"\"\"Saves the classifier to the specified path.\n\n        Args:\n            filepath: The directory where the classifier will be saved. If the directory\n                does not exist, this function will attempt to create it. If the\n                directory already exists, this function will overwrite any existing\n                content with conflicting filenames.\n        \"\"\"\n        if not os.path.exists(filepath):\n            os.makedirs(filepath)\n        with open(os.path.join(filepath, \"clf_attrs.pkl\"), \"wb\") as f:\n            attrs = {key: self.__dict__[key] for key in [\"features\", \"model_config\"]}\n            pickle.dump(attrs, f)\n        if self.model is not None:\n            model_path = os.path.join(filepath, \"model.pt\")\n            torch.save(self.model, model_path)\n\n    @classmethod\n    def load(cls, filepath: str) -&gt; Classifier:\n        \"\"\"Loads a `Classifier` from the specified path.\n\n        Args:\n            filepath: The filepath from which to load the classifier. The filepath\n                should point to the filepath used in the `save` method when saving the\n                classifier.\n\n        Returns:\n            A `Classifier` instance.\n        \"\"\"\n        with open(os.path.join(filepath, \"clf_attrs.pkl\"), \"rb\") as f:\n            attrs = pickle.load(f)\n\n        clf = cls([])\n        clf.__dict__.update(attrs)\n\n        model_path = os.path.join(filepath, \"model.pt\")\n        if os.path.exists(model_path):\n            clf.model = torch.load(model_path)\n\n        return clf\n\n    ################################################################################\n    ############################## PRIVATE METHODS #################################\n    ################################################################################\n\n    def _create_model(\n        self, X: pd.DataFrame\n    ) -&gt; Union[CalibratedLinear, CalibratedLattice]:\n        \"\"\"Returns a model based on `self.features` and `self.model_config`.\"\"\"\n        features: list[Union[CategoricalFeature, NumericalFeature]] = []\n\n        for feature_name, feature in self.features.items():\n            if X[feature_name].dtype.kind in [\"S\", \"O\", \"b\"]:  # string, object, bool\n                if feature._categories is None:\n                    categories = X[feature_name].unique().tolist()\n                    feature.categories(categories)\n                else:\n                    categories = feature._categories\n                if feature._monotonicity is not None and isinstance(\n                    feature._monotonicity, list\n                ):\n                    monotonicity_pairs = feature._monotonicity\n                else:\n                    monotonicity_pairs = None\n                features.append(\n                    CategoricalFeature(\n                        feature_name=feature_name,\n                        categories=categories,\n                        missing_input_value=MISSING_INPUT_VALUE,\n                        monotonicity_pairs=monotonicity_pairs,\n                        lattice_size=feature._lattice_size,\n                    )\n                )\n            else:  # numerical feature\n                if feature._monotonicity is not None and isinstance(\n                    feature._monotonicity, str\n                ):\n                    monotonicity = feature._monotonicity\n                else:\n                    monotonicity = None\n                features.append(\n                    NumericalFeature(\n                        feature_name=feature_name,\n                        data=np.array(X[feature_name].values),\n                        num_keypoints=feature._num_keypoints,\n                        input_keypoints_init=feature._input_keypoints_init,\n                        missing_input_value=MISSING_INPUT_VALUE,\n                        monotonicity=monotonicity,\n                        projection_iterations=feature._projection_iterations,\n                        lattice_size=feature._lattice_size,\n                    )\n                )\n\n        if isinstance(self.model_config, LinearConfig):\n            return CalibratedLinear(\n                features,\n                self.model_config.output_min,\n                self.model_config.output_max,\n                self.model_config.use_bias,\n                self.model_config.output_calibration_num_keypoints,\n            )\n        else:\n            return CalibratedLattice(\n                features,\n                True,\n                self.model_config.output_min,\n                self.model_config.output_max,\n                self.model_config.kernel_init,\n                self.model_config.interpolation,\n                self.model_config.output_calibration_num_keypoints,\n            )\n</code></pre>"},{"location":"api/classifier/#pytorch_lattice.classifier.Classifier.__init__","title":"<code>__init__(feature_names, model_config=None)</code>","text":"<p>Initializes an instance of <code>Classifier</code>.</p> Source code in <code>pytorch_lattice/classifier.py</code> <pre><code>def __init__(\n    self,\n    feature_names: list[str],\n    model_config: Optional[Union[LinearConfig, LatticeConfig]] = None,\n):\n    \"\"\"Initializes an instance of `Classifier`.\"\"\"\n    self.features = {\n        feature_name: FeatureConfig(name=feature_name)\n        for feature_name in feature_names\n    }\n    self.model_config = model_config if model_config is not None else LinearConfig()\n    self.model: Optional[Union[CalibratedLinear, CalibratedLattice]] = None\n</code></pre>"},{"location":"api/classifier/#pytorch_lattice.classifier.Classifier.configure","title":"<code>configure(feature_name)</code>","text":"<p>Returns a <code>FeatureConfig</code> object for the given feature name.</p> Source code in <code>pytorch_lattice/classifier.py</code> <pre><code>def configure(self, feature_name: str):\n    \"\"\"Returns a `FeatureConfig` object for the given feature name.\"\"\"\n    return self.features[feature_name]\n</code></pre>"},{"location":"api/classifier/#pytorch_lattice.classifier.Classifier.fit","title":"<code>fit(X, y, epochs=50, batch_size=64, learning_rate=0.001, shuffle=False)</code>","text":"<p>Returns this classifier after fitting a model to the given data.</p> <p>Note that calling this function will overwrite any existing model and train a new model from scratch.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>A <code>pd.DataFrame</code> containing the features for the training data.</p> required <code>y</code> <code>ndarray</code> <p>A <code>np.ndarray</code> containing the labels for the training data.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs for which to fit the classifier.</p> <code>50</code> <code>batch_size</code> <code>int</code> <p>The batch size to use for fitting.</p> <code>64</code> <code>learning_rate</code> <code>float</code> <p>The learning rate to use for fitting the model.</p> <code>0.001</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before fitting.</p> <code>False</code> Source code in <code>pytorch_lattice/classifier.py</code> <pre><code>def fit(\n    self,\n    X: pd.DataFrame,\n    y: np.ndarray,\n    epochs: int = 50,\n    batch_size: int = 64,\n    learning_rate: float = 1e-3,\n    shuffle: bool = False,\n) -&gt; Classifier:\n    \"\"\"Returns this classifier after fitting a model to the given data.\n\n    Note that calling this function will overwrite any existing model and train a\n    new model from scratch.\n\n    Args:\n        X: A `pd.DataFrame` containing the features for the training data.\n        y: A `np.ndarray` containing the labels for the training data.\n        epochs: The number of epochs for which to fit the classifier.\n        batch_size: The batch size to use for fitting.\n        learning_rate: The learning rate to use for fitting the model.\n        shuffle: Whether to shuffle the data before fitting.\n    \"\"\"\n    model = self._create_model(X)\n    optimizer = torch.optim.Adam(model.parameters(recurse=True), lr=learning_rate)\n    loss_fn = torch.nn.BCEWithLogitsLoss()\n\n    dataset = Dataset(X, y, model.features)\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle\n    )\n    for _ in trange(epochs, desc=\"Training Progress\"):\n        for inputs, labels in dataloader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            model.apply_constraints()\n\n    self.model = model\n    return self\n</code></pre>"},{"location":"api/classifier/#pytorch_lattice.classifier.Classifier.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Loads a <code>Classifier</code> from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The filepath from which to load the classifier. The filepath should point to the filepath used in the <code>save</code> method when saving the classifier.</p> required <p>Returns:</p> Type Description <code>Classifier</code> <p>A <code>Classifier</code> instance.</p> Source code in <code>pytorch_lattice/classifier.py</code> <pre><code>@classmethod\ndef load(cls, filepath: str) -&gt; Classifier:\n    \"\"\"Loads a `Classifier` from the specified path.\n\n    Args:\n        filepath: The filepath from which to load the classifier. The filepath\n            should point to the filepath used in the `save` method when saving the\n            classifier.\n\n    Returns:\n        A `Classifier` instance.\n    \"\"\"\n    with open(os.path.join(filepath, \"clf_attrs.pkl\"), \"rb\") as f:\n        attrs = pickle.load(f)\n\n    clf = cls([])\n    clf.__dict__.update(attrs)\n\n    model_path = os.path.join(filepath, \"model.pt\")\n    if os.path.exists(model_path):\n        clf.model = torch.load(model_path)\n\n    return clf\n</code></pre>"},{"location":"api/classifier/#pytorch_lattice.classifier.Classifier.predict","title":"<code>predict(X, logits=False)</code>","text":"<p>Returns predictions for the given data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>a <code>pd.DataFrame</code> containing to data for which to generate predictions.</p> required <code>logits</code> <code>bool</code> <p>If <code>True</code>, returns the logits of the predictions. Otherwise, returns probabilities.</p> <code>False</code> Source code in <code>pytorch_lattice/classifier.py</code> <pre><code>def predict(self, X: pd.DataFrame, logits: bool = False) -&gt; np.ndarray:\n    \"\"\"Returns predictions for the given data.\n\n    Args:\n        X: a `pd.DataFrame` containing to data for which to generate predictions.\n        logits: If `True`, returns the logits of the predictions. Otherwise, returns\n            probabilities.\n    \"\"\"\n    if self.model is None:\n        raise RuntimeError(\"Cannot predict before fitting the model.\")\n\n    self.model.eval()\n    X_copy = X[[feature.feature_name for feature in self.model.features]].copy()\n    prepare_features(X_copy, self.model.features)\n    X_tensor = torch.tensor(X_copy.values).double()\n    with torch.no_grad():\n        preds = self.model(X_tensor).numpy()\n\n    if logits:\n        return preds\n    else:\n        return 1.0 / (1.0 + np.exp(-preds))\n</code></pre>"},{"location":"api/classifier/#pytorch_lattice.classifier.Classifier.save","title":"<code>save(filepath)</code>","text":"<p>Saves the classifier to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The directory where the classifier will be saved. If the directory does not exist, this function will attempt to create it. If the directory already exists, this function will overwrite any existing content with conflicting filenames.</p> required Source code in <code>pytorch_lattice/classifier.py</code> <pre><code>def save(self, filepath: str):\n    \"\"\"Saves the classifier to the specified path.\n\n    Args:\n        filepath: The directory where the classifier will be saved. If the directory\n            does not exist, this function will attempt to create it. If the\n            directory already exists, this function will overwrite any existing\n            content with conflicting filenames.\n    \"\"\"\n    if not os.path.exists(filepath):\n        os.makedirs(filepath)\n    with open(os.path.join(filepath, \"clf_attrs.pkl\"), \"wb\") as f:\n        attrs = {key: self.__dict__[key] for key in [\"features\", \"model_config\"]}\n        pickle.dump(attrs, f)\n    if self.model is not None:\n        model_path = os.path.join(filepath, \"model.pt\")\n        torch.save(self.model, model_path)\n</code></pre>"},{"location":"api/constrained_module/","title":"constrained_module","text":""},{"location":"api/constrained_module/#pytorch_lattice.constrained_module.ConstrainedModule","title":"<code>pytorch_lattice.constrained_module.ConstrainedModule</code>","text":"<p>             Bases: <code>Module</code></p> <p>A base class for constrained implementations of a <code>torch.nn.Module</code>.</p> Source code in <code>pytorch_lattice/constrained_module.py</code> <pre><code>class ConstrainedModule(torch.nn.Module):\n    \"\"\"A base class for constrained implementations of a `torch.nn.Module`.\"\"\"\n\n    @torch.no_grad()\n    @abstractmethod\n    def apply_constraints(self) -&gt; None:\n        \"\"\"Applies defined constraints to the module.\"\"\"\n        raise NotImplementedError()\n\n    @torch.no_grad()\n    @abstractmethod\n    def assert_constraints(\n        self, eps: float = 1e-6\n    ) -&gt; Union[list[str], dict[str, list[str]]]:\n        \"\"\"Asserts that the module satisfied specified constraints.\"\"\"\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/constrained_module/#pytorch_lattice.constrained_module.ConstrainedModule.apply_constraints","title":"<code>apply_constraints()</code>  <code>abstractmethod</code>","text":"<p>Applies defined constraints to the module.</p> Source code in <code>pytorch_lattice/constrained_module.py</code> <pre><code>@torch.no_grad()\n@abstractmethod\ndef apply_constraints(self) -&gt; None:\n    \"\"\"Applies defined constraints to the module.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/constrained_module/#pytorch_lattice.constrained_module.ConstrainedModule.assert_constraints","title":"<code>assert_constraints(eps=1e-06)</code>  <code>abstractmethod</code>","text":"<p>Asserts that the module satisfied specified constraints.</p> Source code in <code>pytorch_lattice/constrained_module.py</code> <pre><code>@torch.no_grad()\n@abstractmethod\ndef assert_constraints(\n    self, eps: float = 1e-6\n) -&gt; Union[list[str], dict[str, list[str]]]:\n    \"\"\"Asserts that the module satisfied specified constraints.\"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api/datasets/","title":"datasets","text":""},{"location":"api/datasets/#pytorch_lattice.datasets","title":"<code>pytorch_lattice.datasets</code>","text":"<p>Functions for loading datasets to use with the PyTorch Lattice package.</p>"},{"location":"api/datasets/#pytorch_lattice.datasets.adult","title":"<code>adult()</code>","text":"<p>Loads the UCI Adult Income dataset.</p> <p>The UCI Adult Income dataset is a classification dataset with 48,842 rows and 14 columns. The target is binary, with 0 indicating an income of less than $50k and 1 indicating an income of at least $50k. The features are a mix of categorical and numerical features. For more information, see https://archive.ics.uci.edu/dataset/2/adult</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>A tuple <code>(X, y)</code> of the features and target.</p> Source code in <code>pytorch_lattice/datasets.py</code> <pre><code>def adult() -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Loads the UCI Adult Income dataset.\n\n    The UCI Adult Income dataset is a classification dataset with 48,842 rows and 14\n    columns. The target is binary, with 0 indicating an income of less than $50k and 1\n    indicating an income of at least $50k. The features are a mix of categorical and\n    numerical features. For more information, see\n    https://archive.ics.uci.edu/dataset/2/adult\n\n    Returns:\n        A tuple `(X, y)` of the features and target.\n    \"\"\"\n    X = pd.read_csv(\n        \"https://raw.githubusercontent.com/ControlAI/datasets/main/adult.csv\"\n    )\n    y = np.array(X.pop(\"label\").values)\n    return X, y\n</code></pre>"},{"location":"api/datasets/#pytorch_lattice.datasets.heart","title":"<code>heart()</code>","text":"<p>Loads the UCI Statlog (Heart) dataset.</p> <p>The UCI Statlog (Heart) dataset is a classification dataset with 303 rows and 14 columns. The target is binary, with 0 indicating no heart disease and 1 indicating heart disease. The features are a mix of categorical and numerical features. For more information, see https://archive.ics.uci.edu/ml/datasets/heart+Disease.</p> <p>Returns:</p> Type Description <code>tuple[DataFrame, ndarray]</code> <p>A tuple <code>(X, y)</code> of the features and target.</p> Source code in <code>pytorch_lattice/datasets.py</code> <pre><code>def heart() -&gt; tuple[pd.DataFrame, np.ndarray]:\n    \"\"\"Loads the UCI Statlog (Heart) dataset.\n\n    The UCI Statlog (Heart) dataset is a classification dataset with 303 rows and 14\n    columns. The target is binary, with 0 indicating no heart disease and 1 indicating\n    heart disease. The features are a mix of categorical and numerical features. For\n    more information, see https://archive.ics.uci.edu/ml/datasets/heart+Disease.\n\n    Returns:\n        A tuple `(X, y)` of the features and target.\n    \"\"\"\n    X = pd.read_csv(\n        \"https://raw.githubusercontent.com/ControlAI/datasets/main/heart.csv\"\n    )\n    y = np.array(X.pop(\"target\").values)\n    return X, y\n</code></pre>"},{"location":"api/enums/","title":"enums","text":""},{"location":"api/enums/#pytorch_lattice.enums","title":"<code>pytorch_lattice.enums</code>","text":"<p>Enum Classes for PyTorch Lattice.</p>"},{"location":"api/enums/#pytorch_lattice.enums.CategoricalCalibratorInit","title":"<code>CategoricalCalibratorInit</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>Type of kernel initialization to use for CategoricalCalibrator.</p> <ul> <li>UNIFORM: initialize the kernel with uniformly distributed values. The sample range     will be [<code>output_min</code>, <code>output_max</code>] if both are provided.</li> <li>CONSTANT: initialize the kernel with a constant value for all categories. This     value will be <code>(output_min + output_max) / 2</code> if both are provided.</li> </ul> Source code in <code>pytorch_lattice/enums.py</code> <pre><code>class CategoricalCalibratorInit(_Enum):\n    \"\"\"Type of kernel initialization to use for CategoricalCalibrator.\n\n    - UNIFORM: initialize the kernel with uniformly distributed values. The sample range\n        will be [`output_min`, `output_max`] if both are provided.\n    - CONSTANT: initialize the kernel with a constant value for all categories. This\n        value will be `(output_min + output_max) / 2` if both are provided.\n    \"\"\"\n\n    UNIFORM = \"uniform\"\n    CONSTANT = \"constant\"\n</code></pre>"},{"location":"api/enums/#pytorch_lattice.enums.InputKeypointsInit","title":"<code>InputKeypointsInit</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>Type of initialization to use for NumericalCalibrator input keypoints.</p> <ul> <li>QUANTILES: initialize the input keypoints such that each segment will see the same     number of examples.</li> <li>UNIFORM: initialize the input keypoints uniformly spaced in the feature range.</li> </ul> Source code in <code>pytorch_lattice/enums.py</code> <pre><code>class InputKeypointsInit(_Enum):\n    \"\"\"Type of initialization to use for NumericalCalibrator input keypoints.\n\n    - QUANTILES: initialize the input keypoints such that each segment will see the same\n        number of examples.\n    - UNIFORM: initialize the input keypoints uniformly spaced in the feature range.\n    \"\"\"\n\n    QUANTILES = \"quantiles\"\n    UNIFORM = \"uniform\"\n</code></pre>"},{"location":"api/enums/#pytorch_lattice.enums.InputKeypointsType","title":"<code>InputKeypointsType</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>The type of input keypoints to use.</p> <ul> <li>FIXED: the input keypoints will be fixed during initialization.</li> <li>LEARNED: the interior keypoints will learn through training to best fit the     piecewise linear function.</li> </ul> Source code in <code>pytorch_lattice/enums.py</code> <pre><code>class InputKeypointsType(_Enum):\n    \"\"\"The type of input keypoints to use.\n\n    - FIXED: the input keypoints will be fixed during initialization.\n    - LEARNED: the interior keypoints will learn through training to best fit the\n        piecewise linear function.\n    \"\"\"\n\n    FIXED = \"fixed\"\n    LEARNED = \"learned\"\n</code></pre>"},{"location":"api/enums/#pytorch_lattice.enums.Interpolation","title":"<code>Interpolation</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>Enum for interpolation method of lattice.</p> <ul> <li>HYPERCUBE: n-dimensional hypercube surrounding input point(s).</li> <li>SIMPLEX: uses only one of the n! simplices in the n-dim hypercube.</li> </ul> Source code in <code>pytorch_lattice/enums.py</code> <pre><code>class Interpolation(_Enum):\n    \"\"\"Enum for interpolation method of lattice.\n\n    - HYPERCUBE: n-dimensional hypercube surrounding input point(s).\n    - SIMPLEX: uses only one of the n! simplices in the n-dim hypercube.\n    \"\"\"\n\n    HYPERCUBE = \"hypercube\"\n    SIMPLEX = \"simplex\"\n</code></pre>"},{"location":"api/enums/#pytorch_lattice.enums.LatticeInit","title":"<code>LatticeInit</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>Type of kernel initialization to use for CategoricalCalibrator.</p> <ul> <li>LINEAR: initialize the kernel with weights represented by a linear function,     conforming to monotonicity and unimodality constraints.</li> <li>RANDOM_MONOTONIC: initialize the kernel with a uniformly random sampled     lattice layer weight tensor, conforming to monotonicity and unimodality     constraints.</li> </ul> Source code in <code>pytorch_lattice/enums.py</code> <pre><code>class LatticeInit(_Enum):\n    \"\"\"Type of kernel initialization to use for CategoricalCalibrator.\n\n    - LINEAR: initialize the kernel with weights represented by a linear function,\n        conforming to monotonicity and unimodality constraints.\n    - RANDOM_MONOTONIC: initialize the kernel with a uniformly random sampled\n        lattice layer weight tensor, conforming to monotonicity and unimodality\n        constraints.\n    \"\"\"\n\n    LINEAR = \"linear\"\n    RANDOM_MONOTONIC = \"random_monotonic\"\n</code></pre>"},{"location":"api/enums/#pytorch_lattice.enums.Monotonicity","title":"<code>Monotonicity</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>Type of monotonicity constraint.</p> <ul> <li>INCREASING: increasing monotonicity i.e. increasing input increases output.</li> <li>DECREASING: decreasing monotonicity i.e. increasing input decreases output.</li> </ul> Source code in <code>pytorch_lattice/enums.py</code> <pre><code>class Monotonicity(_Enum):\n    \"\"\"Type of monotonicity constraint.\n\n    - INCREASING: increasing monotonicity i.e. increasing input increases output.\n    - DECREASING: decreasing monotonicity i.e. increasing input decreases output.\n    \"\"\"\n\n    INCREASING = \"increasing\"\n    DECREASING = \"decreasing\"\n</code></pre>"},{"location":"api/enums/#pytorch_lattice.enums.NumericalCalibratorInit","title":"<code>NumericalCalibratorInit</code>","text":"<p>             Bases: <code>_Enum</code></p> <p>Type of kernel initialization to use for NumericalCalibrator.</p> <ul> <li>EQUAL_HEIGHTS: initialize the kernel such that all segments have the same height.</li> <li>EQUAL_SLOPES: initialize the kernel such that all segments have the same slope.</li> </ul> Source code in <code>pytorch_lattice/enums.py</code> <pre><code>class NumericalCalibratorInit(_Enum):\n    \"\"\"Type of kernel initialization to use for NumericalCalibrator.\n\n    - EQUAL_HEIGHTS: initialize the kernel such that all segments have the same height.\n    - EQUAL_SLOPES: initialize the kernel such that all segments have the same slope.\n    \"\"\"\n\n    EQUAL_HEIGHTS = \"equal_heights\"\n    EQUAL_SLOPES = \"equal_slopes\"\n</code></pre>"},{"location":"api/feature_config/","title":"feature_config","text":""},{"location":"api/feature_config/#pytorch_lattice.feature_config","title":"<code>pytorch_lattice.feature_config</code>","text":"<p>Configuration objects for the PyTorch Lattice library.</p>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig","title":"<code>FeatureConfig</code>","text":"<p>A configuration object for a feature in a calibrated model.</p> <p>This configuration object handles both numerical and categorical features. If the <code>categeories</code> attribute is <code>None</code>, then this feature will be handled as numerical. Otherwise, it will be handled as categorical.</p> <p>Example: <pre><code>fc = FeatureConfig(name=\"feature_name\").num_keypoints(10).monotonicity(\"increasing\")\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>name</code> <p>The name of the feature.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>class FeatureConfig:\n    \"\"\"A configuration object for a feature in a calibrated model.\n\n    This configuration object handles both numerical and categorical features. If the\n    `categeories` attribute is `None`, then this feature will be handled as numerical.\n    Otherwise, it will be handled as categorical.\n\n    Example:\n    ```python\n    fc = FeatureConfig(name=\"feature_name\").num_keypoints(10).monotonicity(\"increasing\")\n    ```\n\n    Attributes:\n        name: The name of the feature.\n    \"\"\"\n\n    def __init__(self, name: str):\n        \"\"\"Initializes an instance of `FeatureConfig` with default values.\"\"\"\n        self.name = name\n        self._categories: Optional[list[str]] = None\n        self._num_keypoints: int = 5\n        self._input_keypoints_init: InputKeypointsInit = InputKeypointsInit.QUANTILES\n        self._input_keypoints_type: InputKeypointsType = InputKeypointsType.FIXED\n        self._monotonicity: Optional[Union[Monotonicity, list[tuple[str, str]]]] = None\n        self._projection_iterations: int = 8\n        self._lattice_size: int = 2  # only used in lattice models\n\n    def categories(self, categories: list[str]) -&gt; FeatureConfig:\n        \"\"\"Sets the categories for a categorical feature.\"\"\"\n        self._categories = categories\n        return self\n\n    def num_keypoints(self, num_keypoints: int) -&gt; FeatureConfig:\n        \"\"\"Sets the categories for a categorical feature.\"\"\"\n        self._num_keypoints = num_keypoints\n        return self\n\n    def input_keypoints_init(\n        self, input_keypoints_init: InputKeypointsInit\n    ) -&gt; FeatureConfig:\n        \"\"\"Sets the input keypoints initialization method for a numerical calibrator.\"\"\"\n        self._input_keypoints_init = input_keypoints_init\n        return self\n\n    def input_keypoints_type(\n        self, input_keypoints_type: InputKeypointsType\n    ) -&gt; FeatureConfig:\n        \"\"\"Sets the input keypoints type for a numerical calibrator.\"\"\"\n        self._input_keypoints_type = input_keypoints_type\n        return self\n\n    def monotonicity(\n        self, monotonicity: Optional[Union[Monotonicity, list[tuple[str, str]]]]\n    ) -&gt; FeatureConfig:\n        \"\"\"Sets the monotonicity constraint for a feature.\"\"\"\n        self._monotonicity = monotonicity\n        return self\n\n    def projection_iterations(self, projection_iterations: int) -&gt; FeatureConfig:\n        \"\"\"Sets the number of projection iterations for a numerical calibrator.\"\"\"\n        self._projection_iterations = projection_iterations\n        return self\n\n    def lattice_size(self, lattice_size: int) -&gt; FeatureConfig:\n        \"\"\"Sets the lattice size for a feature.\"\"\"\n        self._lattice_size = lattice_size\n        return self\n</code></pre>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig.__init__","title":"<code>__init__(name)</code>","text":"<p>Initializes an instance of <code>FeatureConfig</code> with default values.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"Initializes an instance of `FeatureConfig` with default values.\"\"\"\n    self.name = name\n    self._categories: Optional[list[str]] = None\n    self._num_keypoints: int = 5\n    self._input_keypoints_init: InputKeypointsInit = InputKeypointsInit.QUANTILES\n    self._input_keypoints_type: InputKeypointsType = InputKeypointsType.FIXED\n    self._monotonicity: Optional[Union[Monotonicity, list[tuple[str, str]]]] = None\n    self._projection_iterations: int = 8\n    self._lattice_size: int = 2  # only used in lattice models\n</code></pre>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig.categories","title":"<code>categories(categories)</code>","text":"<p>Sets the categories for a categorical feature.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>def categories(self, categories: list[str]) -&gt; FeatureConfig:\n    \"\"\"Sets the categories for a categorical feature.\"\"\"\n    self._categories = categories\n    return self\n</code></pre>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig.input_keypoints_init","title":"<code>input_keypoints_init(input_keypoints_init)</code>","text":"<p>Sets the input keypoints initialization method for a numerical calibrator.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>def input_keypoints_init(\n    self, input_keypoints_init: InputKeypointsInit\n) -&gt; FeatureConfig:\n    \"\"\"Sets the input keypoints initialization method for a numerical calibrator.\"\"\"\n    self._input_keypoints_init = input_keypoints_init\n    return self\n</code></pre>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig.input_keypoints_type","title":"<code>input_keypoints_type(input_keypoints_type)</code>","text":"<p>Sets the input keypoints type for a numerical calibrator.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>def input_keypoints_type(\n    self, input_keypoints_type: InputKeypointsType\n) -&gt; FeatureConfig:\n    \"\"\"Sets the input keypoints type for a numerical calibrator.\"\"\"\n    self._input_keypoints_type = input_keypoints_type\n    return self\n</code></pre>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig.lattice_size","title":"<code>lattice_size(lattice_size)</code>","text":"<p>Sets the lattice size for a feature.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>def lattice_size(self, lattice_size: int) -&gt; FeatureConfig:\n    \"\"\"Sets the lattice size for a feature.\"\"\"\n    self._lattice_size = lattice_size\n    return self\n</code></pre>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig.monotonicity","title":"<code>monotonicity(monotonicity)</code>","text":"<p>Sets the monotonicity constraint for a feature.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>def monotonicity(\n    self, monotonicity: Optional[Union[Monotonicity, list[tuple[str, str]]]]\n) -&gt; FeatureConfig:\n    \"\"\"Sets the monotonicity constraint for a feature.\"\"\"\n    self._monotonicity = monotonicity\n    return self\n</code></pre>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig.num_keypoints","title":"<code>num_keypoints(num_keypoints)</code>","text":"<p>Sets the categories for a categorical feature.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>def num_keypoints(self, num_keypoints: int) -&gt; FeatureConfig:\n    \"\"\"Sets the categories for a categorical feature.\"\"\"\n    self._num_keypoints = num_keypoints\n    return self\n</code></pre>"},{"location":"api/feature_config/#pytorch_lattice.feature_config.FeatureConfig.projection_iterations","title":"<code>projection_iterations(projection_iterations)</code>","text":"<p>Sets the number of projection iterations for a numerical calibrator.</p> Source code in <code>pytorch_lattice/feature_config.py</code> <pre><code>def projection_iterations(self, projection_iterations: int) -&gt; FeatureConfig:\n    \"\"\"Sets the number of projection iterations for a numerical calibrator.\"\"\"\n    self._projection_iterations = projection_iterations\n    return self\n</code></pre>"},{"location":"api/layers/","title":"layers","text":""},{"location":"api/layers/#pytorch_lattice.layers.CategoricalCalibrator","title":"<code>pytorch_lattice.layers.CategoricalCalibrator</code>","text":"<p>             Bases: <code>ConstrainedModule</code></p> <p>A categorical calibrator.</p> <p>This module takes an input of shape <code>(batch_size, 1)</code> and calibrates it by mapping a given category to its learned output value. The output will have the same shape as the input.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>__init__</code> arguments.</p> <code>kernel</code> <p><code>torch.nn.Parameter</code> that stores the categorical mapping weights.</p> <p>Example: <pre><code>inputs = torch.tensor(...)  # shape: (batch_size, 1)\ncalibrator = CategoricalCalibrator(\n    num_categories=5,\n    missing_input_value=-1,\n    output_min=0.0\n    output_max=1.0,\n    monotonicity_pairs=[(0, 1), (1, 2)],\n    kernel_init=CateegoricalCalibratorInit.UNIFORM,\n)\noutputs = calibrator(inputs)\n</code></pre></p> Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code> <pre><code>class CategoricalCalibrator(ConstrainedModule):\n    \"\"\"A categorical calibrator.\n\n    This module takes an input of shape `(batch_size, 1)` and calibrates it by mapping a\n    given category to its learned output value. The output will have the same shape as\n    the input.\n\n    Attributes:\n        All: `__init__` arguments.\n        kernel: `torch.nn.Parameter` that stores the categorical mapping weights.\n\n    Example:\n    ```python\n    inputs = torch.tensor(...)  # shape: (batch_size, 1)\n    calibrator = CategoricalCalibrator(\n        num_categories=5,\n        missing_input_value=-1,\n        output_min=0.0\n        output_max=1.0,\n        monotonicity_pairs=[(0, 1), (1, 2)],\n        kernel_init=CateegoricalCalibratorInit.UNIFORM,\n    )\n    outputs = calibrator(inputs)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        num_categories: int,\n        missing_input_value: Optional[float] = None,\n        output_min: Optional[float] = None,\n        output_max: Optional[float] = None,\n        monotonicity_pairs: Optional[list[tuple[int, int]]] = None,\n        kernel_init: CategoricalCalibratorInit = CategoricalCalibratorInit.UNIFORM,\n    ) -&gt; None:\n        \"\"\"Initializes an instance of `CategoricalCalibrator`.\n\n        Args:\n            num_categories: The number of known categories.\n            missing_input_value: If provided, the calibrator will learn to map all\n                instances of this missing input value to a learned output value just\n                the same as it does for known categories. Note that `num_categories`\n                will be one greater to include this missing category.\n            output_min: Minimum output value. If `None`, the minimum output value will\n                be unbounded.\n            output_max: Maximum output value. If `None`, the maximum output value will\n                be unbounded.\n            monotonicity_pairs: List of pairs of indices `(i,j)` indicating that the\n                calibrator output for index `j` should be greater than or equal to that\n                of index `i`.\n            kernel_init: Initialization scheme to use for the kernel.\n\n        Raises:\n            ValueError: If `monotonicity_pairs` is cyclic.\n            ValueError: If `kernel_init` is invalid.\n        \"\"\"\n        super().__init__()\n\n        self.num_categories = (\n            num_categories + 1 if missing_input_value is not None else num_categories\n        )\n        self.missing_input_value = missing_input_value\n        self.output_min = output_min\n        self.output_max = output_max\n        self.monotonicity_pairs = monotonicity_pairs\n        if monotonicity_pairs:\n            self._monotonicity_graph = defaultdict(list)\n            self._reverse_monotonicity_graph = defaultdict(list)\n            for i, j in monotonicity_pairs:\n                self._monotonicity_graph[i].append(j)\n                self._reverse_monotonicity_graph[j].append(i)\n            try:\n                self._monotonically_sorted_indices = [\n                    *TopologicalSorter(self._reverse_monotonicity_graph).static_order()\n                ]\n            except CycleError as exc:\n                raise ValueError(\"monotonicity_pairs is cyclic\") from exc\n        self.kernel_init = kernel_init\n\n        self.kernel = torch.nn.Parameter(torch.Tensor(self.num_categories, 1).double())\n        if kernel_init == CategoricalCalibratorInit.CONSTANT:\n            if output_min is not None and output_max is not None:\n                init_value = (output_min + output_max) / 2\n            elif output_min is not None:\n                init_value = output_min\n            elif output_max is not None:\n                init_value = output_max\n            else:\n                init_value = 0.0\n            torch.nn.init.constant_(self.kernel, init_value)\n        elif kernel_init == CategoricalCalibratorInit.UNIFORM:\n            if output_min is not None and output_max is not None:\n                low, high = output_min, output_max\n            elif output_min is None and output_max is not None:\n                low, high = output_max - 0.05, output_max\n            elif output_min is not None and output_max is None:\n                low, high = output_min, output_min + 0.05\n            else:\n                low, high = -0.05, 0.05\n            torch.nn.init.uniform_(self.kernel, low, high)\n        else:\n            raise ValueError(f\"Unknown kernel init: {kernel_init}\")\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Calibrates categorical inputs through a learned mapping.\n\n        Args:\n            x: The input tensor of category indices of shape `(batch_size, 1)`.\n\n        Returns:\n            torch.Tensor of shape `(batch_size, 1)` containing calibrated input values.\n        \"\"\"\n        if self.missing_input_value is not None:\n            missing_category_tensor = torch.zeros_like(x) + (self.num_categories - 1)\n            x = torch.where(x == self.missing_input_value, missing_category_tensor, x)\n        # TODO: test if using torch.gather is faster than one-hot matmul.\n        one_hot = torch.nn.functional.one_hot(\n            torch.squeeze(x, -1).long(), num_classes=self.num_categories\n        ).double()\n        return torch.mm(one_hot, self.kernel)\n\n    @torch.no_grad()\n    def apply_constraints(self) -&gt; None:\n        \"\"\"Projects kernel into desired constraints.\"\"\"\n        projected_kernel_data = self.kernel.data\n        if self.monotonicity_pairs:\n            projected_kernel_data = self._approximately_project_monotonicity_pairs(\n                projected_kernel_data\n            )\n        if self.output_min is not None:\n            projected_kernel_data = torch.maximum(\n                projected_kernel_data, torch.tensor(self.output_min)\n            )\n        if self.output_max is not None:\n            projected_kernel_data = torch.minimum(\n                projected_kernel_data, torch.tensor(self.output_max)\n            )\n        self.kernel.data = projected_kernel_data\n\n    @torch.no_grad()\n    def assert_constraints(self, eps: float = 1e-6) -&gt; list[str]:\n        \"\"\"Asserts that layer satisfies specified constraints.\n\n        This checks that weights at the indexes of monotonicity pairs are in the correct\n        order and that the output is within bounds.\n\n        Args:\n            eps: the margin of error allowed\n\n        Returns:\n            A list of messages describing violated constraints including violated\n            monotonicity pairs. If no constraints  violated, the list will be empty.\n        \"\"\"\n        weights = torch.squeeze(self.kernel.data)\n        messages = []\n\n        if self.output_max is not None and torch.max(weights) &gt; self.output_max + eps:\n            messages.append(\"Max weight greater than output_max.\")\n        if self.output_min is not None and torch.min(weights) &lt; self.output_min - eps:\n            messages.append(\"Min weight less than output_min.\")\n\n        if self.monotonicity_pairs:\n            violation_indices = [\n                (i, j)\n                for (i, j) in self.monotonicity_pairs\n                if weights[i] - weights[j] &gt; eps\n            ]\n            if violation_indices:\n                messages.append(f\"Monotonicity violated at: {str(violation_indices)}.\")\n\n        return messages\n\n    @torch.no_grad()\n    def keypoints_inputs(self) -&gt; torch.Tensor:\n        \"\"\"Returns a tensor of keypoint inputs (category indices).\"\"\"\n        if self.missing_input_value is not None:\n            return torch.cat(\n                (\n                    torch.arange(self.num_categories - 1),\n                    torch.tensor([self.missing_input_value]),\n                ),\n                0,\n            )\n        return torch.arange(self.num_categories)\n\n    @torch.no_grad()\n    def keypoints_outputs(self) -&gt; torch.Tensor:\n        \"\"\"Returns a tensor of keypoint outputs.\"\"\"\n        return torch.squeeze(self.kernel.data, -1)\n\n    ################################################################################\n    ############################## PRIVATE METHODS #################################\n    ################################################################################\n\n    def _approximately_project_monotonicity_pairs(self, kernel_data) -&gt; torch.Tensor:\n        \"\"\"Projects kernel such that the monotonicity pairs are satisfied.\n\n        The kernel will be projected such that `kernel_data[i] &lt;= kernel_data[j]`. This\n        results in calibrated outputs that adhere to the desired constraints.\n\n        Args:\n            kernel_data: The tensor of shape `(self.num_categories, 1)` to be projected\n                into the constraints specified by `self.monotonicity pairs`.\n\n        Returns:\n            Projected kernel data. To prevent the kernel from drifting in one direction,\n            the data returned is the average of the min/max and max/min projections.\n        \"\"\"\n        projected_kernel_data = torch.unbind(kernel_data, 0)\n\n        def project(data, monotonicity_graph, step, minimum):\n            projected_data = list(data)\n            sorted_indices = self._monotonically_sorted_indices\n            if minimum:\n                sorted_indices = sorted_indices[::-1]\n            for i in sorted_indices:\n                if i in monotonicity_graph:\n                    projection = projected_data[i]\n                    for j in monotonicity_graph[i]:\n                        if minimum:\n                            projection = torch.minimum(projection, projected_data[j])\n                        else:\n                            projection = torch.maximum(projection, projected_data[j])\n                        if step == 1.0:\n                            projected_data[i] = projection\n                        else:\n                            projected_data[i] = (\n                                step * projection + (1 - step) * projected_data[i]\n                            )\n            return projected_data\n\n        projected_kernel_min_max = project(\n            projected_kernel_data, self._monotonicity_graph, 0.5, minimum=True\n        )\n        projected_kernel_min_max = project(\n            projected_kernel_min_max,\n            self._reverse_monotonicity_graph,\n            1.0,\n            minimum=False,\n        )\n        projected_kernel_min_max = torch.stack(projected_kernel_min_max)\n\n        projected_kernel_max_min = project(\n            projected_kernel_data, self._reverse_monotonicity_graph, 0.5, minimum=False\n        )\n        projected_kernel_max_min = project(\n            projected_kernel_max_min, self._monotonicity_graph, 1.0, minimum=True\n        )\n        projected_kernel_max_min = torch.stack(projected_kernel_max_min)\n\n        return (projected_kernel_min_max + projected_kernel_max_min) / 2\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.CategoricalCalibrator.__init__","title":"<code>__init__(num_categories, missing_input_value=None, output_min=None, output_max=None, monotonicity_pairs=None, kernel_init=CategoricalCalibratorInit.UNIFORM)</code>","text":"<p>Initializes an instance of <code>CategoricalCalibrator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>num_categories</code> <code>int</code> <p>The number of known categories.</p> required <code>missing_input_value</code> <code>Optional[float]</code> <p>If provided, the calibrator will learn to map all instances of this missing input value to a learned output value just the same as it does for known categories. Note that <code>num_categories</code> will be one greater to include this missing category.</p> <code>None</code> <code>output_min</code> <code>Optional[float]</code> <p>Minimum output value. If <code>None</code>, the minimum output value will be unbounded.</p> <code>None</code> <code>output_max</code> <code>Optional[float]</code> <p>Maximum output value. If <code>None</code>, the maximum output value will be unbounded.</p> <code>None</code> <code>monotonicity_pairs</code> <code>Optional[list[tuple[int, int]]]</code> <p>List of pairs of indices <code>(i,j)</code> indicating that the calibrator output for index <code>j</code> should be greater than or equal to that of index <code>i</code>.</p> <code>None</code> <code>kernel_init</code> <code>CategoricalCalibratorInit</code> <p>Initialization scheme to use for the kernel.</p> <code>UNIFORM</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>monotonicity_pairs</code> is cyclic.</p> <code>ValueError</code> <p>If <code>kernel_init</code> is invalid.</p> Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code> <pre><code>def __init__(\n    self,\n    num_categories: int,\n    missing_input_value: Optional[float] = None,\n    output_min: Optional[float] = None,\n    output_max: Optional[float] = None,\n    monotonicity_pairs: Optional[list[tuple[int, int]]] = None,\n    kernel_init: CategoricalCalibratorInit = CategoricalCalibratorInit.UNIFORM,\n) -&gt; None:\n    \"\"\"Initializes an instance of `CategoricalCalibrator`.\n\n    Args:\n        num_categories: The number of known categories.\n        missing_input_value: If provided, the calibrator will learn to map all\n            instances of this missing input value to a learned output value just\n            the same as it does for known categories. Note that `num_categories`\n            will be one greater to include this missing category.\n        output_min: Minimum output value. If `None`, the minimum output value will\n            be unbounded.\n        output_max: Maximum output value. If `None`, the maximum output value will\n            be unbounded.\n        monotonicity_pairs: List of pairs of indices `(i,j)` indicating that the\n            calibrator output for index `j` should be greater than or equal to that\n            of index `i`.\n        kernel_init: Initialization scheme to use for the kernel.\n\n    Raises:\n        ValueError: If `monotonicity_pairs` is cyclic.\n        ValueError: If `kernel_init` is invalid.\n    \"\"\"\n    super().__init__()\n\n    self.num_categories = (\n        num_categories + 1 if missing_input_value is not None else num_categories\n    )\n    self.missing_input_value = missing_input_value\n    self.output_min = output_min\n    self.output_max = output_max\n    self.monotonicity_pairs = monotonicity_pairs\n    if monotonicity_pairs:\n        self._monotonicity_graph = defaultdict(list)\n        self._reverse_monotonicity_graph = defaultdict(list)\n        for i, j in monotonicity_pairs:\n            self._monotonicity_graph[i].append(j)\n            self._reverse_monotonicity_graph[j].append(i)\n        try:\n            self._monotonically_sorted_indices = [\n                *TopologicalSorter(self._reverse_monotonicity_graph).static_order()\n            ]\n        except CycleError as exc:\n            raise ValueError(\"monotonicity_pairs is cyclic\") from exc\n    self.kernel_init = kernel_init\n\n    self.kernel = torch.nn.Parameter(torch.Tensor(self.num_categories, 1).double())\n    if kernel_init == CategoricalCalibratorInit.CONSTANT:\n        if output_min is not None and output_max is not None:\n            init_value = (output_min + output_max) / 2\n        elif output_min is not None:\n            init_value = output_min\n        elif output_max is not None:\n            init_value = output_max\n        else:\n            init_value = 0.0\n        torch.nn.init.constant_(self.kernel, init_value)\n    elif kernel_init == CategoricalCalibratorInit.UNIFORM:\n        if output_min is not None and output_max is not None:\n            low, high = output_min, output_max\n        elif output_min is None and output_max is not None:\n            low, high = output_max - 0.05, output_max\n        elif output_min is not None and output_max is None:\n            low, high = output_min, output_min + 0.05\n        else:\n            low, high = -0.05, 0.05\n        torch.nn.init.uniform_(self.kernel, low, high)\n    else:\n        raise ValueError(f\"Unknown kernel init: {kernel_init}\")\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.CategoricalCalibrator.apply_constraints","title":"<code>apply_constraints()</code>","text":"<p>Projects kernel into desired constraints.</p> Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code> <pre><code>@torch.no_grad()\ndef apply_constraints(self) -&gt; None:\n    \"\"\"Projects kernel into desired constraints.\"\"\"\n    projected_kernel_data = self.kernel.data\n    if self.monotonicity_pairs:\n        projected_kernel_data = self._approximately_project_monotonicity_pairs(\n            projected_kernel_data\n        )\n    if self.output_min is not None:\n        projected_kernel_data = torch.maximum(\n            projected_kernel_data, torch.tensor(self.output_min)\n        )\n    if self.output_max is not None:\n        projected_kernel_data = torch.minimum(\n            projected_kernel_data, torch.tensor(self.output_max)\n        )\n    self.kernel.data = projected_kernel_data\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.CategoricalCalibrator.assert_constraints","title":"<code>assert_constraints(eps=1e-06)</code>","text":"<p>Asserts that layer satisfies specified constraints.</p> <p>This checks that weights at the indexes of monotonicity pairs are in the correct order and that the output is within bounds.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>the margin of error allowed</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of messages describing violated constraints including violated</p> <code>list[str]</code> <p>monotonicity pairs. If no constraints  violated, the list will be empty.</p> Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code> <pre><code>@torch.no_grad()\ndef assert_constraints(self, eps: float = 1e-6) -&gt; list[str]:\n    \"\"\"Asserts that layer satisfies specified constraints.\n\n    This checks that weights at the indexes of monotonicity pairs are in the correct\n    order and that the output is within bounds.\n\n    Args:\n        eps: the margin of error allowed\n\n    Returns:\n        A list of messages describing violated constraints including violated\n        monotonicity pairs. If no constraints  violated, the list will be empty.\n    \"\"\"\n    weights = torch.squeeze(self.kernel.data)\n    messages = []\n\n    if self.output_max is not None and torch.max(weights) &gt; self.output_max + eps:\n        messages.append(\"Max weight greater than output_max.\")\n    if self.output_min is not None and torch.min(weights) &lt; self.output_min - eps:\n        messages.append(\"Min weight less than output_min.\")\n\n    if self.monotonicity_pairs:\n        violation_indices = [\n            (i, j)\n            for (i, j) in self.monotonicity_pairs\n            if weights[i] - weights[j] &gt; eps\n        ]\n        if violation_indices:\n            messages.append(f\"Monotonicity violated at: {str(violation_indices)}.\")\n\n    return messages\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.CategoricalCalibrator.forward","title":"<code>forward(x)</code>","text":"<p>Calibrates categorical inputs through a learned mapping.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of category indices of shape <code>(batch_size, 1)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor of shape <code>(batch_size, 1)</code> containing calibrated input values.</p> Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Calibrates categorical inputs through a learned mapping.\n\n    Args:\n        x: The input tensor of category indices of shape `(batch_size, 1)`.\n\n    Returns:\n        torch.Tensor of shape `(batch_size, 1)` containing calibrated input values.\n    \"\"\"\n    if self.missing_input_value is not None:\n        missing_category_tensor = torch.zeros_like(x) + (self.num_categories - 1)\n        x = torch.where(x == self.missing_input_value, missing_category_tensor, x)\n    # TODO: test if using torch.gather is faster than one-hot matmul.\n    one_hot = torch.nn.functional.one_hot(\n        torch.squeeze(x, -1).long(), num_classes=self.num_categories\n    ).double()\n    return torch.mm(one_hot, self.kernel)\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.CategoricalCalibrator.keypoints_inputs","title":"<code>keypoints_inputs()</code>","text":"<p>Returns a tensor of keypoint inputs (category indices).</p> Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code> <pre><code>@torch.no_grad()\ndef keypoints_inputs(self) -&gt; torch.Tensor:\n    \"\"\"Returns a tensor of keypoint inputs (category indices).\"\"\"\n    if self.missing_input_value is not None:\n        return torch.cat(\n            (\n                torch.arange(self.num_categories - 1),\n                torch.tensor([self.missing_input_value]),\n            ),\n            0,\n        )\n    return torch.arange(self.num_categories)\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.CategoricalCalibrator.keypoints_outputs","title":"<code>keypoints_outputs()</code>","text":"<p>Returns a tensor of keypoint outputs.</p> Source code in <code>pytorch_lattice/layers/categorical_calibrator.py</code> <pre><code>@torch.no_grad()\ndef keypoints_outputs(self) -&gt; torch.Tensor:\n    \"\"\"Returns a tensor of keypoint outputs.\"\"\"\n    return torch.squeeze(self.kernel.data, -1)\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Lattice","title":"<code>pytorch_lattice.layers.Lattice</code>","text":"<p>             Bases: <code>ConstrainedModule</code></p> <p>A Lattice Module.</p> <p>Layer performs interpolation using one of 'units' d-dimensional lattices with arbitrary number of keypoints per dimension. Each lattice vertex has a trainable weight, and input is considered to be a d-dimensional point within the lattice.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>__init__</code> arguments.</p> <code>kernel</code> <p><code>torch.nn.Parameter</code> of shape <code>(prod(lattice_sizes), units)</code> which stores weights at each vertex of lattice.</p> <p>Example: <pre><code>lattice_sizes = [2, 2, 4, 3]\ninputs=torch.tensor(...) # shape: (batch_size, len(lattice_sizes))\nlattice=Lattice(\n    lattice_sizes,\n    clip_inputs=True,\n    interpolation=Interpolation.HYPERCUBE,\n    units=1,\n)\noutputs = Lattice(inputs)\n</code></pre></p> Source code in <code>pytorch_lattice/layers/lattice.py</code> <pre><code>class Lattice(ConstrainedModule):\n    \"\"\"A Lattice Module.\n\n    Layer performs interpolation using one of 'units' d-dimensional lattices with\n    arbitrary number of keypoints per dimension. Each lattice vertex has a trainable\n    weight, and input is considered to be a d-dimensional point within the lattice.\n\n    Attributes:\n        All: `__init__` arguments.\n        kernel: `torch.nn.Parameter` of shape `(prod(lattice_sizes), units)` which\n            stores\n            weights at each vertex of lattice.\n\n    Example:\n    ```python\n    lattice_sizes = [2, 2, 4, 3]\n    inputs=torch.tensor(...) # shape: (batch_size, len(lattice_sizes))\n    lattice=Lattice(\n        lattice_sizes,\n        clip_inputs=True,\n        interpolation=Interpolation.HYPERCUBE,\n        units=1,\n    )\n    outputs = Lattice(inputs)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        lattice_sizes: Union[list[int], tuple[int]],\n        output_min: Optional[float] = None,\n        output_max: Optional[float] = None,\n        kernel_init: LatticeInit = LatticeInit.LINEAR,\n        monotonicities: Optional[list[Optional[Monotonicity]]] = None,\n        clip_inputs: bool = True,\n        interpolation: Interpolation = Interpolation.HYPERCUBE,\n        units: int = 1,\n    ) -&gt; None:\n        \"\"\"Initializes an instance of 'Lattice'.\n\n        Args:\n            lattice_sizes: List or tuple of size of lattice along each dimension.\n            output_min: Minimum output value for weights at vertices of lattice.\n            output_max: Maximum output value for weights at vertices of lattice.\n            kernel_init: Initialization scheme to use for the kernel.\n            monotonicities: `None` or list of `NONE` or\n                `Monotonicity.INCREASING` of length `len(lattice_sizes)` specifying\n                monotonicity of each feature of lattice. A monotonically decreasing\n                 feature should use `Monotonicity.INCREASING` in the lattice layer but\n                `Monotonicity.DECREASING` in the calibrator.\n            clip_inputs: Whether input points should be clipped to the range of lattice.\n            interpolation: Interpolation scheme for a given input.\n            units: Dimensionality of weights stored at each vertex of lattice.\n\n        Raises:\n            ValueError: if `kernel_init` is invalid.\n            NotImplementedError: Random monotonic initialization not yet implemented.\n        \"\"\"\n        super().__init__()\n\n        self.lattice_sizes = list(lattice_sizes)\n        self.output_min = output_min\n        self.output_max = output_max\n        self.kernel_init = kernel_init\n        self.clip_inputs = clip_inputs\n        self.interpolation = interpolation\n        self.units = units\n\n        if monotonicities is not None:\n            self.monotonicities = monotonicities\n        else:\n            self.monotonicities = [None] * len(lattice_sizes)\n\n        if output_min is not None and output_max is not None:\n            output_init_min, output_init_max = output_min, output_max\n        elif output_min is not None:\n            output_init_min, output_init_max = output_min, output_min + 4.0\n        elif output_max is not None:\n            output_init_min, output_init_max = output_max - 4.0, output_max\n        else:\n            output_init_min, output_init_max = -2.0, 2.0\n        self._output_init_min, self._output_init_max = output_init_min, output_init_max\n\n        @torch.no_grad()\n        def initialize_kernel() -&gt; torch.Tensor:\n            if self.kernel_init == LatticeInit.LINEAR:\n                return self._linear_initializer()\n            if self.kernel_init == LatticeInit.RANDOM_MONOTONIC:\n                raise NotImplementedError(\n                    \"Random monotonic initialization not yet implemented.\"\n                )\n            raise ValueError(f\"Unknown kernel init: {self.kernel_init}\")\n\n        self.kernel = torch.nn.Parameter(initialize_kernel())\n\n    def forward(self, x: Union[torch.Tensor, list[torch.Tensor]]) -&gt; torch.Tensor:\n        \"\"\"Calculates interpolation from input, using method of self.interpolation.\n\n        Args:\n            x: input tensor. If `units == 1`, tensor of shape:\n                `(batch_size, ..., len(lattice_size))` or list of `len(lattice_sizes)`\n                tensors of same shape: `(batch_size, ..., 1)`. If `units &gt; 1`, tensor of\n                shape `(batch_size, ..., units, len(lattice_sizes))` or list of\n                `len(lattice_sizes)` tensors OF same shape `(batch_size, ..., units, 1)`\n\n        Returns:\n            torch.Tensor of shape `(batch_size, ..., units)` containing interpolated\n            values.\n\n        Raises:\n            ValueError: If the type of interpolation is unknown.\n        \"\"\"\n        x = [xi.double() for xi in x] if isinstance(x, list) else x.double()\n        if self.interpolation == Interpolation.HYPERCUBE:\n            return self._compute_hypercube_interpolation(x)\n        if self.interpolation == Interpolation.SIMPLEX:\n            return self._compute_simplex_interpolation(x)\n        raise ValueError(f\"Unknown interpolation type: {self.interpolation}\")\n\n    @torch.no_grad()\n    def apply_constraints(self) -&gt; None:\n        \"\"\"Aggregate function for enforcing constraints of lattice.\"\"\"\n        weights = self.kernel.clone()\n\n        if self._count_non_zeros(self.monotonicities):\n            lattice_sizes = self.lattice_sizes\n            monotonicities = self.monotonicities\n            if self.units &gt; 1:\n                lattice_sizes = lattice_sizes + [int(self.units)]\n                if self.monotonicities:\n                    monotonicities = monotonicities + [None]\n\n            weights = weights.reshape(*lattice_sizes)\n            weights = self._approximately_project_monotonicity(\n                weights, lattice_sizes, monotonicities\n            )\n\n        if self.output_min is not None:\n            weights = torch.clamp_min(weights, self.output_min)\n        if self.output_max is not None:\n            weights = torch.clamp_max(weights, self.output_max)\n\n        self.kernel.data = weights.view(-1, self.units)\n\n    @torch.no_grad()\n    def assert_constraints(self, eps: float = 1e-6) -&gt; list[str]:\n        \"\"\"Asserts that layer satisfies specified constraints.\n\n        This checks that weights follow monotonicity and bounds constraints.\n\n        Args:\n            eps: the margin of error allowed\n\n        Returns:\n            A list of dicts describing violated constraints including indices of\n            monotonicity violations. If no constraints violated, the list will be empty.\n        \"\"\"\n        messages = []\n        lattice_sizes = self.lattice_sizes\n        monotonicities = self.monotonicities\n        weights = self.kernel.data.clone()\n\n        if weights.shape[1] &gt; 1:\n            lattice_sizes = lattice_sizes + [int(weights.shape[1])]\n            if monotonicities:\n                monotonicities = monotonicities + [None]\n\n        # Reshape weights to match lattice sizes\n        weights = weights.reshape(*lattice_sizes)\n\n        for i in range(len(monotonicities or [])):\n            if monotonicities[i] != Monotonicity.INCREASING:\n                continue\n            weights_layers = torch.unbind(weights, dim=i)\n\n            for j in range(1, len(weights_layers)):\n                diff = torch.min(weights_layers[j] - weights_layers[j - 1])\n                if diff.item() &lt; -eps:\n                    messages.append(f\"Monotonicity violated at feature index {i}.\")\n\n        if self.output_max is not None and torch.max(weights) &gt; self.output_max + eps:\n            messages.append(\"Max weight greater than output_max.\")\n        if self.output_min is not None and torch.min(weights) &lt; self.output_min - eps:\n            messages.append(\"Min weight less than output_min.\")\n\n        return messages\n\n    ################################################################################\n    ############################## PRIVATE METHODS #################################\n    ################################################################################\n\n    def _linear_initializer(self) -&gt; torch.Tensor:\n        \"\"\"Creates initial weights tensor for linear initialization.\n\n        Args:\n            monotonicities: monotonicity constraints of lattice, enforced in\n                initialization.\n\n        Returns:\n            `torch.Tensor` of shape `(prod(lattice_sizes), units)`\n        \"\"\"\n        monotonicities = self.monotonicities[:]\n\n        if monotonicities is None:\n            monotonicities = [None] * len(self.lattice_sizes)\n\n        num_constraint_dims = self._count_non_zeros(monotonicities)\n        if num_constraint_dims == 0:\n            monotonicities = [Monotonicity.INCREASING] * len(self.lattice_sizes)\n            num_constraint_dims = len(self.lattice_sizes)\n\n        dim_range = (\n            float(self._output_init_max - self._output_init_min) / num_constraint_dims\n        )\n        one_d_weights = []\n\n        for monotonicity, dim_size in zip(monotonicities, self.lattice_sizes):\n            if monotonicity is not None:\n                one_d = np.linspace(start=0.0, stop=dim_range, num=dim_size)\n            else:\n                one_d = np.array([0.0] * dim_size)\n\n            one_d_weights.append(torch.tensor(one_d, dtype=torch.double).unsqueeze(0))\n\n        weights = self._batch_outer_operation(one_d_weights, operation=torch.add)\n        weights = (weights + self._output_init_min).view(-1, 1)\n        if self.units &gt; 1:\n            weights = weights.repeat(1, self.units)\n\n        return weights\n\n    @staticmethod\n    def _count_non_zeros(*iterables) -&gt; int:\n        \"\"\"Returns total number of non 0/None enum elements in given iterables.\n\n        Args:\n            *iterables: Any number of the value `None` or iterables of `None` or\n                `Monotonicity` enum values.\n        \"\"\"\n        result = 0\n        for iterable in iterables:\n            if iterable is not None:\n                for element in iterable:\n                    if element is not None:\n                        result += 1\n        return result\n\n    def _compute_simplex_interpolation(\n        self, inputs: Union[torch.Tensor, list[torch.Tensor]]\n    ) -&gt; torch.Tensor:\n        \"\"\"Evaluates a lattice using simplex interpolation.\n\n        Each `d`-dimensional unit hypercube of the lattice can be partitioned into `d!`\n        disjoint simplices with `d+1` vertices. `S` is the unique simplex which contains\n        input point `P`, and `S` has vertices `ABCD...`. For any vertex such as `A`, a\n        new simplex `S'` can be created using the vertices `PBCD...`. The weight of `A`\n        within the interpolation is then `vol(S')/vol(S)`. This process is repeated\n        for every vertex in `S`, and the resulting values are summed.\n\n        This interpolation can be computed in `O(D log(D))` time because it is only\n        necessary to compute the volume of the simplex containing input point `P`. For\n        context, the unit hypercube can be partitioned into `d!` simplices by starting\n        at `(0,0,...,0)` and incrementing `0` to `1` dimension-by-dimensionuntil one\n        reaches `(1,1,...,1)`. There are `d!` possible paths from `(0,0,...,0)` to\n        `(1,1,...,1)`, which account for the number of unique, disjoint simplices\n        created by the method. There are `d` steps for each possible path where each\n        step comprises the vertices of one simplex. Thus, one can find the containing\n        simplex for input `P` by argsorting the coordinates of `P` in descending order\n        and pathing along said order. To compute the intepolation weights simply take\n        the deltas from `[1, desc_sort(P_coords), 0]`.\n\n        Args:\n            inputs: input tensor. If `units == 1`, tensor of shape:\n                `(batch_size, ..., len(lattice_size))` or list of `len(lattice_sizes)`\n                tensors of same shape: `(batch_size, ..., 1)`. If `units &gt; 1`, tensor of\n                shape `(batch_size, ..., units, len(lattice_sizes))` or list of\n                `len(lattice_sizes)` tensors of same shape `(batch_size, ..., units, 1)`\n\n        Returns:\n            `torch.Tensor` of shape `(batch_size, ..., units)` containing interpolated\n            values.\n        \"\"\"\n        if isinstance(inputs, list):\n            inputs = torch.cat(inputs, dim=-1)\n\n        if self.clip_inputs:\n            inputs = self._clip_onto_lattice_range(inputs)\n\n        lattice_rank = len(self.lattice_sizes)\n        input_dim = len(inputs.shape)\n        all_size_2 = all(size == 2 for size in self.lattice_sizes)\n\n        # Strides are the index shift (with respect to flattened kernel data) of each\n        # dimension, which can be used in a dot product with multi-dimensional\n        # coordinates to give an index for the flattened lattice weights.\n        # Ex): for lattice_sizes = [4, 3, 2], we get strides = [6, 2, 1]: when looking\n        # at lattice coords (i, j, k) and kernel data flattened into 1-D, incrementing i\n        # corresponds to a shift of 6 in flattened kernel data, j corresponds to a shift\n        # of 2, and k corresponds to a shift of 1. Consequently, we can do\n        # (coords * strides) for any coordinates to obtain the flattened index.\n        strides = torch.tensor(\n            np.cumprod([1] + self.lattice_sizes[::-1][:-1])[::-1].copy()\n        )\n        if not all_size_2:\n            lower_corner_coordinates = inputs.int()\n            lower_corner_coordinates = torch.min(\n                lower_corner_coordinates, torch.tensor(self.lattice_sizes) - 2\n            )\n            inputs = inputs - lower_corner_coordinates.float()\n\n        sorted_indices = torch.argsort(inputs, descending=True)\n        sorted_inputs = torch.sort(inputs, descending=True).values\n\n        # Pad the 1 and 0 onto the ends of sorted coordinates and compute deltas.\n        no_padding_dims = [(0, 0)] * (input_dim - 1)\n        flat_no_padding = [item for sublist in no_padding_dims for item in sublist]\n        sorted_inputs_padded_left = torch.nn.functional.pad(\n            sorted_inputs, [1, 0] + flat_no_padding, value=1.0\n        )\n        sorted_inputs_padded_right = torch.nn.functional.pad(\n            sorted_inputs, [0, 1] + flat_no_padding, value=0.0\n        )\n        weights = sorted_inputs_padded_left - sorted_inputs_padded_right\n\n        # Use strides to find indices of simplex vertices in flattened form.\n        sorted_strides = torch.gather(strides, 0, sorted_indices.view(-1)).view(\n            sorted_indices.shape\n        )\n        if all_size_2:\n            corner_offset_and_sorted_strides = torch.nn.functional.pad(\n                sorted_strides, [1, 0] + flat_no_padding\n            )\n        else:\n            lower_corner_offset = (lower_corner_coordinates * strides).sum(\n                dim=-1, keepdim=True\n            )\n            corner_offset_and_sorted_strides = torch.cat(\n                [lower_corner_offset, sorted_strides], dim=-1\n            )\n        indices = torch.cumsum(corner_offset_and_sorted_strides, dim=-1)\n\n        # Get kernel data from corresponding simplex vertices.\n        if self.units == 1:\n            gathered_params = torch.index_select(\n                self.kernel.view(-1), 0, indices.view(-1)\n            ).view(indices.shape)\n        else:\n            unit_offset = torch.tensor(\n                [[i] * (lattice_rank + 1) for i in range(self.units)]\n            )\n            flat_indices = indices * self.units + unit_offset\n            gathered_params = torch.index_select(\n                self.kernel.view(-1), 0, flat_indices.view(-1)\n            ).view(indices.shape)\n\n        return (gathered_params * weights).sum(dim=-1, keepdim=self.units == 1)\n\n    def _compute_hypercube_interpolation(\n        self,\n        inputs: Union[torch.Tensor, list[torch.Tensor]],\n    ) -&gt; torch.Tensor:\n        \"\"\"Performs hypercube interpolation using the surrounding unit hypercube.\n\n        Args:\n            inputs: input tensor. If `units == 1`, tensor of shape:\n                `(batch_size, ..., len(lattice_size))` or list of `len(lattice_sizes)`\n                tensors of same shape: `(batch_size, ..., 1)`. If `units &gt; 1`, tensor of\n                shape `(batch_size, ..., units, len(lattice_sizes))` or list of\n                `len(lattice_sizes)` tensors of same shape `(batch_size, ..., units, 1)`\n\n        Returns:\n            `torch.Tensor` of shape `(batch_size, ..., units)` containing interpolated\n            value(s).\n        \"\"\"\n        interpolation_weights = self._compute_hypercube_interpolation_weights(\n            inputs=inputs, clip_inputs=self.clip_inputs\n        )\n        if self.units == 1:\n            return torch.matmul(interpolation_weights, self.kernel)\n\n        return torch.sum(interpolation_weights * self.kernel.t(), dim=-1)\n\n    def _compute_hypercube_interpolation_weights(\n        self, inputs: Union[torch.Tensor, list[torch.Tensor]], clip_inputs: bool = True\n    ) -&gt; torch.Tensor:\n        \"\"\"Computes weights for hypercube lattice interpolation.\n\n        For each n-dim unit in \"inputs,\" the weights matrix will generate the weights\n        corresponding to the unit's location within its surrounding hypercube. These\n        weights can then be multiplied by the lattice layer's kernel to compute the\n        actual hypercube interpolation. Specifically, the outer product of the set\n        `(1-x_i, x_i)` for all x_i in input unit x calculates the weights for each\n        vertex in the surrounding hypercube, and every other vertex in the lattice is\n        set to zero since it is not used. In addition, for consecutive dimensions of\n        equal size in the lattice, broadcasting is used to speed up calculations.\n\n        Args:\n            inputs: torch.Tensor of shape `(batch_size, ..., len(lattice_sizes)` or list\n                of `len(lattice_sizes)` tensors of same shape `(batch_size, ..., 1)`\n            clip_inputs: Boolean to determine whether input values outside lattice\n                bounds should be clipped to the min or max supported values.\n\n        Returns:\n            `torch.Tensor` of shape `(batch_size, ..., prod(lattice_sizes))` containing\n            the weights which can be matrix multiplied with the kernel to perform\n            hypercube interpolation.\n        \"\"\"\n        if isinstance(inputs, list):\n            input_dtype = inputs[0].dtype\n        else:\n            input_dtype = inputs.dtype\n\n        # Special case: 2^d lattice with input passed in as a single tensor\n        if all(size == 2 for size in self.lattice_sizes) and not isinstance(\n            inputs, list\n        ):\n            w = torch.stack([(1.0 - inputs), inputs], dim=-1)\n            if clip_inputs:\n                w = torch.clamp(w, min=0, max=1)\n            one_d_interpolation_weights = list(torch.unbind(w, dim=-2))\n            return self._batch_outer_operation(one_d_interpolation_weights)\n\n        if clip_inputs:\n            inputs = self._clip_onto_lattice_range(inputs)\n\n        # Set up buckets of consecutive equal dimensions for broadcasting later\n        dim_keypoints = {}\n        for dim_size in set(self.lattice_sizes):\n            dim_keypoints[dim_size] = torch.tensor(\n                list(range(dim_size)), dtype=input_dtype\n            )\n        bucketized_inputs = self._bucketize_consecutive_equal_dims(inputs)\n        one_d_interpolation_weights = []\n\n        for tensor, bucket_size, dim_size in bucketized_inputs:\n            if bucket_size &gt; 1:\n                tensor = torch.unsqueeze(tensor, dim=-1)\n            distance = torch.abs(tensor - dim_keypoints[dim_size])\n            weights = 1.0 - torch.minimum(\n                distance, torch.tensor(1.0, dtype=distance.dtype)\n            )\n            if bucket_size == 1:\n                one_d_interpolation_weights.append(weights)\n            else:\n                one_d_interpolation_weights.extend(torch.unbind(weights, dim=-2))\n\n        return self._batch_outer_operation(one_d_interpolation_weights)\n\n    @staticmethod\n    def _batch_outer_operation(\n        list_of_tensors: list[torch.Tensor],\n        operation: Optional[Callable] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Computes the flattened outer product of a list of tensors.\n\n        Args:\n            list_of_tensors: List of tensors of same shape `(batch_size, ..., k[i])`\n                where everything except `k_i` matches.\n            operation: A torch operation which supports broadcasting to be applied. If\n                `None` is provided, this will apply `torch.mul` for the first several\n                tensors and `torch.matmul` for the remaining tensors.\n\n        Returns:\n            `torch.Tensor` of shape `(batch_size, ..., k_i * k_j * ...)` containing a\n            flattened version of the outer product.\n        \"\"\"\n        if len(list_of_tensors) == 1:\n            return list_of_tensors[0]\n\n        result = torch.unsqueeze(list_of_tensors[0], dim=-1)\n\n        for i, tensor in enumerate(list_of_tensors[1:]):\n            if not operation:\n                op = torch.mul if i &lt; 6 else torch.matmul\n            else:\n                op = operation\n\n            result = op(result, torch.unsqueeze(tensor, dim=-2))\n            shape = [-1] + [int(size) for size in result.shape[1:]]\n            new_shape = shape[:-2] + [shape[-2] * shape[-1]]\n            if i &lt; len(list_of_tensors) - 2:\n                new_shape.append(1)\n            result = torch.reshape(result, new_shape)\n\n        return result\n\n    @overload\n    def _clip_onto_lattice_range(self, inputs: torch.Tensor) -&gt; torch.Tensor:\n        ...\n\n    @overload\n    def _clip_onto_lattice_range(\n        self, inputs: list[torch.Tensor]\n    ) -&gt; list[torch.Tensor]:\n        ...\n\n    def _clip_onto_lattice_range(\n        self,\n        inputs: Union[torch.Tensor, list[torch.Tensor]],\n    ) -&gt; Union[torch.Tensor, list[torch.Tensor]]:\n        \"\"\"Clips inputs onto valid input range for given lattice_sizes.\n\n        Args:\n            inputs: `inputs` argument of `_compute_interpolation_weights()`.\n\n        Returns:\n            `torch.Tensor` of shape `inputs` with values within range\n            `[0, dim_size - 1]`.\n        \"\"\"\n        clipped_inputs: Union[torch.Tensor, list[torch.Tensor]]\n        if not isinstance(inputs, list):\n            upper_bounds = torch.tensor(\n                [dim_size - 1.0 for dim_size in self.lattice_sizes]\n            ).double()\n            clipped_inputs = torch.clamp(\n                inputs, min=torch.zeros_like(upper_bounds), max=upper_bounds\n            )\n        else:\n            dim_upper_bounds = {}\n            for dim_size in set(self.lattice_sizes):\n                dim_upper_bounds[dim_size] = torch.tensor(\n                    dim_size - 1.0, dtype=inputs[0].dtype\n                )\n            dim_lower_bound = torch.zeros(1, dtype=inputs[0].dtype)\n\n            clipped_inputs = [\n                torch.clamp(\n                    one_d_input, min=dim_lower_bound, max=dim_upper_bounds[dim_size]\n                )\n                for one_d_input, dim_size in zip(inputs, self.lattice_sizes)\n            ]\n\n        return clipped_inputs\n\n    def _bucketize_consecutive_equal_dims(\n        self,\n        inputs: Union[torch.Tensor, list[torch.Tensor]],\n    ) -&gt; Iterator[tuple[torch.Tensor, int, int]]:\n        \"\"\"Creates buckets of equal sized dimensions for broadcasting ops.\n\n        Args:\n            inputs: `inputs` argument of `_compute_interpolation_weights()`.\n\n        Returns:\n            An `Iterable` containing `(torch.Tensor, int, int)` where the tensor\n            contains individual values from \"inputs\" corresponding to its bucket, the\n            first `int` is bucket size, and the second `int` is size of the dimension of\n            the bucket.\n        \"\"\"\n        if not isinstance(inputs, list):\n            bucket_sizes = []\n            bucket_dim_sizes = []\n            current_size = 1\n            for i in range(1, len(self.lattice_sizes)):\n                if self.lattice_sizes[i] != self.lattice_sizes[i - 1]:\n                    bucket_sizes.append(current_size)\n                    bucket_dim_sizes.append(self.lattice_sizes[i - 1])\n                    current_size = 1\n                else:\n                    current_size += 1\n            bucket_sizes.append(current_size)\n            bucket_dim_sizes.append(self.lattice_sizes[-1])\n            inputs = torch.split(inputs, split_size_or_sections=bucket_sizes, dim=-1)\n        else:\n            bucket_sizes = [1] * len(self.lattice_sizes)\n            bucket_dim_sizes = self.lattice_sizes\n\n        return zip(inputs, bucket_sizes, bucket_dim_sizes)\n\n    def _approximately_project_monotonicity(\n        self,\n        weights: torch.Tensor,\n        lattice_sizes: list[int],\n        monotonicities: list[Optional[Monotonicity]],\n    ) -&gt; torch.Tensor:\n        \"\"\"Projects weights of lattice to meet monotonicity constraints.\n\n        Note that this projection is an approximation which guarantees monotonicity\n        constraints but is not an exact projection with respect to the L2 norm.\n\n        Algorithm:\n        1. `max_projection`: For each vertex V in the lattice, the weight is adjusted to\n        be the maximum of all weights of vertices X such that X has all coordinates\n        less than or equal to V in monotonic dimensions.\n\n        2. `half_projection`: We adjust the weights to be the average of the original\n        weights and the `max_projection` weights.\n\n        3. `min_projection`: For each vertex V in the lattice, the weight is adjusted\n        based on the `half_projection` to be the minimum of all weights of vertices X\n        such that V has all coordinates less than or equal to X in monotonic dimensions.\n\n        This algorithm ensures that weights conform to the monotonicity constraints\n        while getting closer to a true projection by adjusting both up/downwards.\n\n        Args:\n            weights: `torch.Tensor` of kernel data reshaped into `(lattice_sizes)` if\n                `units == 1` or `(lattice_sizes, units)` if `units &gt; 1`.\n            lattice_sizes: List of size of each dimension of lattice, but for\n                `units &gt; 1`, `units` is appended to the end for computation purposes.\n            monotonicities: List of `None` or `Monotonicity.INCREASING`\n                of length `len(lattice_sizes)` for `units == 1` or\n                `len(lattice_sizes)+1` if `units &gt; 1` specifying monotonicity of each\n                feature of lattice.\n\n        Returns:\n            `torch.Tensor` of shape `self.kernel` with updated weights which meet\n            monotonicity constraints.\n        \"\"\"\n        max_projection = weights\n        for dim in range(len(lattice_sizes)):\n            if monotonicities[dim] is None:\n                continue\n            layers = list(torch.unbind(max_projection, dim))\n            for i in range(1, len(layers)):\n                layers[i] = torch.max(layers[i], layers[i - 1])\n            max_projection = torch.stack(layers, dim)\n\n        half_projection = (weights + max_projection) / 2.0\n\n        min_projection = half_projection\n        for dim in range(len(lattice_sizes)):\n            if monotonicities[dim] is None:\n                continue\n            layers = list(torch.unbind(min_projection, dim))\n            for i in range(len(layers) - 2, -1, -1):\n                # Compute cumulative minimum in reverse order\n                layers[i] = torch.min(layers[i], layers[i + 1])\n            min_projection = torch.stack(layers, dim)\n\n        return min_projection\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Lattice.__init__","title":"<code>__init__(lattice_sizes, output_min=None, output_max=None, kernel_init=LatticeInit.LINEAR, monotonicities=None, clip_inputs=True, interpolation=Interpolation.HYPERCUBE, units=1)</code>","text":"<p>Initializes an instance of 'Lattice'.</p> <p>Parameters:</p> Name Type Description Default <code>lattice_sizes</code> <code>Union[list[int], tuple[int]]</code> <p>List or tuple of size of lattice along each dimension.</p> required <code>output_min</code> <code>Optional[float]</code> <p>Minimum output value for weights at vertices of lattice.</p> <code>None</code> <code>output_max</code> <code>Optional[float]</code> <p>Maximum output value for weights at vertices of lattice.</p> <code>None</code> <code>kernel_init</code> <code>LatticeInit</code> <p>Initialization scheme to use for the kernel.</p> <code>LINEAR</code> <code>monotonicities</code> <code>Optional[list[Optional[Monotonicity]]]</code> <p><code>None</code> or list of <code>NONE</code> or <code>Monotonicity.INCREASING</code> of length <code>len(lattice_sizes)</code> specifying monotonicity of each feature of lattice. A monotonically decreasing  feature should use <code>Monotonicity.INCREASING</code> in the lattice layer but <code>Monotonicity.DECREASING</code> in the calibrator.</p> <code>None</code> <code>clip_inputs</code> <code>bool</code> <p>Whether input points should be clipped to the range of lattice.</p> <code>True</code> <code>interpolation</code> <code>Interpolation</code> <p>Interpolation scheme for a given input.</p> <code>HYPERCUBE</code> <code>units</code> <code>int</code> <p>Dimensionality of weights stored at each vertex of lattice.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>kernel_init</code> is invalid.</p> <code>NotImplementedError</code> <p>Random monotonic initialization not yet implemented.</p> Source code in <code>pytorch_lattice/layers/lattice.py</code> <pre><code>def __init__(\n    self,\n    lattice_sizes: Union[list[int], tuple[int]],\n    output_min: Optional[float] = None,\n    output_max: Optional[float] = None,\n    kernel_init: LatticeInit = LatticeInit.LINEAR,\n    monotonicities: Optional[list[Optional[Monotonicity]]] = None,\n    clip_inputs: bool = True,\n    interpolation: Interpolation = Interpolation.HYPERCUBE,\n    units: int = 1,\n) -&gt; None:\n    \"\"\"Initializes an instance of 'Lattice'.\n\n    Args:\n        lattice_sizes: List or tuple of size of lattice along each dimension.\n        output_min: Minimum output value for weights at vertices of lattice.\n        output_max: Maximum output value for weights at vertices of lattice.\n        kernel_init: Initialization scheme to use for the kernel.\n        monotonicities: `None` or list of `NONE` or\n            `Monotonicity.INCREASING` of length `len(lattice_sizes)` specifying\n            monotonicity of each feature of lattice. A monotonically decreasing\n             feature should use `Monotonicity.INCREASING` in the lattice layer but\n            `Monotonicity.DECREASING` in the calibrator.\n        clip_inputs: Whether input points should be clipped to the range of lattice.\n        interpolation: Interpolation scheme for a given input.\n        units: Dimensionality of weights stored at each vertex of lattice.\n\n    Raises:\n        ValueError: if `kernel_init` is invalid.\n        NotImplementedError: Random monotonic initialization not yet implemented.\n    \"\"\"\n    super().__init__()\n\n    self.lattice_sizes = list(lattice_sizes)\n    self.output_min = output_min\n    self.output_max = output_max\n    self.kernel_init = kernel_init\n    self.clip_inputs = clip_inputs\n    self.interpolation = interpolation\n    self.units = units\n\n    if monotonicities is not None:\n        self.monotonicities = monotonicities\n    else:\n        self.monotonicities = [None] * len(lattice_sizes)\n\n    if output_min is not None and output_max is not None:\n        output_init_min, output_init_max = output_min, output_max\n    elif output_min is not None:\n        output_init_min, output_init_max = output_min, output_min + 4.0\n    elif output_max is not None:\n        output_init_min, output_init_max = output_max - 4.0, output_max\n    else:\n        output_init_min, output_init_max = -2.0, 2.0\n    self._output_init_min, self._output_init_max = output_init_min, output_init_max\n\n    @torch.no_grad()\n    def initialize_kernel() -&gt; torch.Tensor:\n        if self.kernel_init == LatticeInit.LINEAR:\n            return self._linear_initializer()\n        if self.kernel_init == LatticeInit.RANDOM_MONOTONIC:\n            raise NotImplementedError(\n                \"Random monotonic initialization not yet implemented.\"\n            )\n        raise ValueError(f\"Unknown kernel init: {self.kernel_init}\")\n\n    self.kernel = torch.nn.Parameter(initialize_kernel())\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Lattice.apply_constraints","title":"<code>apply_constraints()</code>","text":"<p>Aggregate function for enforcing constraints of lattice.</p> Source code in <code>pytorch_lattice/layers/lattice.py</code> <pre><code>@torch.no_grad()\ndef apply_constraints(self) -&gt; None:\n    \"\"\"Aggregate function for enforcing constraints of lattice.\"\"\"\n    weights = self.kernel.clone()\n\n    if self._count_non_zeros(self.monotonicities):\n        lattice_sizes = self.lattice_sizes\n        monotonicities = self.monotonicities\n        if self.units &gt; 1:\n            lattice_sizes = lattice_sizes + [int(self.units)]\n            if self.monotonicities:\n                monotonicities = monotonicities + [None]\n\n        weights = weights.reshape(*lattice_sizes)\n        weights = self._approximately_project_monotonicity(\n            weights, lattice_sizes, monotonicities\n        )\n\n    if self.output_min is not None:\n        weights = torch.clamp_min(weights, self.output_min)\n    if self.output_max is not None:\n        weights = torch.clamp_max(weights, self.output_max)\n\n    self.kernel.data = weights.view(-1, self.units)\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Lattice.assert_constraints","title":"<code>assert_constraints(eps=1e-06)</code>","text":"<p>Asserts that layer satisfies specified constraints.</p> <p>This checks that weights follow monotonicity and bounds constraints.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>the margin of error allowed</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of dicts describing violated constraints including indices of</p> <code>list[str]</code> <p>monotonicity violations. If no constraints violated, the list will be empty.</p> Source code in <code>pytorch_lattice/layers/lattice.py</code> <pre><code>@torch.no_grad()\ndef assert_constraints(self, eps: float = 1e-6) -&gt; list[str]:\n    \"\"\"Asserts that layer satisfies specified constraints.\n\n    This checks that weights follow monotonicity and bounds constraints.\n\n    Args:\n        eps: the margin of error allowed\n\n    Returns:\n        A list of dicts describing violated constraints including indices of\n        monotonicity violations. If no constraints violated, the list will be empty.\n    \"\"\"\n    messages = []\n    lattice_sizes = self.lattice_sizes\n    monotonicities = self.monotonicities\n    weights = self.kernel.data.clone()\n\n    if weights.shape[1] &gt; 1:\n        lattice_sizes = lattice_sizes + [int(weights.shape[1])]\n        if monotonicities:\n            monotonicities = monotonicities + [None]\n\n    # Reshape weights to match lattice sizes\n    weights = weights.reshape(*lattice_sizes)\n\n    for i in range(len(monotonicities or [])):\n        if monotonicities[i] != Monotonicity.INCREASING:\n            continue\n        weights_layers = torch.unbind(weights, dim=i)\n\n        for j in range(1, len(weights_layers)):\n            diff = torch.min(weights_layers[j] - weights_layers[j - 1])\n            if diff.item() &lt; -eps:\n                messages.append(f\"Monotonicity violated at feature index {i}.\")\n\n    if self.output_max is not None and torch.max(weights) &gt; self.output_max + eps:\n        messages.append(\"Max weight greater than output_max.\")\n    if self.output_min is not None and torch.min(weights) &lt; self.output_min - eps:\n        messages.append(\"Min weight less than output_min.\")\n\n    return messages\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Lattice.forward","title":"<code>forward(x)</code>","text":"<p>Calculates interpolation from input, using method of self.interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[Tensor, list[Tensor]]</code> <p>input tensor. If <code>units == 1</code>, tensor of shape: <code>(batch_size, ..., len(lattice_size))</code> or list of <code>len(lattice_sizes)</code> tensors of same shape: <code>(batch_size, ..., 1)</code>. If <code>units &gt; 1</code>, tensor of shape <code>(batch_size, ..., units, len(lattice_sizes))</code> or list of <code>len(lattice_sizes)</code> tensors OF same shape <code>(batch_size, ..., units, 1)</code></p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor of shape <code>(batch_size, ..., units)</code> containing interpolated</p> <code>Tensor</code> <p>values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the type of interpolation is unknown.</p> Source code in <code>pytorch_lattice/layers/lattice.py</code> <pre><code>def forward(self, x: Union[torch.Tensor, list[torch.Tensor]]) -&gt; torch.Tensor:\n    \"\"\"Calculates interpolation from input, using method of self.interpolation.\n\n    Args:\n        x: input tensor. If `units == 1`, tensor of shape:\n            `(batch_size, ..., len(lattice_size))` or list of `len(lattice_sizes)`\n            tensors of same shape: `(batch_size, ..., 1)`. If `units &gt; 1`, tensor of\n            shape `(batch_size, ..., units, len(lattice_sizes))` or list of\n            `len(lattice_sizes)` tensors OF same shape `(batch_size, ..., units, 1)`\n\n    Returns:\n        torch.Tensor of shape `(batch_size, ..., units)` containing interpolated\n        values.\n\n    Raises:\n        ValueError: If the type of interpolation is unknown.\n    \"\"\"\n    x = [xi.double() for xi in x] if isinstance(x, list) else x.double()\n    if self.interpolation == Interpolation.HYPERCUBE:\n        return self._compute_hypercube_interpolation(x)\n    if self.interpolation == Interpolation.SIMPLEX:\n        return self._compute_simplex_interpolation(x)\n    raise ValueError(f\"Unknown interpolation type: {self.interpolation}\")\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Linear","title":"<code>pytorch_lattice.layers.Linear</code>","text":"<p>             Bases: <code>ConstrainedModule</code></p> <p>A constrained linear module.</p> <p>This module takes an input of shape <code>(batch_size, input_dim)</code> and applied a linear transformation. The output will have the same shape as the input.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>__init__</code> arguments.</p> <code>kernel</code> <p><code>torch.nn.Parameter</code> that stores the linear combination weighting.</p> <code>bias</code> <p><code>torch.nn.Parameter</code> that stores the bias term. Only available is <code>use_bias</code> is true.</p> <p>Example: <pre><code>input_dim = 3\ninputs = torch.tensor(...)  # shape: (batch_size, input_dim)\nlinear = Linear(\n    input_dim,\n    monotonicities=[\n        None,\n        Monotonicity.INCREASING,\n        Monotonicity.DECREASING\n    ],\n    use_bias=False,\n    weighted_average=True,\n)\noutputs = linear(inputs)\n</code></pre></p> Source code in <code>pytorch_lattice/layers/linear.py</code> <pre><code>class Linear(ConstrainedModule):\n    \"\"\"A constrained linear module.\n\n    This module takes an input of shape `(batch_size, input_dim)` and applied a linear\n    transformation. The output will have the same shape as the input.\n\n    Attributes:\n        All: `__init__` arguments.\n        kernel: `torch.nn.Parameter` that stores the linear combination weighting.\n        bias: `torch.nn.Parameter` that stores the bias term. Only available is\n            `use_bias` is true.\n\n    Example:\n    ```python\n    input_dim = 3\n    inputs = torch.tensor(...)  # shape: (batch_size, input_dim)\n    linear = Linear(\n        input_dim,\n        monotonicities=[\n            None,\n            Monotonicity.INCREASING,\n            Monotonicity.DECREASING\n        ],\n        use_bias=False,\n        weighted_average=True,\n    )\n    outputs = linear(inputs)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        monotonicities: Optional[list[Optional[Monotonicity]]] = None,\n        use_bias: bool = True,\n        weighted_average: bool = False,\n    ) -&gt; None:\n        \"\"\"Initializes an instance of `Linear`.\n\n        Args:\n            input_dim: The number of inputs that will be combined.\n            monotonicities: If provided, specifies the monotonicity of each input\n                dimension.\n            use_bias: Whether to use a bias term for the linear combination.\n            weighted_average: Whether to make the output a weighted average i.e. all\n                coefficients are positive and add up to a total of 1.0. No bias term\n                will be used, and `use_bias` will be set to false regardless of the\n                original value. `monotonicities` will also be set to increasing for all\n                input dimensions to ensure that all coefficients are positive.\n\n        Raises:\n            ValueError: If monotonicities does not have length input_dim (if provided).\n        \"\"\"\n        super().__init__()\n\n        self.input_dim = input_dim\n        if monotonicities and len(monotonicities) != input_dim:\n            raise ValueError(\"Monotonicities, if provided, must have length input_dim.\")\n        self.monotonicities = (\n            monotonicities\n            if not weighted_average\n            else [Monotonicity.INCREASING] * input_dim\n        )\n        self.use_bias = use_bias if not weighted_average else False\n        self.weighted_average = weighted_average\n\n        self.kernel = torch.nn.Parameter(torch.Tensor(input_dim, 1).double())\n        torch.nn.init.constant_(self.kernel, 1.0 / input_dim)\n        if use_bias:\n            self.bias = torch.nn.Parameter(torch.Tensor(1).double())\n            torch.nn.init.constant_(self.bias, 0.0)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Transforms inputs using a linear combination.\n\n        Args:\n            x: The input tensor of shape `(batch_size, input_dim)`.\n\n        Returns:\n            torch.Tensor of shape `(batch_size, 1)` containing transformed input values.\n        \"\"\"\n        result = torch.mm(x, self.kernel)\n        if self.use_bias:\n            result += self.bias\n        return result\n\n    @torch.no_grad()\n    def apply_constraints(self) -&gt; None:\n        \"\"\"Projects kernel into desired constraints.\"\"\"\n        projected_kernel_data = self.kernel.data\n\n        if self.monotonicities:\n            if Monotonicity.INCREASING in self.monotonicities:\n                increasing_mask = torch.tensor(\n                    [\n                        [0.0] if m == Monotonicity.INCREASING else [1.0]\n                        for m in self.monotonicities\n                    ]\n                )\n                projected_kernel_data = torch.maximum(\n                    projected_kernel_data, projected_kernel_data * increasing_mask\n                )\n            if Monotonicity.DECREASING in self.monotonicities:\n                decreasing_mask = torch.tensor(\n                    [\n                        [0.0] if m == Monotonicity.DECREASING else [1.0]\n                        for m in self.monotonicities\n                    ]\n                )\n                projected_kernel_data = torch.minimum(\n                    projected_kernel_data, projected_kernel_data * decreasing_mask\n                )\n\n        if self.weighted_average:\n            norm = torch.norm(projected_kernel_data, 1)\n            norm = torch.where(norm &lt; 1e-8, 1.0, norm)\n            projected_kernel_data /= norm\n\n        self.kernel.data = projected_kernel_data\n\n    @torch.no_grad()\n    def assert_constraints(self, eps: float = 1e-6) -&gt; list[str]:\n        \"\"\"Asserts that layer satisfies specified constraints.\n\n        This checks that decreasing monotonicity corresponds to negative weights,\n        increasing monotonicity corresponds to positive weights, and weights sum to 1\n        for weighted_average=True.\n\n        Args:\n            eps: the margin of error allowed\n\n        Returns:\n            A list of messages describing violated constraints. If no constraints\n            violated, the list will be empty.\n        \"\"\"\n        messages = []\n\n        if self.weighted_average:\n            total_weight = torch.sum(self.kernel.data)\n            if torch.abs(total_weight - 1.0) &gt; eps:\n                messages.append(\"Weights do not sum to 1.\")\n\n        if self.monotonicities:\n            monotonicities_constant = torch.tensor(\n                [\n                    1\n                    if m == Monotonicity.INCREASING\n                    else -1\n                    if m == Monotonicity.DECREASING\n                    else 0\n                    for m in self.monotonicities\n                ],\n                device=self.kernel.device,\n                dtype=self.kernel.dtype,\n            ).view(-1, 1)\n\n            violated_monotonicities = (self.kernel * monotonicities_constant) &lt; -eps\n            violation_indices = torch.where(violated_monotonicities)\n            if violation_indices[0].numel() &gt; 0:\n                messages.append(\n                    f\"Monotonicity violated at: {violation_indices[0].tolist()}\"\n                )\n\n        return messages\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Linear.__init__","title":"<code>__init__(input_dim, monotonicities=None, use_bias=True, weighted_average=False)</code>","text":"<p>Initializes an instance of <code>Linear</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>The number of inputs that will be combined.</p> required <code>monotonicities</code> <code>Optional[list[Optional[Monotonicity]]]</code> <p>If provided, specifies the monotonicity of each input dimension.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to use a bias term for the linear combination.</p> <code>True</code> <code>weighted_average</code> <code>bool</code> <p>Whether to make the output a weighted average i.e. all coefficients are positive and add up to a total of 1.0. No bias term will be used, and <code>use_bias</code> will be set to false regardless of the original value. <code>monotonicities</code> will also be set to increasing for all input dimensions to ensure that all coefficients are positive.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If monotonicities does not have length input_dim (if provided).</p> Source code in <code>pytorch_lattice/layers/linear.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    monotonicities: Optional[list[Optional[Monotonicity]]] = None,\n    use_bias: bool = True,\n    weighted_average: bool = False,\n) -&gt; None:\n    \"\"\"Initializes an instance of `Linear`.\n\n    Args:\n        input_dim: The number of inputs that will be combined.\n        monotonicities: If provided, specifies the monotonicity of each input\n            dimension.\n        use_bias: Whether to use a bias term for the linear combination.\n        weighted_average: Whether to make the output a weighted average i.e. all\n            coefficients are positive and add up to a total of 1.0. No bias term\n            will be used, and `use_bias` will be set to false regardless of the\n            original value. `monotonicities` will also be set to increasing for all\n            input dimensions to ensure that all coefficients are positive.\n\n    Raises:\n        ValueError: If monotonicities does not have length input_dim (if provided).\n    \"\"\"\n    super().__init__()\n\n    self.input_dim = input_dim\n    if monotonicities and len(monotonicities) != input_dim:\n        raise ValueError(\"Monotonicities, if provided, must have length input_dim.\")\n    self.monotonicities = (\n        monotonicities\n        if not weighted_average\n        else [Monotonicity.INCREASING] * input_dim\n    )\n    self.use_bias = use_bias if not weighted_average else False\n    self.weighted_average = weighted_average\n\n    self.kernel = torch.nn.Parameter(torch.Tensor(input_dim, 1).double())\n    torch.nn.init.constant_(self.kernel, 1.0 / input_dim)\n    if use_bias:\n        self.bias = torch.nn.Parameter(torch.Tensor(1).double())\n        torch.nn.init.constant_(self.bias, 0.0)\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Linear.apply_constraints","title":"<code>apply_constraints()</code>","text":"<p>Projects kernel into desired constraints.</p> Source code in <code>pytorch_lattice/layers/linear.py</code> <pre><code>@torch.no_grad()\ndef apply_constraints(self) -&gt; None:\n    \"\"\"Projects kernel into desired constraints.\"\"\"\n    projected_kernel_data = self.kernel.data\n\n    if self.monotonicities:\n        if Monotonicity.INCREASING in self.monotonicities:\n            increasing_mask = torch.tensor(\n                [\n                    [0.0] if m == Monotonicity.INCREASING else [1.0]\n                    for m in self.monotonicities\n                ]\n            )\n            projected_kernel_data = torch.maximum(\n                projected_kernel_data, projected_kernel_data * increasing_mask\n            )\n        if Monotonicity.DECREASING in self.monotonicities:\n            decreasing_mask = torch.tensor(\n                [\n                    [0.0] if m == Monotonicity.DECREASING else [1.0]\n                    for m in self.monotonicities\n                ]\n            )\n            projected_kernel_data = torch.minimum(\n                projected_kernel_data, projected_kernel_data * decreasing_mask\n            )\n\n    if self.weighted_average:\n        norm = torch.norm(projected_kernel_data, 1)\n        norm = torch.where(norm &lt; 1e-8, 1.0, norm)\n        projected_kernel_data /= norm\n\n    self.kernel.data = projected_kernel_data\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Linear.assert_constraints","title":"<code>assert_constraints(eps=1e-06)</code>","text":"<p>Asserts that layer satisfies specified constraints.</p> <p>This checks that decreasing monotonicity corresponds to negative weights, increasing monotonicity corresponds to positive weights, and weights sum to 1 for weighted_average=True.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>the margin of error allowed</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of messages describing violated constraints. If no constraints</p> <code>list[str]</code> <p>violated, the list will be empty.</p> Source code in <code>pytorch_lattice/layers/linear.py</code> <pre><code>@torch.no_grad()\ndef assert_constraints(self, eps: float = 1e-6) -&gt; list[str]:\n    \"\"\"Asserts that layer satisfies specified constraints.\n\n    This checks that decreasing monotonicity corresponds to negative weights,\n    increasing monotonicity corresponds to positive weights, and weights sum to 1\n    for weighted_average=True.\n\n    Args:\n        eps: the margin of error allowed\n\n    Returns:\n        A list of messages describing violated constraints. If no constraints\n        violated, the list will be empty.\n    \"\"\"\n    messages = []\n\n    if self.weighted_average:\n        total_weight = torch.sum(self.kernel.data)\n        if torch.abs(total_weight - 1.0) &gt; eps:\n            messages.append(\"Weights do not sum to 1.\")\n\n    if self.monotonicities:\n        monotonicities_constant = torch.tensor(\n            [\n                1\n                if m == Monotonicity.INCREASING\n                else -1\n                if m == Monotonicity.DECREASING\n                else 0\n                for m in self.monotonicities\n            ],\n            device=self.kernel.device,\n            dtype=self.kernel.dtype,\n        ).view(-1, 1)\n\n        violated_monotonicities = (self.kernel * monotonicities_constant) &lt; -eps\n        violation_indices = torch.where(violated_monotonicities)\n        if violation_indices[0].numel() &gt; 0:\n            messages.append(\n                f\"Monotonicity violated at: {violation_indices[0].tolist()}\"\n            )\n\n    return messages\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.Linear.forward","title":"<code>forward(x)</code>","text":"<p>Transforms inputs using a linear combination.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape <code>(batch_size, input_dim)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor of shape <code>(batch_size, 1)</code> containing transformed input values.</p> Source code in <code>pytorch_lattice/layers/linear.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Transforms inputs using a linear combination.\n\n    Args:\n        x: The input tensor of shape `(batch_size, input_dim)`.\n\n    Returns:\n        torch.Tensor of shape `(batch_size, 1)` containing transformed input values.\n    \"\"\"\n    result = torch.mm(x, self.kernel)\n    if self.use_bias:\n        result += self.bias\n    return result\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.NumericalCalibrator","title":"<code>pytorch_lattice.layers.NumericalCalibrator</code>","text":"<p>             Bases: <code>ConstrainedModule</code></p> <p>A numerical calibrator.</p> <p>This module takes an input of shape <code>(batch_size, 1)</code> and calibrates it using a piece-wise linear function that conforms to any provided constraints. The output will have the same shape as the input.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>__init__</code> arguments.</p> <code>kernel</code> <p><code>torch.nn.Parameter</code> that stores the piece-wise linear function weights.</p> <code>missing_output</code> <p><code>torch.nn.Parameter</code> that stores the output learned for any missing inputs. Only available if <code>missing_input_value</code> is provided.</p> <p>Example: <pre><code>inputs = torch.tensor(...)  # shape: (batch_size, 1)\ncalibrator = NumericalCalibrator(\n    input_keypoints=np.linspace(1., 5., num=5),\n    output_min=0.0,\n    output_max=1.0,\n    monotonicity=Monotonicity.INCREASING,\n    kernel_init=NumericalCalibratorInit.EQUAL_HEIGHTS,\n)\noutputs = calibrator(inputs)\n</code></pre></p> Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code> <pre><code>class NumericalCalibrator(ConstrainedModule):\n    \"\"\"A numerical calibrator.\n\n    This module takes an input of shape `(batch_size, 1)` and calibrates it using a\n    piece-wise linear function that conforms to any provided constraints. The output\n    will have the same shape as the input.\n\n    Attributes:\n        All: `__init__` arguments.\n        kernel: `torch.nn.Parameter` that stores the piece-wise linear function weights.\n        missing_output: `torch.nn.Parameter` that stores the output learned for any\n            missing inputs. Only available if `missing_input_value` is provided.\n\n    Example:\n    ```python\n    inputs = torch.tensor(...)  # shape: (batch_size, 1)\n    calibrator = NumericalCalibrator(\n        input_keypoints=np.linspace(1., 5., num=5),\n        output_min=0.0,\n        output_max=1.0,\n        monotonicity=Monotonicity.INCREASING,\n        kernel_init=NumericalCalibratorInit.EQUAL_HEIGHTS,\n    )\n    outputs = calibrator(inputs)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        input_keypoints: np.ndarray,\n        missing_input_value: Optional[float] = None,\n        output_min: Optional[float] = None,\n        output_max: Optional[float] = None,\n        monotonicity: Optional[Monotonicity] = None,\n        kernel_init: NumericalCalibratorInit = NumericalCalibratorInit.EQUAL_HEIGHTS,\n        projection_iterations: int = 8,\n        input_keypoints_type: InputKeypointsType = InputKeypointsType.FIXED,\n    ) -&gt; None:\n        \"\"\"Initializes an instance of `NumericalCalibrator`.\n\n        Args:\n            input_keypoints: Ordered list of float-valued keypoints for the underlying\n                piece-wise linear function.\n            missing_input_value: If provided, the calibrator will learn to map all\n                instances of this missing input value to a learned output value.\n            output_min: Minimum output value. If `None`, the minimum output value will\n                be unbounded.\n            output_max: Maximum output value. If `None`, the maximum output value will\n                be unbounded.\n            monotonicity: Monotonicity constraint for the underlying piece-wise linear\n                function.\n            kernel_init: Initialization scheme to use for the kernel.\n            projection_iterations: Number of times to run Dykstra's projection\n                algorithm when applying constraints.\n            input_keypoints_type: `InputKeypointType` of either `FIXED` or `LEARNED`. If\n                `LEARNED`, keypoints other than the first or last will follow\n                `input_keypoints` for initialization but adapt during training.\n\n        Raises:\n            ValueError: If `kernel_init` is invalid.\n        \"\"\"\n        super().__init__()\n\n        self.input_keypoints = input_keypoints\n        self.missing_input_value = missing_input_value\n        self.output_min = output_min\n        self.output_max = output_max\n        self.monotonicity = monotonicity\n        self.kernel_init = kernel_init\n        self.projection_iterations = projection_iterations\n        self.input_keypoints_type = input_keypoints_type\n\n        # Determine default output initialization values if bounds are not fully set.\n        if output_min is not None and output_max is not None:\n            output_init_min, output_init_max = output_min, output_max\n        elif output_min is not None:\n            output_init_min, output_init_max = output_min, output_min + 4.0\n        elif output_max is not None:\n            output_init_min, output_init_max = output_max - 4.0, output_max\n        else:\n            output_init_min, output_init_max = -2.0, 2.0\n        self._output_init_min, self._output_init_max = output_init_min, output_init_max\n\n        self._interpolation_keypoints = torch.from_numpy(input_keypoints[:-1])\n        self._lengths = torch.from_numpy(input_keypoints[1:] - input_keypoints[:-1])\n        if self.input_keypoints_type == InputKeypointsType.LEARNED:\n            self._keypoint_min = input_keypoints[0]\n            self._keypoint_range = input_keypoints[-1] - input_keypoints[0]\n            initial_logits = torch.from_numpy(\n                np.log(\n                    (input_keypoints[1:] - input_keypoints[:-1]) / self._keypoint_range\n                )\n            ).double()\n            self._interpolation_logits = torch.nn.Parameter(initial_logits)\n\n        # First row of the kernel represents the bias. The remaining rows represent\n        # the y-value delta compared to the previous point i.e. the segment heights.\n        @torch.no_grad()\n        def initialize_kernel() -&gt; torch.Tensor:\n            output_init_range = self._output_init_max - self._output_init_min\n            if kernel_init == NumericalCalibratorInit.EQUAL_HEIGHTS:\n                num_segments = self._interpolation_keypoints.size()[0]\n                segment_height = output_init_range / num_segments\n                heights = torch.tensor([[segment_height]] * num_segments)\n            elif kernel_init == NumericalCalibratorInit.EQUAL_SLOPES:\n                heights = (\n                    self._lengths * output_init_range / torch.sum(self._lengths)\n                )[:, None]\n            else:\n                raise ValueError(f\"Unknown kernel init: {self.kernel_init}\")\n\n            if monotonicity == Monotonicity.DECREASING:\n                bias = torch.tensor([[self._output_init_max]])\n                heights = -heights\n            else:\n                bias = torch.tensor([[self._output_init_min]])\n            return torch.cat((bias, heights), 0).double()\n\n        self.kernel = torch.nn.Parameter(initialize_kernel())\n\n        if missing_input_value:\n            self.missing_output = torch.nn.Parameter(torch.Tensor(1))\n            torch.nn.init.constant_(\n                self.missing_output,\n                (self._output_init_min + self._output_init_max) / 2.0,\n            )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Calibrates numerical inputs through piece-wise linear interpolation.\n\n        Args:\n            x: The input tensor of shape `(batch_size, 1)`.\n\n        Returns:\n            torch.Tensor of shape `(batch_size, 1)` containing calibrated input values.\n        \"\"\"\n        if self.input_keypoints_type == InputKeypointsType.LEARNED:\n            softmaxed_logits = torch.nn.functional.softmax(\n                self._interpolation_logits, dim=-1\n            )\n            self._lengths = softmaxed_logits * self._keypoint_range\n            interior_keypoints = (\n                torch.cumsum(self._lengths, dim=-1) + self._keypoint_min\n            )\n            self._interpolation_keypoints = torch.cat(\n                [torch.tensor([self._keypoint_min]), interior_keypoints[:-1]]\n            )\n\n        interpolation_weights = (x - self._interpolation_keypoints) / self._lengths\n        interpolation_weights = torch.minimum(interpolation_weights, torch.tensor(1.0))\n        interpolation_weights = torch.maximum(interpolation_weights, torch.tensor(0.0))\n        interpolation_weights = torch.cat(\n            (torch.ones_like(x), interpolation_weights), -1\n        )\n        result = torch.mm(interpolation_weights, self.kernel)\n\n        if self.missing_input_value is not None:\n            missing_mask = torch.eq(x, self.missing_input_value).long()\n            result = missing_mask * self.missing_output + (1.0 - missing_mask) * result\n\n        return result\n\n    @torch.no_grad()\n    def apply_constraints(self) -&gt; None:\n        \"\"\"Jointly projects kernel into desired constraints.\n\n        Uses Dykstra's alternating projection algorithm to jointly project onto all\n        given constraints. This algorithm projects with respect to the L2 norm, but it\n        approached the norm from the \"wrong\" side. To ensure that all constraints are\n        strictly met, we do final approximate projections that project strictly into the\n        feasible space, but this is not an exact projection with respect to the L2 norm.\n        Enough iterations make the impact of this approximation negligible.\n        \"\"\"\n        constrain_bounds = self.output_min is not None or self.output_max is not None\n        constrain_monotonicity = self.monotonicity is not None\n        num_constraints = sum([constrain_bounds, constrain_monotonicity])\n\n        # We do nothing to the weights in this case\n        if num_constraints == 0:\n            return\n\n        original_bias, original_heights = self.kernel.data[0:1], self.kernel.data[1:]\n        previous_bias_delta: dict[str, torch.Tensor] = defaultdict(\n            lambda: torch.zeros_like(original_bias)\n        )\n        previous_heights_delta: dict[str, torch.Tensor] = defaultdict(\n            lambda: torch.zeros_like(original_heights)\n        )\n\n        def apply_bound_constraints(bias, heights):\n            previous_bias = bias - previous_bias_delta[\"BOUNDS\"]\n            previous_heights = heights - previous_heights_delta[\"BOUNDS\"]\n            if constrain_monotonicity:\n                bias, heights = self._project_monotonic_bounds(\n                    previous_bias, previous_heights\n                )\n            else:\n                bias, heights = self._approximately_project_bounds_only(\n                    previous_bias, previous_heights\n                )\n            previous_bias_delta[\"BOUNDS\"] = bias - previous_bias\n            previous_heights_delta[\"BOUNDS\"] = heights - previous_heights\n            return bias, heights\n\n        def apply_monotonicity_constraints(heights):\n            previous_heights = heights - previous_bias_delta[\"MONOTONICITY\"]\n            heights = self._project_monotonicity(previous_heights)\n            previous_heights_delta[\"MONOTONICITY\"] = heights - previous_heights\n            return heights\n\n        def apply_dykstras_projection(bias, heights):\n            if constrain_bounds:\n                bias, heights = apply_bound_constraints(bias, heights)\n            if constrain_monotonicity:\n                heights = apply_monotonicity_constraints(heights)\n            return bias, heights\n\n        def finalize_constraints(bias, heights):\n            if constrain_monotonicity:\n                heights = self._project_monotonicity(heights)\n            if constrain_bounds:\n                if constrain_monotonicity:\n                    bias, heights = self._squeeze_by_scaling(bias, heights)\n                else:\n                    bias, heights = self._approximately_project_bounds_only(\n                        bias, heights\n                    )\n            return bias, heights\n\n        projected_bias, projected_heights = apply_dykstras_projection(\n            original_bias, original_heights\n        )\n        if num_constraints &gt; 1:\n            for _ in range(self.projection_iterations - 1):\n                projected_bias, projected_heights = apply_dykstras_projection(\n                    projected_bias, projected_heights\n                )\n            projected_bias, projected_heights = finalize_constraints(\n                projected_bias, projected_heights\n            )\n\n        self.kernel.data = torch.cat((projected_bias, projected_heights), 0)\n\n    @torch.no_grad()\n    def assert_constraints(self, eps: float = 1e-6) -&gt; list[str]:\n        \"\"\"Asserts that layer satisfies specified constraints.\n\n        This checks that weights follow monotonicity constraints and that the output is\n        within bounds.\n\n        Args:\n            eps: the margin of error allowed\n\n        Returns:\n            A list of messages describing violated constraints including indices of\n            monotonicity violations. If no constraints violated, the list will be empty.\n        \"\"\"\n        weights = torch.squeeze(self.kernel.data)\n        messages = []\n\n        if (\n            self.output_max is not None\n            and torch.max(self.keypoints_outputs()) &gt; self.output_max + eps\n        ):\n            messages.append(\"Max weight greater than output_max.\")\n        if (\n            self.output_min is not None\n            and torch.min(self.keypoints_outputs()) &lt; self.output_min - eps\n        ):\n            messages.append(\"Min weight less than output_min.\")\n\n        diffs = weights[1:]\n        violation_indices = []\n\n        if self.monotonicity == Monotonicity.INCREASING:\n            violation_indices = (diffs &lt; -eps).nonzero().tolist()\n        elif self.monotonicity == Monotonicity.DECREASING:\n            violation_indices = (diffs &gt; eps).nonzero().tolist()\n\n        violation_indices = [(i[0], i[0] + 1) for i in violation_indices]\n        if violation_indices:\n            messages.append(f\"Monotonicity violated at: {str(violation_indices)}.\")\n\n        return messages\n\n    @torch.no_grad()\n    def keypoints_inputs(self) -&gt; torch.Tensor:\n        \"\"\"Returns tensor of keypoint inputs.\"\"\"\n        return torch.cat(\n            (\n                self._interpolation_keypoints,\n                self._interpolation_keypoints[-1:] + self._lengths[-1:],\n            ),\n            0,\n        )\n\n    @torch.no_grad()\n    def keypoints_outputs(self) -&gt; torch.Tensor:\n        \"\"\"Returns tensor of keypoint outputs.\"\"\"\n        return torch.cumsum(self.kernel.data, 0).T[0]\n\n    ################################################################################\n    ############################## PRIVATE METHODS #################################\n    ################################################################################\n\n    def _project_monotonic_bounds(\n        self, bias: torch.Tensor, heights: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Projects bias and heights into bounds considering monotonicity.\n\n        For computation simplification in the case of decreasing monotonicity, we mirror\n        bias and heights and swap-mirror the output bounds. After doing the standard\n        projection with resepct to increasing monotonicity, we then mirror everything\n        back to get the correct projection.\n\n        Args:\n            bias: The bias of the underlying piece-wise linear function.\n            heights: The heights of each segment of the underlying piece-wise linear\n                function.\n\n        Returns:\n            A tuple containing the projected bias and projected heights.\n        \"\"\"\n        output_min, output_max = self.output_min, self.output_max\n        decreasing = self.monotonicity == Monotonicity.DECREASING\n        if decreasing:\n            bias, heights = -bias, -heights\n            output_min = None if self.output_max is None else -1 * self.output_max\n            output_max = None if self.output_min is None else -1 * self.output_min\n        if output_max is not None:\n            num_heights = heights.size()[0]\n            output_max_diffs = output_max - (bias + torch.sum(heights, 0))\n            bias_delta = output_max_diffs / (num_heights + 1)\n            bias_delta = torch.minimum(bias_delta, torch.tensor(0.0))\n            if output_min is not None:\n                bias = torch.maximum(bias + bias_delta, torch.tensor(output_min))\n                heights_delta = output_max_diffs / num_heights\n            else:\n                bias += bias_delta\n                heights_delta = bias_delta\n            heights += torch.minimum(heights_delta, torch.tensor(0.0))\n        elif output_min is not None:\n            bias = torch.maximum(bias, torch.tensor(output_min))\n        if decreasing:\n            bias, heights = -bias, -heights\n        return bias, heights\n\n    def _approximately_project_bounds_only(\n        self, bias: torch.Tensor, heights: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Projects bias and heights without considering monotonicity.\n\n        It is worth noting that this projection is an approximation and is not an exact\n        projection with respect to the L2 norm; however, it is sufficiently accurate and\n        efficient in practice for non-monotonic functions.\n\n        Args:\n            bias: The bias of the underlying piece-wise linear function.\n            heights: The heights of each segment of the underlying piece-wise linear\n                function.\n\n        Returns:\n            A tuple containing the projected bias and projected heights.\n        \"\"\"\n        sums = torch.cumsum(torch.cat((bias, heights), 0), 0)\n        if self.output_min is not None:\n            sums = torch.maximum(sums, torch.tensor(self.output_min))\n        if self.output_max is not None:\n            sums = torch.minimum(sums, torch.tensor(self.output_max))\n        bias = sums[0:1]\n        heights = sums[1:] - sums[:-1]\n        return bias, heights\n\n    def _project_monotonicity(self, heights: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns bias and heights projected into desired monotonicity constraints.\"\"\"\n        if self.monotonicity == Monotonicity.INCREASING:\n            return torch.maximum(heights, torch.tensor(0.0))\n        if self.monotonicity == Monotonicity.DECREASING:\n            return torch.minimum(heights, torch.tensor(0.0))\n        return heights\n\n    def _squeeze_by_scaling(\n        self, bias: torch.Tensor, heights: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Squeezes monotonic calibrators by scaling them into bound constraints.\n\n        It is worth noting that this is not an exact projection with respect to the L2\n        norm; however, it maintains convexity, which projection by shift does not.\n\n        Args:\n            bias: The bias of the underlying piece-wise linear function.\n            heights: The heights of each segment of the underlying piece-wise linear\n                function.\n\n        Returns:\n            A tuple containing the projected bias and projected heights.\n        \"\"\"\n        decreasing = self.monotonicity == Monotonicity.DECREASING\n        output_max = self.output_max\n        if decreasing:\n            if self.output_min is None:\n                return bias, heights\n            bias, heights = -bias, -heights\n            output_max = None if self.output_min is None else -1 * self.output_min\n        if output_max is None:\n            return bias, heights\n        delta = output_max - bias\n        scaling_factor = torch.where(\n            delta &gt; 0.0001, torch.sum(heights, 0) / delta, torch.ones_like(delta)\n        )\n        heights /= torch.maximum(scaling_factor, torch.tensor(1.0))\n        if decreasing:\n            bias, heights = -bias, -heights\n        return bias, heights\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.NumericalCalibrator.__init__","title":"<code>__init__(input_keypoints, missing_input_value=None, output_min=None, output_max=None, monotonicity=None, kernel_init=NumericalCalibratorInit.EQUAL_HEIGHTS, projection_iterations=8, input_keypoints_type=InputKeypointsType.FIXED)</code>","text":"<p>Initializes an instance of <code>NumericalCalibrator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_keypoints</code> <code>ndarray</code> <p>Ordered list of float-valued keypoints for the underlying piece-wise linear function.</p> required <code>missing_input_value</code> <code>Optional[float]</code> <p>If provided, the calibrator will learn to map all instances of this missing input value to a learned output value.</p> <code>None</code> <code>output_min</code> <code>Optional[float]</code> <p>Minimum output value. If <code>None</code>, the minimum output value will be unbounded.</p> <code>None</code> <code>output_max</code> <code>Optional[float]</code> <p>Maximum output value. If <code>None</code>, the maximum output value will be unbounded.</p> <code>None</code> <code>monotonicity</code> <code>Optional[Monotonicity]</code> <p>Monotonicity constraint for the underlying piece-wise linear function.</p> <code>None</code> <code>kernel_init</code> <code>NumericalCalibratorInit</code> <p>Initialization scheme to use for the kernel.</p> <code>EQUAL_HEIGHTS</code> <code>projection_iterations</code> <code>int</code> <p>Number of times to run Dykstra's projection algorithm when applying constraints.</p> <code>8</code> <code>input_keypoints_type</code> <code>InputKeypointsType</code> <p><code>InputKeypointType</code> of either <code>FIXED</code> or <code>LEARNED</code>. If <code>LEARNED</code>, keypoints other than the first or last will follow <code>input_keypoints</code> for initialization but adapt during training.</p> <code>FIXED</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>kernel_init</code> is invalid.</p> Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code> <pre><code>def __init__(\n    self,\n    input_keypoints: np.ndarray,\n    missing_input_value: Optional[float] = None,\n    output_min: Optional[float] = None,\n    output_max: Optional[float] = None,\n    monotonicity: Optional[Monotonicity] = None,\n    kernel_init: NumericalCalibratorInit = NumericalCalibratorInit.EQUAL_HEIGHTS,\n    projection_iterations: int = 8,\n    input_keypoints_type: InputKeypointsType = InputKeypointsType.FIXED,\n) -&gt; None:\n    \"\"\"Initializes an instance of `NumericalCalibrator`.\n\n    Args:\n        input_keypoints: Ordered list of float-valued keypoints for the underlying\n            piece-wise linear function.\n        missing_input_value: If provided, the calibrator will learn to map all\n            instances of this missing input value to a learned output value.\n        output_min: Minimum output value. If `None`, the minimum output value will\n            be unbounded.\n        output_max: Maximum output value. If `None`, the maximum output value will\n            be unbounded.\n        monotonicity: Monotonicity constraint for the underlying piece-wise linear\n            function.\n        kernel_init: Initialization scheme to use for the kernel.\n        projection_iterations: Number of times to run Dykstra's projection\n            algorithm when applying constraints.\n        input_keypoints_type: `InputKeypointType` of either `FIXED` or `LEARNED`. If\n            `LEARNED`, keypoints other than the first or last will follow\n            `input_keypoints` for initialization but adapt during training.\n\n    Raises:\n        ValueError: If `kernel_init` is invalid.\n    \"\"\"\n    super().__init__()\n\n    self.input_keypoints = input_keypoints\n    self.missing_input_value = missing_input_value\n    self.output_min = output_min\n    self.output_max = output_max\n    self.monotonicity = monotonicity\n    self.kernel_init = kernel_init\n    self.projection_iterations = projection_iterations\n    self.input_keypoints_type = input_keypoints_type\n\n    # Determine default output initialization values if bounds are not fully set.\n    if output_min is not None and output_max is not None:\n        output_init_min, output_init_max = output_min, output_max\n    elif output_min is not None:\n        output_init_min, output_init_max = output_min, output_min + 4.0\n    elif output_max is not None:\n        output_init_min, output_init_max = output_max - 4.0, output_max\n    else:\n        output_init_min, output_init_max = -2.0, 2.0\n    self._output_init_min, self._output_init_max = output_init_min, output_init_max\n\n    self._interpolation_keypoints = torch.from_numpy(input_keypoints[:-1])\n    self._lengths = torch.from_numpy(input_keypoints[1:] - input_keypoints[:-1])\n    if self.input_keypoints_type == InputKeypointsType.LEARNED:\n        self._keypoint_min = input_keypoints[0]\n        self._keypoint_range = input_keypoints[-1] - input_keypoints[0]\n        initial_logits = torch.from_numpy(\n            np.log(\n                (input_keypoints[1:] - input_keypoints[:-1]) / self._keypoint_range\n            )\n        ).double()\n        self._interpolation_logits = torch.nn.Parameter(initial_logits)\n\n    # First row of the kernel represents the bias. The remaining rows represent\n    # the y-value delta compared to the previous point i.e. the segment heights.\n    @torch.no_grad()\n    def initialize_kernel() -&gt; torch.Tensor:\n        output_init_range = self._output_init_max - self._output_init_min\n        if kernel_init == NumericalCalibratorInit.EQUAL_HEIGHTS:\n            num_segments = self._interpolation_keypoints.size()[0]\n            segment_height = output_init_range / num_segments\n            heights = torch.tensor([[segment_height]] * num_segments)\n        elif kernel_init == NumericalCalibratorInit.EQUAL_SLOPES:\n            heights = (\n                self._lengths * output_init_range / torch.sum(self._lengths)\n            )[:, None]\n        else:\n            raise ValueError(f\"Unknown kernel init: {self.kernel_init}\")\n\n        if monotonicity == Monotonicity.DECREASING:\n            bias = torch.tensor([[self._output_init_max]])\n            heights = -heights\n        else:\n            bias = torch.tensor([[self._output_init_min]])\n        return torch.cat((bias, heights), 0).double()\n\n    self.kernel = torch.nn.Parameter(initialize_kernel())\n\n    if missing_input_value:\n        self.missing_output = torch.nn.Parameter(torch.Tensor(1))\n        torch.nn.init.constant_(\n            self.missing_output,\n            (self._output_init_min + self._output_init_max) / 2.0,\n        )\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.NumericalCalibrator.apply_constraints","title":"<code>apply_constraints()</code>","text":"<p>Jointly projects kernel into desired constraints.</p> <p>Uses Dykstra's alternating projection algorithm to jointly project onto all given constraints. This algorithm projects with respect to the L2 norm, but it approached the norm from the \"wrong\" side. To ensure that all constraints are strictly met, we do final approximate projections that project strictly into the feasible space, but this is not an exact projection with respect to the L2 norm. Enough iterations make the impact of this approximation negligible.</p> Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code> <pre><code>@torch.no_grad()\ndef apply_constraints(self) -&gt; None:\n    \"\"\"Jointly projects kernel into desired constraints.\n\n    Uses Dykstra's alternating projection algorithm to jointly project onto all\n    given constraints. This algorithm projects with respect to the L2 norm, but it\n    approached the norm from the \"wrong\" side. To ensure that all constraints are\n    strictly met, we do final approximate projections that project strictly into the\n    feasible space, but this is not an exact projection with respect to the L2 norm.\n    Enough iterations make the impact of this approximation negligible.\n    \"\"\"\n    constrain_bounds = self.output_min is not None or self.output_max is not None\n    constrain_monotonicity = self.monotonicity is not None\n    num_constraints = sum([constrain_bounds, constrain_monotonicity])\n\n    # We do nothing to the weights in this case\n    if num_constraints == 0:\n        return\n\n    original_bias, original_heights = self.kernel.data[0:1], self.kernel.data[1:]\n    previous_bias_delta: dict[str, torch.Tensor] = defaultdict(\n        lambda: torch.zeros_like(original_bias)\n    )\n    previous_heights_delta: dict[str, torch.Tensor] = defaultdict(\n        lambda: torch.zeros_like(original_heights)\n    )\n\n    def apply_bound_constraints(bias, heights):\n        previous_bias = bias - previous_bias_delta[\"BOUNDS\"]\n        previous_heights = heights - previous_heights_delta[\"BOUNDS\"]\n        if constrain_monotonicity:\n            bias, heights = self._project_monotonic_bounds(\n                previous_bias, previous_heights\n            )\n        else:\n            bias, heights = self._approximately_project_bounds_only(\n                previous_bias, previous_heights\n            )\n        previous_bias_delta[\"BOUNDS\"] = bias - previous_bias\n        previous_heights_delta[\"BOUNDS\"] = heights - previous_heights\n        return bias, heights\n\n    def apply_monotonicity_constraints(heights):\n        previous_heights = heights - previous_bias_delta[\"MONOTONICITY\"]\n        heights = self._project_monotonicity(previous_heights)\n        previous_heights_delta[\"MONOTONICITY\"] = heights - previous_heights\n        return heights\n\n    def apply_dykstras_projection(bias, heights):\n        if constrain_bounds:\n            bias, heights = apply_bound_constraints(bias, heights)\n        if constrain_monotonicity:\n            heights = apply_monotonicity_constraints(heights)\n        return bias, heights\n\n    def finalize_constraints(bias, heights):\n        if constrain_monotonicity:\n            heights = self._project_monotonicity(heights)\n        if constrain_bounds:\n            if constrain_monotonicity:\n                bias, heights = self._squeeze_by_scaling(bias, heights)\n            else:\n                bias, heights = self._approximately_project_bounds_only(\n                    bias, heights\n                )\n        return bias, heights\n\n    projected_bias, projected_heights = apply_dykstras_projection(\n        original_bias, original_heights\n    )\n    if num_constraints &gt; 1:\n        for _ in range(self.projection_iterations - 1):\n            projected_bias, projected_heights = apply_dykstras_projection(\n                projected_bias, projected_heights\n            )\n        projected_bias, projected_heights = finalize_constraints(\n            projected_bias, projected_heights\n        )\n\n    self.kernel.data = torch.cat((projected_bias, projected_heights), 0)\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.NumericalCalibrator.assert_constraints","title":"<code>assert_constraints(eps=1e-06)</code>","text":"<p>Asserts that layer satisfies specified constraints.</p> <p>This checks that weights follow monotonicity constraints and that the output is within bounds.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>the margin of error allowed</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of messages describing violated constraints including indices of</p> <code>list[str]</code> <p>monotonicity violations. If no constraints violated, the list will be empty.</p> Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code> <pre><code>@torch.no_grad()\ndef assert_constraints(self, eps: float = 1e-6) -&gt; list[str]:\n    \"\"\"Asserts that layer satisfies specified constraints.\n\n    This checks that weights follow monotonicity constraints and that the output is\n    within bounds.\n\n    Args:\n        eps: the margin of error allowed\n\n    Returns:\n        A list of messages describing violated constraints including indices of\n        monotonicity violations. If no constraints violated, the list will be empty.\n    \"\"\"\n    weights = torch.squeeze(self.kernel.data)\n    messages = []\n\n    if (\n        self.output_max is not None\n        and torch.max(self.keypoints_outputs()) &gt; self.output_max + eps\n    ):\n        messages.append(\"Max weight greater than output_max.\")\n    if (\n        self.output_min is not None\n        and torch.min(self.keypoints_outputs()) &lt; self.output_min - eps\n    ):\n        messages.append(\"Min weight less than output_min.\")\n\n    diffs = weights[1:]\n    violation_indices = []\n\n    if self.monotonicity == Monotonicity.INCREASING:\n        violation_indices = (diffs &lt; -eps).nonzero().tolist()\n    elif self.monotonicity == Monotonicity.DECREASING:\n        violation_indices = (diffs &gt; eps).nonzero().tolist()\n\n    violation_indices = [(i[0], i[0] + 1) for i in violation_indices]\n    if violation_indices:\n        messages.append(f\"Monotonicity violated at: {str(violation_indices)}.\")\n\n    return messages\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.NumericalCalibrator.forward","title":"<code>forward(x)</code>","text":"<p>Calibrates numerical inputs through piece-wise linear interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of shape <code>(batch_size, 1)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor of shape <code>(batch_size, 1)</code> containing calibrated input values.</p> Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Calibrates numerical inputs through piece-wise linear interpolation.\n\n    Args:\n        x: The input tensor of shape `(batch_size, 1)`.\n\n    Returns:\n        torch.Tensor of shape `(batch_size, 1)` containing calibrated input values.\n    \"\"\"\n    if self.input_keypoints_type == InputKeypointsType.LEARNED:\n        softmaxed_logits = torch.nn.functional.softmax(\n            self._interpolation_logits, dim=-1\n        )\n        self._lengths = softmaxed_logits * self._keypoint_range\n        interior_keypoints = (\n            torch.cumsum(self._lengths, dim=-1) + self._keypoint_min\n        )\n        self._interpolation_keypoints = torch.cat(\n            [torch.tensor([self._keypoint_min]), interior_keypoints[:-1]]\n        )\n\n    interpolation_weights = (x - self._interpolation_keypoints) / self._lengths\n    interpolation_weights = torch.minimum(interpolation_weights, torch.tensor(1.0))\n    interpolation_weights = torch.maximum(interpolation_weights, torch.tensor(0.0))\n    interpolation_weights = torch.cat(\n        (torch.ones_like(x), interpolation_weights), -1\n    )\n    result = torch.mm(interpolation_weights, self.kernel)\n\n    if self.missing_input_value is not None:\n        missing_mask = torch.eq(x, self.missing_input_value).long()\n        result = missing_mask * self.missing_output + (1.0 - missing_mask) * result\n\n    return result\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.NumericalCalibrator.keypoints_inputs","title":"<code>keypoints_inputs()</code>","text":"<p>Returns tensor of keypoint inputs.</p> Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code> <pre><code>@torch.no_grad()\ndef keypoints_inputs(self) -&gt; torch.Tensor:\n    \"\"\"Returns tensor of keypoint inputs.\"\"\"\n    return torch.cat(\n        (\n            self._interpolation_keypoints,\n            self._interpolation_keypoints[-1:] + self._lengths[-1:],\n        ),\n        0,\n    )\n</code></pre>"},{"location":"api/layers/#pytorch_lattice.layers.NumericalCalibrator.keypoints_outputs","title":"<code>keypoints_outputs()</code>","text":"<p>Returns tensor of keypoint outputs.</p> Source code in <code>pytorch_lattice/layers/numerical_calibrator.py</code> <pre><code>@torch.no_grad()\ndef keypoints_outputs(self) -&gt; torch.Tensor:\n    \"\"\"Returns tensor of keypoint outputs.\"\"\"\n    return torch.cumsum(self.kernel.data, 0).T[0]\n</code></pre>"},{"location":"api/model_configs/","title":"model_configs","text":""},{"location":"api/model_configs/#pytorch_lattice.model_configs","title":"<code>pytorch_lattice.model_configs</code>","text":"<p>Model configurations classes for PyTorch Calibrated Models.</p>"},{"location":"api/model_configs/#pytorch_lattice.model_configs.LatticeConfig","title":"<code>LatticeConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>_BaseModelConfig</code></p> <p>Configuration for a calibrated lattice model.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>_BaseModelConfig</code> attributes.</p> <code>kernel_init</code> <code>LatticeInit</code> <p>The <code>LatticeInit</code> scheme to use to initialize the lattice kernel.</p> <code>interpolation</code> <code>Interpolation</code> <p>The <code>Interpolation</code> scheme to use in the lattice. Note that <code>HYPERCUBE</code> has exponential time complexity while <code>SIMPLEX</code> has log-linear time complexity.</p> Source code in <code>pytorch_lattice/model_configs.py</code> <pre><code>@dataclass\nclass LatticeConfig(_BaseModelConfig):\n    \"\"\"Configuration for a calibrated lattice model.\n\n    Attributes:\n        All: `_BaseModelConfig` attributes.\n        kernel_init: The `LatticeInit` scheme to use to initialize the lattice kernel.\n        interpolation: The `Interpolation` scheme to use in the lattice. Note that\n            `HYPERCUBE` has exponential time complexity while `SIMPLEX` has\n            log-linear time complexity.\n    \"\"\"\n\n    kernel_init: LatticeInit = LatticeInit.LINEAR\n    interpolation: Interpolation = Interpolation.SIMPLEX\n</code></pre>"},{"location":"api/model_configs/#pytorch_lattice.model_configs.LinearConfig","title":"<code>LinearConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>_BaseModelConfig</code></p> <p>Configuration for a calibrated linear model.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>_BaseModelConfig</code> attributes.</p> <code>use_bias</code> <code>bool</code> <p>Whether to use a bias term for the linear combination.</p> Source code in <code>pytorch_lattice/model_configs.py</code> <pre><code>@dataclass\nclass LinearConfig(_BaseModelConfig):\n    \"\"\"Configuration for a calibrated linear model.\n\n    Attributes:\n        All: `_BaseModelConfig` attributes.\n        use_bias: Whether to use a bias term for the linear combination.\n    \"\"\"\n\n    use_bias: bool = True\n</code></pre>"},{"location":"api/models/","title":"models","text":""},{"location":"api/models/#pytorch_lattice.models.CalibratedLattice","title":"<code>pytorch_lattice.models.CalibratedLattice</code>","text":"<p>             Bases: <code>ConstrainedModule</code></p> <p>PyTorch Calibrated Lattice Model.</p> <p>Creates a <code>torch.nn.Module</code> representing a calibrated lattice model, which will be constructed using the provided model configuration. Note that the model inputs should match the order in which they are defined in the <code>feature_configs</code>.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>__init__</code> arguments.</p> <code>calibrators</code> <p>A dictionary that maps feature names to their calibrators.</p> <code>lattice</code> <p>The <code>Lattice</code> layer of the model.</p> <code>output_calibrator</code> <p>The output <code>NumericalCalibrator</code> calibration layer. This will be <code>None</code> if no output calibration is desired.</p> <p>Example:</p> <pre><code>feature_configs = [...]\ncalibrated_model = CalibratedLattice(feature_configs, ...)\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(calibrated_model.parameters(recurse=True), lr=1e-1)\n\ndataset = pyl.utils.data.Dataset(...)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\nfor epoch in range(100):\n    for inputs, labels in dataloader:\n        optimizer.zero_grad()\n        outputs = calibrated_model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        calibrated_model.apply_constraints()\n</code></pre> Source code in <code>pytorch_lattice/models/calibrated_lattice.py</code> <pre><code>class CalibratedLattice(ConstrainedModule):\n    \"\"\"PyTorch Calibrated Lattice Model.\n\n    Creates a `torch.nn.Module` representing a calibrated lattice model, which will be\n    constructed using the provided model configuration. Note that the model inputs\n    should match the order in which they are defined in the `feature_configs`.\n\n    Attributes:\n        All: `__init__` arguments.\n        calibrators: A dictionary that maps feature names to their calibrators.\n        lattice: The `Lattice` layer of the model.\n        output_calibrator: The output `NumericalCalibrator` calibration layer. This\n            will be `None` if no output calibration is desired.\n\n    Example:\n\n    ```python\n    feature_configs = [...]\n    calibrated_model = CalibratedLattice(feature_configs, ...)\n\n    loss_fn = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(calibrated_model.parameters(recurse=True), lr=1e-1)\n\n    dataset = pyl.utils.data.Dataset(...)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n    for epoch in range(100):\n        for inputs, labels in dataloader:\n            optimizer.zero_grad()\n            outputs = calibrated_model(inputs)\n            loss = loss_fn(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            calibrated_model.apply_constraints()\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        features: list[Union[NumericalFeature, CategoricalFeature]],\n        clip_inputs: bool = True,\n        output_min: Optional[float] = None,\n        output_max: Optional[float] = None,\n        kernel_init: LatticeInit = LatticeInit.LINEAR,\n        interpolation: Interpolation = Interpolation.HYPERCUBE,\n        output_calibration_num_keypoints: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initializes an instance of `CalibratedLattice`.\n\n        Args:\n            features: A list of numerical and/or categorical feature configs.\n            clip_inputs: Whether to restrict inputs to the bounds of lattice.\n            output_min: The minimum output value for the model. If `None`, the minimum\n                output value will be unbounded.\n            output_max: The maximum output value for the model. If `None`, the maximum\n                output value will be unbounded.\n            kernel_init: the method of initializing kernel weights. If otherwise\n                unspecified, will default to `LatticeInit.LINEAR`.\n            interpolation: the method of interpolation in the lattice's forward pass.\n                If otherwise unspecified, will default to `Interpolation.HYPERCUBE`.\n            output_calibration_num_keypoints: The number of keypoints to use for the\n                output calibrator. If `None`, no output calibration will be used.\n\n        Raises:\n            ValueError: If any feature configs are not `NUMERICAL` or `CATEGORICAL`.\n        \"\"\"\n        super().__init__()\n\n        self.features = features\n        self.clip_inputs = clip_inputs\n        self.output_min = output_min\n        self.output_max = output_max\n        self.kernel_init = kernel_init\n        self.interpolation = interpolation\n        self.output_calibration_num_keypoints = output_calibration_num_keypoints\n        self.monotonicities = initialize_monotonicities(features)\n        self.calibrators = initialize_feature_calibrators(\n            features=features,\n            output_min=0,\n            output_max=[feature.lattice_size - 1 for feature in features],\n        )\n\n        self.lattice = Lattice(\n            lattice_sizes=[feature.lattice_size for feature in features],\n            monotonicities=self.monotonicities,\n            clip_inputs=self.clip_inputs,\n            output_min=self.output_min,\n            output_max=self.output_max,\n            interpolation=interpolation,\n            kernel_init=kernel_init,\n        )\n\n        self.output_calibrator = initialize_output_calibrator(\n            output_calibration_num_keypoints=output_calibration_num_keypoints,\n            monotonic=not all(m is None for m in self.monotonicities),\n            output_min=output_min,\n            output_max=output_max,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Runs an input through the network to produce a calibrated lattice output.\n\n        Args:\n            x: The input tensor of feature values of shape `(batch_size, num_features)`.\n\n        Returns:\n            torch.Tensor of shape `(batch_size, 1)` containing the model output result.\n        \"\"\"\n        result = calibrate_and_stack(x, self.calibrators)\n        result = self.lattice(result)\n        if self.output_calibrator is not None:\n            result = self.output_calibrator(result)\n\n        return result\n\n    @torch.no_grad()\n    def apply_constraints(self) -&gt; None:\n        \"\"\"Constrains the model into desired constraints specified by the config.\"\"\"\n        for calibrator in self.calibrators.values():\n            calibrator.apply_constraints()\n        self.lattice.apply_constraints()\n        if self.output_calibrator:\n            self.output_calibrator.apply_constraints()\n\n    @torch.no_grad()\n    def assert_constraints(self, eps: float = 1e-6) -&gt; dict[str, list[str]]:\n        \"\"\"Asserts all layers within model satisfied specified constraints.\n\n        Asserts monotonicity pairs and output bounds for categorical calibrators,\n        monotonicity and output bounds for numerical calibrators, and monotonicity and\n        weights summing to 1 if weighted_average for linear layer.\n\n        Args:\n            eps: the margin of error allowed\n\n        Returns:\n            A dict where key is feature_name for calibrators and 'linear' for the linear\n            layer, and value is the error messages for each layer. Layers with no error\n            messages are not present in the dictionary.\n        \"\"\"\n        messages = {}\n\n        for name, calibrator in self.calibrators.items():\n            calibrator_messages = calibrator.assert_constraints(eps)\n            if calibrator_messages:\n                messages[f\"{name}_calibrator\"] = calibrator_messages\n        lattice_messages = self.lattice.assert_constraints(eps)\n        if lattice_messages:\n            messages[\"lattice\"] = lattice_messages\n        if self.output_calibrator:\n            output_calibrator_messages = self.output_calibrator.assert_constraints(eps)\n            if output_calibrator_messages:\n                messages[\"output_calibrator\"] = output_calibrator_messages\n\n        return messages\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLattice.__init__","title":"<code>__init__(features, clip_inputs=True, output_min=None, output_max=None, kernel_init=LatticeInit.LINEAR, interpolation=Interpolation.HYPERCUBE, output_calibration_num_keypoints=None)</code>","text":"<p>Initializes an instance of <code>CalibratedLattice</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[Union[NumericalFeature, CategoricalFeature]]</code> <p>A list of numerical and/or categorical feature configs.</p> required <code>clip_inputs</code> <code>bool</code> <p>Whether to restrict inputs to the bounds of lattice.</p> <code>True</code> <code>output_min</code> <code>Optional[float]</code> <p>The minimum output value for the model. If <code>None</code>, the minimum output value will be unbounded.</p> <code>None</code> <code>output_max</code> <code>Optional[float]</code> <p>The maximum output value for the model. If <code>None</code>, the maximum output value will be unbounded.</p> <code>None</code> <code>kernel_init</code> <code>LatticeInit</code> <p>the method of initializing kernel weights. If otherwise unspecified, will default to <code>LatticeInit.LINEAR</code>.</p> <code>LINEAR</code> <code>interpolation</code> <code>Interpolation</code> <p>the method of interpolation in the lattice's forward pass. If otherwise unspecified, will default to <code>Interpolation.HYPERCUBE</code>.</p> <code>HYPERCUBE</code> <code>output_calibration_num_keypoints</code> <code>Optional[int]</code> <p>The number of keypoints to use for the output calibrator. If <code>None</code>, no output calibration will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any feature configs are not <code>NUMERICAL</code> or <code>CATEGORICAL</code>.</p> Source code in <code>pytorch_lattice/models/calibrated_lattice.py</code> <pre><code>def __init__(\n    self,\n    features: list[Union[NumericalFeature, CategoricalFeature]],\n    clip_inputs: bool = True,\n    output_min: Optional[float] = None,\n    output_max: Optional[float] = None,\n    kernel_init: LatticeInit = LatticeInit.LINEAR,\n    interpolation: Interpolation = Interpolation.HYPERCUBE,\n    output_calibration_num_keypoints: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initializes an instance of `CalibratedLattice`.\n\n    Args:\n        features: A list of numerical and/or categorical feature configs.\n        clip_inputs: Whether to restrict inputs to the bounds of lattice.\n        output_min: The minimum output value for the model. If `None`, the minimum\n            output value will be unbounded.\n        output_max: The maximum output value for the model. If `None`, the maximum\n            output value will be unbounded.\n        kernel_init: the method of initializing kernel weights. If otherwise\n            unspecified, will default to `LatticeInit.LINEAR`.\n        interpolation: the method of interpolation in the lattice's forward pass.\n            If otherwise unspecified, will default to `Interpolation.HYPERCUBE`.\n        output_calibration_num_keypoints: The number of keypoints to use for the\n            output calibrator. If `None`, no output calibration will be used.\n\n    Raises:\n        ValueError: If any feature configs are not `NUMERICAL` or `CATEGORICAL`.\n    \"\"\"\n    super().__init__()\n\n    self.features = features\n    self.clip_inputs = clip_inputs\n    self.output_min = output_min\n    self.output_max = output_max\n    self.kernel_init = kernel_init\n    self.interpolation = interpolation\n    self.output_calibration_num_keypoints = output_calibration_num_keypoints\n    self.monotonicities = initialize_monotonicities(features)\n    self.calibrators = initialize_feature_calibrators(\n        features=features,\n        output_min=0,\n        output_max=[feature.lattice_size - 1 for feature in features],\n    )\n\n    self.lattice = Lattice(\n        lattice_sizes=[feature.lattice_size for feature in features],\n        monotonicities=self.monotonicities,\n        clip_inputs=self.clip_inputs,\n        output_min=self.output_min,\n        output_max=self.output_max,\n        interpolation=interpolation,\n        kernel_init=kernel_init,\n    )\n\n    self.output_calibrator = initialize_output_calibrator(\n        output_calibration_num_keypoints=output_calibration_num_keypoints,\n        monotonic=not all(m is None for m in self.monotonicities),\n        output_min=output_min,\n        output_max=output_max,\n    )\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLattice.apply_constraints","title":"<code>apply_constraints()</code>","text":"<p>Constrains the model into desired constraints specified by the config.</p> Source code in <code>pytorch_lattice/models/calibrated_lattice.py</code> <pre><code>@torch.no_grad()\ndef apply_constraints(self) -&gt; None:\n    \"\"\"Constrains the model into desired constraints specified by the config.\"\"\"\n    for calibrator in self.calibrators.values():\n        calibrator.apply_constraints()\n    self.lattice.apply_constraints()\n    if self.output_calibrator:\n        self.output_calibrator.apply_constraints()\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLattice.assert_constraints","title":"<code>assert_constraints(eps=1e-06)</code>","text":"<p>Asserts all layers within model satisfied specified constraints.</p> <p>Asserts monotonicity pairs and output bounds for categorical calibrators, monotonicity and output bounds for numerical calibrators, and monotonicity and weights summing to 1 if weighted_average for linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>the margin of error allowed</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>A dict where key is feature_name for calibrators and 'linear' for the linear</p> <code>dict[str, list[str]]</code> <p>layer, and value is the error messages for each layer. Layers with no error</p> <code>dict[str, list[str]]</code> <p>messages are not present in the dictionary.</p> Source code in <code>pytorch_lattice/models/calibrated_lattice.py</code> <pre><code>@torch.no_grad()\ndef assert_constraints(self, eps: float = 1e-6) -&gt; dict[str, list[str]]:\n    \"\"\"Asserts all layers within model satisfied specified constraints.\n\n    Asserts monotonicity pairs and output bounds for categorical calibrators,\n    monotonicity and output bounds for numerical calibrators, and monotonicity and\n    weights summing to 1 if weighted_average for linear layer.\n\n    Args:\n        eps: the margin of error allowed\n\n    Returns:\n        A dict where key is feature_name for calibrators and 'linear' for the linear\n        layer, and value is the error messages for each layer. Layers with no error\n        messages are not present in the dictionary.\n    \"\"\"\n    messages = {}\n\n    for name, calibrator in self.calibrators.items():\n        calibrator_messages = calibrator.assert_constraints(eps)\n        if calibrator_messages:\n            messages[f\"{name}_calibrator\"] = calibrator_messages\n    lattice_messages = self.lattice.assert_constraints(eps)\n    if lattice_messages:\n        messages[\"lattice\"] = lattice_messages\n    if self.output_calibrator:\n        output_calibrator_messages = self.output_calibrator.assert_constraints(eps)\n        if output_calibrator_messages:\n            messages[\"output_calibrator\"] = output_calibrator_messages\n\n    return messages\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLattice.forward","title":"<code>forward(x)</code>","text":"<p>Runs an input through the network to produce a calibrated lattice output.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of feature values of shape <code>(batch_size, num_features)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor of shape <code>(batch_size, 1)</code> containing the model output result.</p> Source code in <code>pytorch_lattice/models/calibrated_lattice.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Runs an input through the network to produce a calibrated lattice output.\n\n    Args:\n        x: The input tensor of feature values of shape `(batch_size, num_features)`.\n\n    Returns:\n        torch.Tensor of shape `(batch_size, 1)` containing the model output result.\n    \"\"\"\n    result = calibrate_and_stack(x, self.calibrators)\n    result = self.lattice(result)\n    if self.output_calibrator is not None:\n        result = self.output_calibrator(result)\n\n    return result\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLinear","title":"<code>pytorch_lattice.models.CalibratedLinear</code>","text":"<p>             Bases: <code>ConstrainedModule</code></p> <p>PyTorch Calibrated Linear Model.</p> <p>Creates a <code>torch.nn.Module</code> representing a calibrated linear model, which will be constructed using the provided model configuration. Note that the model inputs should match the order in which they are defined in the <code>feature_configs</code>.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>__init__</code> arguments.</p> <code>calibrators</code> <p>A dictionary that maps feature names to their calibrators.</p> <code>linear</code> <p>The <code>Linear</code> layer of the model.</p> <code>output_calibrator</code> <p>The output <code>NumericalCalibrator</code> calibration layer. This will be <code>None</code> if no output calibration is desired.</p> <p>Example:</p> <pre><code>feature_configs = [...]\ncalibrated_model = pyl.models.CalibratedLinear(feature_configs, ...)\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(calibrated_model.parameters(recurse=True), lr=1e-1)\n\ndataset = pyl.utils.data.Dataset(...)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\nfor epoch in range(100):\n    for inputs, labels in dataloader:\n        optimizer.zero_grad()\n        outputs = calibrated_model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        calibrated_model.apply_constraints()\n</code></pre> Source code in <code>pytorch_lattice/models/calibrated_linear.py</code> <pre><code>class CalibratedLinear(ConstrainedModule):\n    \"\"\"PyTorch Calibrated Linear Model.\n\n    Creates a `torch.nn.Module` representing a calibrated linear model, which will be\n    constructed using the provided model configuration. Note that the model inputs\n    should match the order in which they are defined in the `feature_configs`.\n\n    Attributes:\n        All: `__init__` arguments.\n        calibrators: A dictionary that maps feature names to their calibrators.\n        linear: The `Linear` layer of the model.\n        output_calibrator: The output `NumericalCalibrator` calibration layer. This\n            will be `None` if no output calibration is desired.\n\n    Example:\n\n    ```python\n    feature_configs = [...]\n    calibrated_model = pyl.models.CalibratedLinear(feature_configs, ...)\n\n    loss_fn = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(calibrated_model.parameters(recurse=True), lr=1e-1)\n\n    dataset = pyl.utils.data.Dataset(...)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n    for epoch in range(100):\n        for inputs, labels in dataloader:\n            optimizer.zero_grad()\n            outputs = calibrated_model(inputs)\n            loss = loss_fn(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            calibrated_model.apply_constraints()\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        features: list[Union[NumericalFeature, CategoricalFeature]],\n        output_min: Optional[float] = None,\n        output_max: Optional[float] = None,\n        use_bias: bool = True,\n        output_calibration_num_keypoints: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initializes an instance of `CalibratedLinear`.\n\n        Args:\n            features: A list of numerical and/or categorical feature configs.\n            output_min: The minimum output value for the model. If `None`, the minimum\n                output value will be unbounded.\n            output_max: The maximum output value for the model. If `None`, the maximum\n                output value will be unbounded.\n            use_bias: Whether to use a bias term for the linear combination. If any of\n                `output_min`, `output_max`, or `output_calibration_num_keypoints` are\n                set, a bias term will not be used regardless of the setting here.\n            output_calibration_num_keypoints: The number of keypoints to use for the\n                output calibrator. If `None`, no output calibration will be used.\n\n        Raises:\n            ValueError: If any feature configs are not `NUMERICAL` or `CATEGORICAL`.\n        \"\"\"\n        super().__init__()\n\n        self.features = features\n        self.output_min = output_min\n        self.output_max = output_max\n        self.use_bias = use_bias\n        self.output_calibration_num_keypoints = output_calibration_num_keypoints\n        self.monotonicities = initialize_monotonicities(features)\n        self.calibrators = initialize_feature_calibrators(\n            features=features, output_min=output_min, output_max=output_max\n        )\n\n        self.linear = Linear(\n            input_dim=len(features),\n            monotonicities=self.monotonicities,\n            use_bias=use_bias,\n            weighted_average=bool(\n                output_min is not None\n                or output_max is not None\n                or output_calibration_num_keypoints\n            ),\n        )\n\n        self.output_calibrator = initialize_output_calibrator(\n            output_calibration_num_keypoints=output_calibration_num_keypoints,\n            monotonic=not all(m is None for m in self.monotonicities),\n            output_min=output_min,\n            output_max=output_max,\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Runs an input through the network to produce a calibrated linear output.\n\n        Args:\n            x: The input tensor of feature values of shape `(batch_size, num_features)`.\n\n        Returns:\n            torch.Tensor of shape `(batch_size, 1)` containing the model output result.\n        \"\"\"\n        result = calibrate_and_stack(x, self.calibrators)\n        result = self.linear(result)\n        if self.output_calibrator is not None:\n            result = self.output_calibrator(result)\n\n        return result\n\n    @torch.no_grad()\n    def apply_constraints(self) -&gt; None:\n        \"\"\"Constrains the model into desired constraints specified by the config.\"\"\"\n        for calibrator in self.calibrators.values():\n            calibrator.apply_constraints()\n        self.linear.apply_constraints()\n        if self.output_calibrator:\n            self.output_calibrator.apply_constraints()\n\n    @torch.no_grad()\n    def assert_constraints(\n        self, eps: float = 1e-6\n    ) -&gt; Union[list[str], dict[str, list[str]]]:\n        \"\"\"Asserts all layers within model satisfied specified constraints.\n\n        Asserts monotonicity pairs and output bounds for categorical calibrators,\n        monotonicity and output bounds for numerical calibrators, and monotonicity and\n        weights summing to 1 if weighted_average for linear layer.\n\n        Args:\n            eps: the margin of error allowed\n\n        Returns:\n            A dict where key is feature_name for calibrators and 'linear' for the linear\n            layer, and value is the error messages for each layer. Layers with no error\n            messages are not present in the dictionary.\n        \"\"\"\n        messages: dict[str, list[str]] = {}\n\n        for name, calibrator in self.calibrators.items():\n            calibrator_messages = calibrator.assert_constraints(eps)\n            if calibrator_messages:\n                messages[f\"{name}_calibrator\"] = calibrator_messages\n        linear_messages = self.linear.assert_constraints(eps)\n        if linear_messages:\n            messages[\"linear\"] = linear_messages\n        if self.output_calibrator:\n            output_calibrator_messages = self.output_calibrator.assert_constraints(eps)\n            if output_calibrator_messages:\n                messages[\"output_calibrator\"] = output_calibrator_messages\n\n        return messages\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLinear.__init__","title":"<code>__init__(features, output_min=None, output_max=None, use_bias=True, output_calibration_num_keypoints=None)</code>","text":"<p>Initializes an instance of <code>CalibratedLinear</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[Union[NumericalFeature, CategoricalFeature]]</code> <p>A list of numerical and/or categorical feature configs.</p> required <code>output_min</code> <code>Optional[float]</code> <p>The minimum output value for the model. If <code>None</code>, the minimum output value will be unbounded.</p> <code>None</code> <code>output_max</code> <code>Optional[float]</code> <p>The maximum output value for the model. If <code>None</code>, the maximum output value will be unbounded.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether to use a bias term for the linear combination. If any of <code>output_min</code>, <code>output_max</code>, or <code>output_calibration_num_keypoints</code> are set, a bias term will not be used regardless of the setting here.</p> <code>True</code> <code>output_calibration_num_keypoints</code> <code>Optional[int]</code> <p>The number of keypoints to use for the output calibrator. If <code>None</code>, no output calibration will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any feature configs are not <code>NUMERICAL</code> or <code>CATEGORICAL</code>.</p> Source code in <code>pytorch_lattice/models/calibrated_linear.py</code> <pre><code>def __init__(\n    self,\n    features: list[Union[NumericalFeature, CategoricalFeature]],\n    output_min: Optional[float] = None,\n    output_max: Optional[float] = None,\n    use_bias: bool = True,\n    output_calibration_num_keypoints: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Initializes an instance of `CalibratedLinear`.\n\n    Args:\n        features: A list of numerical and/or categorical feature configs.\n        output_min: The minimum output value for the model. If `None`, the minimum\n            output value will be unbounded.\n        output_max: The maximum output value for the model. If `None`, the maximum\n            output value will be unbounded.\n        use_bias: Whether to use a bias term for the linear combination. If any of\n            `output_min`, `output_max`, or `output_calibration_num_keypoints` are\n            set, a bias term will not be used regardless of the setting here.\n        output_calibration_num_keypoints: The number of keypoints to use for the\n            output calibrator. If `None`, no output calibration will be used.\n\n    Raises:\n        ValueError: If any feature configs are not `NUMERICAL` or `CATEGORICAL`.\n    \"\"\"\n    super().__init__()\n\n    self.features = features\n    self.output_min = output_min\n    self.output_max = output_max\n    self.use_bias = use_bias\n    self.output_calibration_num_keypoints = output_calibration_num_keypoints\n    self.monotonicities = initialize_monotonicities(features)\n    self.calibrators = initialize_feature_calibrators(\n        features=features, output_min=output_min, output_max=output_max\n    )\n\n    self.linear = Linear(\n        input_dim=len(features),\n        monotonicities=self.monotonicities,\n        use_bias=use_bias,\n        weighted_average=bool(\n            output_min is not None\n            or output_max is not None\n            or output_calibration_num_keypoints\n        ),\n    )\n\n    self.output_calibrator = initialize_output_calibrator(\n        output_calibration_num_keypoints=output_calibration_num_keypoints,\n        monotonic=not all(m is None for m in self.monotonicities),\n        output_min=output_min,\n        output_max=output_max,\n    )\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLinear.apply_constraints","title":"<code>apply_constraints()</code>","text":"<p>Constrains the model into desired constraints specified by the config.</p> Source code in <code>pytorch_lattice/models/calibrated_linear.py</code> <pre><code>@torch.no_grad()\ndef apply_constraints(self) -&gt; None:\n    \"\"\"Constrains the model into desired constraints specified by the config.\"\"\"\n    for calibrator in self.calibrators.values():\n        calibrator.apply_constraints()\n    self.linear.apply_constraints()\n    if self.output_calibrator:\n        self.output_calibrator.apply_constraints()\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLinear.assert_constraints","title":"<code>assert_constraints(eps=1e-06)</code>","text":"<p>Asserts all layers within model satisfied specified constraints.</p> <p>Asserts monotonicity pairs and output bounds for categorical calibrators, monotonicity and output bounds for numerical calibrators, and monotonicity and weights summing to 1 if weighted_average for linear layer.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>the margin of error allowed</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>Union[list[str], dict[str, list[str]]]</code> <p>A dict where key is feature_name for calibrators and 'linear' for the linear</p> <code>Union[list[str], dict[str, list[str]]]</code> <p>layer, and value is the error messages for each layer. Layers with no error</p> <code>Union[list[str], dict[str, list[str]]]</code> <p>messages are not present in the dictionary.</p> Source code in <code>pytorch_lattice/models/calibrated_linear.py</code> <pre><code>@torch.no_grad()\ndef assert_constraints(\n    self, eps: float = 1e-6\n) -&gt; Union[list[str], dict[str, list[str]]]:\n    \"\"\"Asserts all layers within model satisfied specified constraints.\n\n    Asserts monotonicity pairs and output bounds for categorical calibrators,\n    monotonicity and output bounds for numerical calibrators, and monotonicity and\n    weights summing to 1 if weighted_average for linear layer.\n\n    Args:\n        eps: the margin of error allowed\n\n    Returns:\n        A dict where key is feature_name for calibrators and 'linear' for the linear\n        layer, and value is the error messages for each layer. Layers with no error\n        messages are not present in the dictionary.\n    \"\"\"\n    messages: dict[str, list[str]] = {}\n\n    for name, calibrator in self.calibrators.items():\n        calibrator_messages = calibrator.assert_constraints(eps)\n        if calibrator_messages:\n            messages[f\"{name}_calibrator\"] = calibrator_messages\n    linear_messages = self.linear.assert_constraints(eps)\n    if linear_messages:\n        messages[\"linear\"] = linear_messages\n    if self.output_calibrator:\n        output_calibrator_messages = self.output_calibrator.assert_constraints(eps)\n        if output_calibrator_messages:\n            messages[\"output_calibrator\"] = output_calibrator_messages\n\n    return messages\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.CalibratedLinear.forward","title":"<code>forward(x)</code>","text":"<p>Runs an input through the network to produce a calibrated linear output.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of feature values of shape <code>(batch_size, num_features)</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor of shape <code>(batch_size, 1)</code> containing the model output result.</p> Source code in <code>pytorch_lattice/models/calibrated_linear.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Runs an input through the network to produce a calibrated linear output.\n\n    Args:\n        x: The input tensor of feature values of shape `(batch_size, num_features)`.\n\n    Returns:\n        torch.Tensor of shape `(batch_size, 1)` containing the model output result.\n    \"\"\"\n    result = calibrate_and_stack(x, self.calibrators)\n    result = self.linear(result)\n    if self.output_calibrator is not None:\n        result = self.output_calibrator(result)\n\n    return result\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.features.CategoricalFeature","title":"<code>pytorch_lattice.models.features.CategoricalFeature</code>","text":"<p>Feature configuration for categorical features.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>__init__</code> arguments.</p> <code>category_indices</code> <p>A dictionary mapping string categories to their index.</p> <code>monotonicity_index_pairs</code> <p>A conversion of <code>monotonicity_pairs</code> from string categories to category indices. Only available if <code>monotonicity_pairs</code> are provided.</p> Source code in <code>pytorch_lattice/models/features.py</code> <pre><code>class CategoricalFeature:\n    \"\"\"Feature configuration for categorical features.\n\n    Attributes:\n        All: `__init__` arguments.\n        category_indices: A dictionary mapping string categories to their index.\n        monotonicity_index_pairs: A conversion of `monotonicity_pairs` from string\n            categories to category indices. Only available if `monotonicity_pairs` are\n            provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_name: str,\n        categories: Union[list[int], list[str]],\n        missing_input_value: Optional[float] = None,\n        monotonicity_pairs: Optional[list[tuple[str, str]]] = None,\n        lattice_size: int = 2,\n    ) -&gt; None:\n        \"\"\"Initializes a `CategoricalFeatureConfig` instance.\n\n        Args:\n            feature_name: The name of the feature. This should match the header for the\n                column in the dataset representing this feature.\n            categories: The categories that should be used for this feature. Any\n                categories not contained will be considered missing or unknown. If you\n                expect to have such missing categories, make sure to\n            missing_input_value: If provided, this feature's calibrator will learn to\n                map all instances of this missing input value to a learned output value.\n            monotonicity_pairs: List of pairs of categories `(category_a, category_b)`\n                indicating that the calibrator output for `category_b` should be greater\n                than or equal to that of `category_a`.\n            lattice_size: The default number of keypoints outputted by the calibrator.\n                Only used within `Lattice` models.\n        \"\"\"\n        self.feature_name = feature_name\n        self.categories = categories\n        self.missing_input_value = missing_input_value\n        self.monotonicity_pairs = monotonicity_pairs\n        self.lattice_size = lattice_size\n\n        self.category_indices = {category: i for i, category in enumerate(categories)}\n        self.monotonicity_index_pairs = [\n            (self.category_indices[a], self.category_indices[b])\n            for a, b in monotonicity_pairs or []\n        ]\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.features.CategoricalFeature.__init__","title":"<code>__init__(feature_name, categories, missing_input_value=None, monotonicity_pairs=None, lattice_size=2)</code>","text":"<p>Initializes a <code>CategoricalFeatureConfig</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>The name of the feature. This should match the header for the column in the dataset representing this feature.</p> required <code>categories</code> <code>Union[list[int], list[str]]</code> <p>The categories that should be used for this feature. Any categories not contained will be considered missing or unknown. If you expect to have such missing categories, make sure to</p> required <code>missing_input_value</code> <code>Optional[float]</code> <p>If provided, this feature's calibrator will learn to map all instances of this missing input value to a learned output value.</p> <code>None</code> <code>monotonicity_pairs</code> <code>Optional[list[tuple[str, str]]]</code> <p>List of pairs of categories <code>(category_a, category_b)</code> indicating that the calibrator output for <code>category_b</code> should be greater than or equal to that of <code>category_a</code>.</p> <code>None</code> <code>lattice_size</code> <code>int</code> <p>The default number of keypoints outputted by the calibrator. Only used within <code>Lattice</code> models.</p> <code>2</code> Source code in <code>pytorch_lattice/models/features.py</code> <pre><code>def __init__(\n    self,\n    feature_name: str,\n    categories: Union[list[int], list[str]],\n    missing_input_value: Optional[float] = None,\n    monotonicity_pairs: Optional[list[tuple[str, str]]] = None,\n    lattice_size: int = 2,\n) -&gt; None:\n    \"\"\"Initializes a `CategoricalFeatureConfig` instance.\n\n    Args:\n        feature_name: The name of the feature. This should match the header for the\n            column in the dataset representing this feature.\n        categories: The categories that should be used for this feature. Any\n            categories not contained will be considered missing or unknown. If you\n            expect to have such missing categories, make sure to\n        missing_input_value: If provided, this feature's calibrator will learn to\n            map all instances of this missing input value to a learned output value.\n        monotonicity_pairs: List of pairs of categories `(category_a, category_b)`\n            indicating that the calibrator output for `category_b` should be greater\n            than or equal to that of `category_a`.\n        lattice_size: The default number of keypoints outputted by the calibrator.\n            Only used within `Lattice` models.\n    \"\"\"\n    self.feature_name = feature_name\n    self.categories = categories\n    self.missing_input_value = missing_input_value\n    self.monotonicity_pairs = monotonicity_pairs\n    self.lattice_size = lattice_size\n\n    self.category_indices = {category: i for i, category in enumerate(categories)}\n    self.monotonicity_index_pairs = [\n        (self.category_indices[a], self.category_indices[b])\n        for a, b in monotonicity_pairs or []\n    ]\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.features.NumericalFeature","title":"<code>pytorch_lattice.models.features.NumericalFeature</code>","text":"<p>Feature configuration for numerical features.</p> <p>Attributes:</p> Name Type Description <code>All</code> <p><code>__init__</code> arguments.</p> <code>input_keypoints</code> <p>The input keypoints used for this feature's calibrator. These keypoints will be initialized using the given <code>data</code> under the desired <code>input_keypoints_init</code> scheme.</p> Source code in <code>pytorch_lattice/models/features.py</code> <pre><code>class NumericalFeature:\n    \"\"\"Feature configuration for numerical features.\n\n    Attributes:\n        All: `__init__` arguments.\n        input_keypoints: The input keypoints used for this feature's calibrator. These\n            keypoints will be initialized using the given `data` under the desired\n            `input_keypoints_init` scheme.\n    \"\"\"\n\n    def __init__(\n        self,\n        feature_name: str,\n        data: np.ndarray,\n        num_keypoints: int = 5,\n        input_keypoints_init: InputKeypointsInit = InputKeypointsInit.QUANTILES,\n        missing_input_value: Optional[float] = None,\n        monotonicity: Optional[Monotonicity] = None,\n        projection_iterations: int = 8,\n        lattice_size: int = 2,\n    ) -&gt; None:\n        \"\"\"Initializes a `NumericalFeatureConfig` instance.\n\n        Args:\n            feature_name: The name of the feature. This should match the header for the\n                column in the dataset representing this feature.\n            data: Numpy array of float-valued data used for calculating keypoint inputs\n                and initializing keypoint outputs.\n            num_keypoints: The number of keypoints used by the underlying piece-wise\n                linear function of a NumericalCalibrator. There will be\n                `num_keypoints - 1` total segments.\n            input_keypoints_init: The scheme to use for initializing the input\n                keypoints. See `InputKeypointsInit` for more details.\n            missing_input_value: If provided, this feature's calibrator will learn to\n                map all instances of this missing input value to a learned output value.\n            monotonicity: Monotonicity constraint for this feature, if any.\n            projection_iterations: Number of times to run Dykstra's projection\n                algorithm when applying constraints.\n            lattice_size: The default number of keypoints outputted by the\n                calibrator. Only used within `Lattice` models.\n\n        Raises:\n            ValueError: If `data` contains NaN values.\n            ValueError: If `input_keypoints_init` is invalid.\n        \"\"\"\n        self.feature_name = feature_name\n\n        if np.isnan(data).any():\n            raise ValueError(\"Data contains NaN values.\")\n\n        self.data = data\n        self.num_keypoints = num_keypoints\n        self.input_keypoints_init = input_keypoints_init\n        self.missing_input_value = missing_input_value\n        self.monotonicity = monotonicity\n        self.projection_iterations = projection_iterations\n        self.lattice_size = lattice_size\n\n        sorted_unique_values = np.unique(data)\n\n        if input_keypoints_init == InputKeypointsInit.QUANTILES:\n            if sorted_unique_values.size &lt; num_keypoints:\n                logging.info(\n                    \"Observed fewer unique values for feature %s than %d desired \"\n                    \"keypoints. Using the observed %d unique values as keypoints.\",\n                    feature_name,\n                    num_keypoints,\n                    sorted_unique_values.size,\n                )\n                self.input_keypoints = sorted_unique_values\n            else:\n                quantiles = np.linspace(0.0, 1.0, num=num_keypoints)\n                self.input_keypoints = np.quantile(\n                    sorted_unique_values, quantiles, method=\"nearest\"\n                )\n        elif input_keypoints_init == InputKeypointsInit.UNIFORM:\n            self.input_keypoints = np.linspace(\n                sorted_unique_values[0], sorted_unique_values[-1], num=num_keypoints\n            )\n        else:\n            raise ValueError(f\"Unknown input keypoints init: {input_keypoints_init}\")\n</code></pre>"},{"location":"api/models/#pytorch_lattice.models.features.NumericalFeature.__init__","title":"<code>__init__(feature_name, data, num_keypoints=5, input_keypoints_init=InputKeypointsInit.QUANTILES, missing_input_value=None, monotonicity=None, projection_iterations=8, lattice_size=2)</code>","text":"<p>Initializes a <code>NumericalFeatureConfig</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>The name of the feature. This should match the header for the column in the dataset representing this feature.</p> required <code>data</code> <code>ndarray</code> <p>Numpy array of float-valued data used for calculating keypoint inputs and initializing keypoint outputs.</p> required <code>num_keypoints</code> <code>int</code> <p>The number of keypoints used by the underlying piece-wise linear function of a NumericalCalibrator. There will be <code>num_keypoints - 1</code> total segments.</p> <code>5</code> <code>input_keypoints_init</code> <code>InputKeypointsInit</code> <p>The scheme to use for initializing the input keypoints. See <code>InputKeypointsInit</code> for more details.</p> <code>QUANTILES</code> <code>missing_input_value</code> <code>Optional[float]</code> <p>If provided, this feature's calibrator will learn to map all instances of this missing input value to a learned output value.</p> <code>None</code> <code>monotonicity</code> <code>Optional[Monotonicity]</code> <p>Monotonicity constraint for this feature, if any.</p> <code>None</code> <code>projection_iterations</code> <code>int</code> <p>Number of times to run Dykstra's projection algorithm when applying constraints.</p> <code>8</code> <code>lattice_size</code> <code>int</code> <p>The default number of keypoints outputted by the calibrator. Only used within <code>Lattice</code> models.</p> <code>2</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>data</code> contains NaN values.</p> <code>ValueError</code> <p>If <code>input_keypoints_init</code> is invalid.</p> Source code in <code>pytorch_lattice/models/features.py</code> <pre><code>def __init__(\n    self,\n    feature_name: str,\n    data: np.ndarray,\n    num_keypoints: int = 5,\n    input_keypoints_init: InputKeypointsInit = InputKeypointsInit.QUANTILES,\n    missing_input_value: Optional[float] = None,\n    monotonicity: Optional[Monotonicity] = None,\n    projection_iterations: int = 8,\n    lattice_size: int = 2,\n) -&gt; None:\n    \"\"\"Initializes a `NumericalFeatureConfig` instance.\n\n    Args:\n        feature_name: The name of the feature. This should match the header for the\n            column in the dataset representing this feature.\n        data: Numpy array of float-valued data used for calculating keypoint inputs\n            and initializing keypoint outputs.\n        num_keypoints: The number of keypoints used by the underlying piece-wise\n            linear function of a NumericalCalibrator. There will be\n            `num_keypoints - 1` total segments.\n        input_keypoints_init: The scheme to use for initializing the input\n            keypoints. See `InputKeypointsInit` for more details.\n        missing_input_value: If provided, this feature's calibrator will learn to\n            map all instances of this missing input value to a learned output value.\n        monotonicity: Monotonicity constraint for this feature, if any.\n        projection_iterations: Number of times to run Dykstra's projection\n            algorithm when applying constraints.\n        lattice_size: The default number of keypoints outputted by the\n            calibrator. Only used within `Lattice` models.\n\n    Raises:\n        ValueError: If `data` contains NaN values.\n        ValueError: If `input_keypoints_init` is invalid.\n    \"\"\"\n    self.feature_name = feature_name\n\n    if np.isnan(data).any():\n        raise ValueError(\"Data contains NaN values.\")\n\n    self.data = data\n    self.num_keypoints = num_keypoints\n    self.input_keypoints_init = input_keypoints_init\n    self.missing_input_value = missing_input_value\n    self.monotonicity = monotonicity\n    self.projection_iterations = projection_iterations\n    self.lattice_size = lattice_size\n\n    sorted_unique_values = np.unique(data)\n\n    if input_keypoints_init == InputKeypointsInit.QUANTILES:\n        if sorted_unique_values.size &lt; num_keypoints:\n            logging.info(\n                \"Observed fewer unique values for feature %s than %d desired \"\n                \"keypoints. Using the observed %d unique values as keypoints.\",\n                feature_name,\n                num_keypoints,\n                sorted_unique_values.size,\n            )\n            self.input_keypoints = sorted_unique_values\n        else:\n            quantiles = np.linspace(0.0, 1.0, num=num_keypoints)\n            self.input_keypoints = np.quantile(\n                sorted_unique_values, quantiles, method=\"nearest\"\n            )\n    elif input_keypoints_init == InputKeypointsInit.UNIFORM:\n        self.input_keypoints = np.linspace(\n            sorted_unique_values[0], sorted_unique_values[-1], num=num_keypoints\n        )\n    else:\n        raise ValueError(f\"Unknown input keypoints init: {input_keypoints_init}\")\n</code></pre>"},{"location":"api/plots/","title":"plots","text":""},{"location":"api/plots/#pytorch_lattice.plots","title":"<code>pytorch_lattice.plots</code>","text":"<p>Plotting functions for PyTorch Lattice calibrated models using matplotlib.</p>"},{"location":"api/plots/#pytorch_lattice.plots.calibrator","title":"<code>calibrator(model, feature_name)</code>","text":"<p>Plots the calibrator for the given feature and calibrated model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[CalibratedLinear, CalibratedLattice]</code> <p>The calibrated model for which to plot calibrators.</p> required <code>feature_name</code> <code>str</code> <p>The name of the feature for which to plot the calibrator.</p> required Source code in <code>pytorch_lattice/plots.py</code> <pre><code>def calibrator(\n    model: Union[CalibratedLinear, CalibratedLattice],\n    feature_name: str,\n) -&gt; None:\n    \"\"\"Plots the calibrator for the given feature and calibrated model.\n\n    Args:\n        model: The calibrated model for which to plot calibrators.\n        feature_name: The name of the feature for which to plot the calibrator.\n    \"\"\"\n    if feature_name not in model.calibrators:\n        raise ValueError(f\"Feature {feature_name} not found in model.\")\n\n    calibrator = model.calibrators[feature_name]\n    input_keypoints = calibrator.keypoints_inputs().numpy()\n    output_keypoints = calibrator.keypoints_outputs().numpy()\n\n    if isinstance(calibrator, CategoricalCalibrator):\n        model_feature = next(\n            (x for x in model.features if x.feature_name == feature_name), None\n        )\n        if isinstance(model_feature, CategoricalFeature):\n            input_keypoints = np.array(\n                [\n                    model_feature.categories[i]\n                    if i &lt; len(input_keypoints) - 1\n                    else \"&lt;Missing&gt;\"\n                    for i, ik in enumerate(input_keypoints)\n                ]\n            )\n        plt.xticks(rotation=45)\n        plt.bar(input_keypoints, output_keypoints)\n    else:\n        plt.plot(input_keypoints, output_keypoints)\n\n    plt.title(f\"Calibrator: {feature_name}\")\n    plt.xlabel(\"Input Keypoints\")\n    plt.ylabel(\"Output Keypoints\")\n    plt.show()\n</code></pre>"},{"location":"api/plots/#pytorch_lattice.plots.linear_coefficients","title":"<code>linear_coefficients(model)</code>","text":"<p>Plots the coefficients for the linear layer of a calibrated linear model.</p> Source code in <code>pytorch_lattice/plots.py</code> <pre><code>def linear_coefficients(model: CalibratedLinear) -&gt; None:\n    \"\"\"Plots the coefficients for the linear layer of a calibrated linear model.\"\"\"\n    if not isinstance(model, CalibratedLinear):\n        raise ValueError(\n            \"Model must be a `CalibratedLinear` model to plot linear coefficients.\"\n        )\n    linear_coefficients = dict(\n        zip(\n            [feature.feature_name for feature in model.features],\n            model.linear.kernel.detach().numpy().flatten(),\n        )\n    )\n    if model.use_bias:\n        linear_coefficients[\"bias\"] = model.linear.bias.detach().numpy()[0]\n\n    plt.bar(list(linear_coefficients.keys()), list(linear_coefficients.values()))\n    plt.title(\"Linear Coefficients\")\n    plt.xlabel(\"Feature Name\")\n    plt.xticks(rotation=45)\n    plt.ylabel(\"Coefficient Value\")\n    plt.show()\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#pytorch_lattice.utils.data","title":"<code>pytorch_lattice.utils.data</code>","text":"<p>Utility functions and classes for handling data.</p>"},{"location":"api/utils/#pytorch_lattice.utils.data.Dataset","title":"<code>Dataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>A class for loading a dataset for a calibrated model.</p> Source code in <code>pytorch_lattice/utils/data.py</code> <pre><code>class Dataset(torch.utils.data.Dataset):\n    \"\"\"A class for loading a dataset for a calibrated model.\"\"\"\n\n    def __init__(\n        self,\n        X: pd.DataFrame,\n        y: np.ndarray,\n        features: list[Union[NumericalFeature, CategoricalFeature]],\n    ):\n        \"\"\"Initializes an instance of `Dataset`.\"\"\"\n        self.X = X.copy()\n        self.y = y.copy()\n\n        selected_features = [feature.feature_name for feature in features]\n        unavailable_features = set(selected_features) - set(self.X.columns)\n        if len(unavailable_features) &gt; 0:\n            raise ValueError(f\"Features {unavailable_features} not found in dataset.\")\n\n        drop_features = list(set(self.X.columns) - set(selected_features))\n        self.X.drop(drop_features, axis=1, inplace=True)\n        prepare_features(self.X, features)\n\n        self.data = torch.from_numpy(self.X.values).double()\n        self.labels = torch.from_numpy(self.y).double()[:, None]\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        if isinstance(idx, torch.Tensor):\n            idx = idx.tolist()\n\n        return [self.data[idx], self.labels[idx]]\n</code></pre>"},{"location":"api/utils/#pytorch_lattice.utils.data.Dataset.__init__","title":"<code>__init__(X, y, features)</code>","text":"<p>Initializes an instance of <code>Dataset</code>.</p> Source code in <code>pytorch_lattice/utils/data.py</code> <pre><code>def __init__(\n    self,\n    X: pd.DataFrame,\n    y: np.ndarray,\n    features: list[Union[NumericalFeature, CategoricalFeature]],\n):\n    \"\"\"Initializes an instance of `Dataset`.\"\"\"\n    self.X = X.copy()\n    self.y = y.copy()\n\n    selected_features = [feature.feature_name for feature in features]\n    unavailable_features = set(selected_features) - set(self.X.columns)\n    if len(unavailable_features) &gt; 0:\n        raise ValueError(f\"Features {unavailable_features} not found in dataset.\")\n\n    drop_features = list(set(self.X.columns) - set(selected_features))\n    self.X.drop(drop_features, axis=1, inplace=True)\n    prepare_features(self.X, features)\n\n    self.data = torch.from_numpy(self.X.values).double()\n    self.labels = torch.from_numpy(self.y).double()[:, None]\n</code></pre>"},{"location":"api/utils/#pytorch_lattice.utils.data.prepare_features","title":"<code>prepare_features(X, features)</code>","text":"<p>Maps categorical features to their integer indices in place.</p> Source code in <code>pytorch_lattice/utils/data.py</code> <pre><code>def prepare_features(\n    X: pd.DataFrame, features: list[Union[NumericalFeature, CategoricalFeature]]\n):\n    \"\"\"Maps categorical features to their integer indices in place.\"\"\"\n    for feature in features:\n        feature_data = X[feature.feature_name]\n\n        if isinstance(feature, CategoricalFeature):\n            feature_data = feature_data.map(feature.category_indices)\n\n        if feature.missing_input_value is not None:\n            feature_data = feature_data.fillna(feature.missing_input_value)\n\n        X[feature.feature_name] = feature_data\n</code></pre>"},{"location":"api/utils/#pytorch_lattice.utils.models","title":"<code>pytorch_lattice.utils.models</code>","text":"<p>Utility functions for use in model classes.</p>"},{"location":"api/utils/#pytorch_lattice.utils.models.calibrate_and_stack","title":"<code>calibrate_and_stack(x, calibrators)</code>","text":"<p>Helper function to run calibrators along columns of given data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor of feature values of shape <code>(batch_size, num_features)</code>.</p> required <code>calibrators</code> <code>ModuleDict</code> <p>A dictionary of calibrator functions.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A torch.Tensor resulting from applying the calibrators and stacking the results.</p> Source code in <code>pytorch_lattice/utils/models.py</code> <pre><code>def calibrate_and_stack(\n    x: torch.Tensor,\n    calibrators: torch.nn.ModuleDict,\n) -&gt; torch.Tensor:\n    \"\"\"Helper function to run calibrators along columns of given data.\n\n    Args:\n        x: The input tensor of feature values of shape `(batch_size, num_features)`.\n        calibrators: A dictionary of calibrator functions.\n\n    Returns:\n        A torch.Tensor resulting from applying the calibrators and stacking the results.\n    \"\"\"\n    return torch.column_stack(\n        tuple(\n            calibrator(x[:, i, None])\n            for i, calibrator in enumerate(calibrators.values())\n        )\n    )\n</code></pre>"},{"location":"api/utils/#pytorch_lattice.utils.models.initialize_feature_calibrators","title":"<code>initialize_feature_calibrators(features, output_min=None, output_max=None)</code>","text":"<p>Helper function to initialize calibrators for calibrated model.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[Union[NumericalFeature, CategoricalFeature]]</code> <p>A list of numerical and/or categorical feature configs.</p> required <code>output_min</code> <code>Optional[float]</code> <p>The minimum output value for the model. If <code>None</code>, the minimum output value will be unbounded.</p> <code>None</code> <code>output_max</code> <code>Union[Optional[float], list[Optional[float]]]</code> <p>A list of maximum output value for each feature of the model. If <code>None</code>, the maximum output value will be unbounded. If a singular value, it will be taken as the maximum of all features.</p> <code>None</code> <p>Returns:</p> Type Description <code>ModuleDict</code> <p>A <code>torch.nn.ModuleDict</code> of calibrators accessible by each feature's name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any feature configs are not <code>NUMERICAL</code> or <code>CATEGORICAL</code>.</p> Source code in <code>pytorch_lattice/utils/models.py</code> <pre><code>def initialize_feature_calibrators(\n    features: list[Union[NumericalFeature, CategoricalFeature]],\n    output_min: Optional[float] = None,\n    output_max: Union[Optional[float], list[Optional[float]]] = None,\n) -&gt; torch.nn.ModuleDict:\n    \"\"\"Helper function to initialize calibrators for calibrated model.\n\n    Args:\n        features: A list of numerical and/or categorical feature configs.\n        output_min: The minimum output value for the model. If `None`, the minimum\n            output value will be unbounded.\n        output_max: A list of maximum output value for each feature of the model. If\n            `None`, the maximum output value will be unbounded. If a singular value, it\n            will be taken as the maximum of all features.\n\n    Returns:\n        A `torch.nn.ModuleDict` of calibrators accessible by each feature's name.\n\n    Raises:\n        ValueError: If any feature configs are not `NUMERICAL` or `CATEGORICAL`.\n    \"\"\"\n    calibrators = torch.nn.ModuleDict()\n    if not isinstance(output_max, list):\n        output_max = [output_max] * len(features)\n    for feature, feature_max in zip(features, output_max):\n        if isinstance(feature, NumericalFeature):\n            calibrators[feature.feature_name] = NumericalCalibrator(\n                input_keypoints=feature.input_keypoints,\n                missing_input_value=feature.missing_input_value,\n                output_min=output_min,\n                output_max=feature_max,\n                monotonicity=feature.monotonicity,\n                kernel_init=NumericalCalibratorInit.EQUAL_SLOPES,\n                projection_iterations=feature.projection_iterations,\n            )\n        elif isinstance(feature, CategoricalFeature):\n            calibrators[feature.feature_name] = CategoricalCalibrator(\n                num_categories=len(feature.categories),\n                missing_input_value=feature.missing_input_value,\n                output_min=output_min,\n                output_max=feature_max,\n                monotonicity_pairs=feature.monotonicity_index_pairs,\n                kernel_init=CategoricalCalibratorInit.UNIFORM,\n            )\n        else:\n            raise ValueError(f\"Unknown type {type(feature)} for feature {feature}\")\n    return calibrators\n</code></pre>"},{"location":"api/utils/#pytorch_lattice.utils.models.initialize_monotonicities","title":"<code>initialize_monotonicities(features)</code>","text":"<p>Helper function to initialize monotonicities for calibrated model.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[Union[NumericalFeature, CategoricalFeature]]</code> <p>A list of numerical and/or categorical feature configs.</p> required <p>Returns:</p> Type Description <code>list[Optional[Monotonicity]]</code> <p>A list of <code>None</code> or <code>Monotonicity.INCREASING</code> based on whether</p> <code>list[Optional[Monotonicity]]</code> <p>each feature has a monotonicity or not.</p> Source code in <code>pytorch_lattice/utils/models.py</code> <pre><code>def initialize_monotonicities(\n    features: list[Union[NumericalFeature, CategoricalFeature]]\n) -&gt; list[Optional[Monotonicity]]:\n    \"\"\"Helper function to initialize monotonicities for calibrated model.\n\n    Args:\n        features: A list of numerical and/or categorical feature configs.\n\n    Returns:\n        A list of `None` or `Monotonicity.INCREASING` based on whether\n        each feature has a monotonicity or not.\n    \"\"\"\n    monotonicities = [\n        None\n        if (isinstance(feature, CategoricalFeature) and not feature.monotonicity_pairs)\n        or (isinstance(feature, NumericalFeature) and feature.monotonicity is None)\n        else Monotonicity.INCREASING\n        for feature in features\n    ]\n    return monotonicities\n</code></pre>"},{"location":"api/utils/#pytorch_lattice.utils.models.initialize_output_calibrator","title":"<code>initialize_output_calibrator(monotonic, output_calibration_num_keypoints, output_min=None, output_max=None)</code>","text":"<p>Helper function to initialize output calibrator for calibrated model.</p> <p>Parameters:</p> Name Type Description Default <code>monotonic</code> <code>bool</code> <p>Whether output calibrator should have monotonicity constraint.</p> required <code>output_calibration_num_keypoints</code> <code>Optional[int]</code> <p>The number of keypoints in output calibrator. If <code>0</code> or <code>None</code>, no output calibrator will be returned.</p> required <code>output_min</code> <code>Optional[float]</code> <p>The minimum output value for the model. If <code>None</code>, the minimum output value will be unbounded.</p> <code>None</code> <code>output_max</code> <code>Optional[float]</code> <p>The maximum output value for the model. If <code>None</code>, the maximum output value will be unbounded.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[NumericalCalibrator]</code> <p>A <code>torch.nn.ModuleDict</code> of calibrators accessible by each feature's name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any feature configs are not <code>NUMERICAL</code> or <code>CATEGORICAL</code>.</p> Source code in <code>pytorch_lattice/utils/models.py</code> <pre><code>def initialize_output_calibrator(\n    monotonic: bool,\n    output_calibration_num_keypoints: Optional[int],\n    output_min: Optional[float] = None,\n    output_max: Optional[float] = None,\n) -&gt; Optional[NumericalCalibrator]:\n    \"\"\"Helper function to initialize output calibrator for calibrated model.\n\n    Args:\n        monotonic: Whether output calibrator should have monotonicity constraint.\n        output_calibration_num_keypoints: The number of keypoints in output\n            calibrator. If `0` or `None`, no output calibrator will be returned.\n        output_min: The minimum output value for the model. If `None`, the minimum\n            output value will be unbounded.\n        output_max: The maximum output value for the model. If `None`, the maximum\n            output value will be unbounded.\n\n    Returns:\n        A `torch.nn.ModuleDict` of calibrators accessible by each feature's name.\n\n    Raises:\n        ValueError: If any feature configs are not `NUMERICAL` or `CATEGORICAL`.\n    \"\"\"\n    if output_calibration_num_keypoints:\n        output_calibrator = NumericalCalibrator(\n            input_keypoints=np.linspace(0.0, 1.0, num=output_calibration_num_keypoints),\n            missing_input_value=None,\n            output_min=output_min,\n            output_max=output_max,\n            monotonicity=Monotonicity.INCREASING if monotonic else None,\n            kernel_init=NumericalCalibratorInit.EQUAL_HEIGHTS,\n        )\n        return output_calibrator\n    return None\n</code></pre>"},{"location":"concepts/calibrators/","title":"Calibrators","text":"<p>Calibrators are one of the core concepts of the PyTorch Lattice library. The library currently implements two types of calibrators:</p> <ul> <li><code>CategoricalCalibrator</code>: calibrates a categorical value through a mapping from a category to a learned value.</li> <li><code>NumericalCalibrator</code>: calibrates a numerical value through a learned piece-wise linear function.</li> </ul> Categorical Calibrator Numerical Calibrator"},{"location":"concepts/calibrators/#feature-calibrators","title":"Feature Calibrators","text":"<p>In a calibrated model, the first layer is the calibration layer that calibrates each feature using a calibrator that's learned per feature.</p> <p>There are three primary benefits to using feature calibrators:</p> <ul> <li>Automated Feature Pre-Processing. Rather than relying on the practitioner to determine how to best transform each feature, feature calibrators learn the best transformations from the data.</li> <li>Additional Interpretability. Plotting calibrators as bar/line charts helps visualize how the model is understanding each feature. For example, if two input values for a feature have the same calibrated value, then the model considers those two input values equivalent with respect to the prediction.</li> <li>Shape Constraints. Calibrators can be constrained to guarantee certain expected input/output behavior. For example, you might a monotonicity constraint on a feature for square footage to ensure that increasing square footage always increases predicted price. Or perhaps you want a concavity constraint such that increasing a feature for price first increases and then decreases predicted sales.</li> </ul>"},{"location":"concepts/calibrators/#output-calibration","title":"Output Calibration","text":"<p>You can also use a <code>NumericalCalibrator</code> as the final layer for a model, which is called output calibration. This can provide additional flexibility to the overall model function.</p> <p>Furthermore, you can use an output calibrator for post-training distribution matching to calibrate your model to a new distribution without retraining the rest of the model.</p>"},{"location":"concepts/classifier/","title":"Classifier","text":"<p>The <code>Classifier</code> class is a high-level wrapper around the calibrated modeling functionality to make it extremely easy to fit a calibrated model to a classification task. The class uses declarative configuration and automatically handles the data preparation, feature configuration, model creation, and model training necessary for properly training a calibrated model.</p>"},{"location":"concepts/classifier/#initialization","title":"Initialization","text":"<p>The only required parameter for creating a classifier is the list of features to use:</p> <pre><code>clf = pyl.Classifier([\"list\", \"of\", \"features\"])\n</code></pre> <p>You do not need to include all of the feature present in your dataset. When you specify only a subset of the features, the classifier will automatically handle selecting only those features for training.</p>"},{"location":"concepts/classifier/#fitting","title":"Fitting","text":"<p>Fitting the classifier to your data is as simple as calling <code>fit(...)</code>:</p> <pre><code>clf.fit(X, y)\n</code></pre> <p>You can additionally further specify hyperparameters used for fitting such as <code>epochs</code>, <code>batch_size</code>, and <code>learning_rate</code>. Just pass the values in as parameters:</p> <pre><code>clf.fit(X, y, epochs=100, batch_size=512, learning_rate=1e-4)\n</code></pre> <p>When you call fit, the classifier will train a new model, overwriting any previously trained model. If you want to run a hyperparameter optimization job to find the best setting of hyperparameters, you can first extract the trained model before calling fit again:</p> <pre><code>models = []\nfor epochs, batch_size, learning_rate in hyperparameters:\n    clf.fit(X, y, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)\n    models.append(clf.model)\n</code></pre> <p>The benefit of extracting the model is that you can reuse the same classifier configuration; however, you can also always create a new classifier for each setting instead:</p> <pre><code>clfs = []\nfor epochs, batch_size, learning_rate in hyperparameters:\n    clf = pyl.Classifier(X.columns).fit(\n        X, y, epochs=epochs, batch_size=batch_size, learning_rate=learning_rate\n    )\n    clfs.append(clf)\n</code></pre>"},{"location":"concepts/classifier/#generate-predictions","title":"Generate Predictions","text":"<p>You can generate predictions using the <code>predict(...)</code> function:</p> <pre><code>probabilities = clf.predict(X)\nlogits = clf.predict(X, logits=True)\n</code></pre> <p>Just make sure that the input <code>pd.DataFrame</code> contains all of the features the classifier is expecting.</p>"},{"location":"concepts/classifier/#model-configuration","title":"Model Configuration","text":"<p>To configure the type of calibrated model to use for the classifier, you can additionally provide a model configuration during initialization:</p> <pre><code>model_config = pyl.model_configs.LinearConfig(use_bias=False)\nclf = pyl.Classifier([\"list\", \"of\", \"features\"], model_config)\n</code></pre> <p>See Model Types for more information on the supported model types and model_configs for more information on configuring these models in a classifier. </p>"},{"location":"concepts/classifier/#feature-configuration","title":"Feature Configuration","text":"<p>When you first initialize a calibrator, all features will be initialized using default values. You can further specify configuration options for features by retrieve the feature's configuration from the classifier and calling the corresponding function to set that option:</p> <pre><code>clf.configure(\"feature\").monotonicity(\"increasing\").num_keypoints(10)\n</code></pre> <p>See feature_configs for all of the available configuration options.</p>"},{"location":"concepts/classifier/#categorical-features","title":"Categorical Features","text":"<p>If the value type for a feature in the dataset is not numerical (e.g. string), the classifier will automatically handle the feature as categorical, using all unique categories present in the dataset as the categories for the calibrator.</p> <p>If you want the classifier to handle a discrete numerical value as a categorical feature, simply convert the values to strings:</p> <pre><code>X[\"categorical_feature\"] = X[\"categorical_feature\"].astype(str)\n</code></pre> <p>Additionally you can specify a list of categories to use as a configuration option:</p> <pre><code>clf.configure(\"categorical_feature\").categories([\"list\", \"of\", \"categories\"])\n</code></pre> <p>Any category in the dataset that is not present in the configured category list will be lumped together into a missing category bucket, which will also have a learned calibration. This can be particularly useful if there are categories in your dataset that appear in very few examples.</p>"},{"location":"concepts/classifier/#saving-loading","title":"Saving &amp; Loading","text":"<p>The <code>Classifier</code> class also provides easy save/load functionality so that you can save your classifiers and load them as necessary to generate predictions:</p> <pre><code>clf.save(\"path/to/dir\")\nloaded_clf = pyl.Classifier.load(\"path/to/dir\")\n</code></pre>"},{"location":"concepts/model_types/","title":"Model Types","text":"<p>The PyTorch Lattice library currently supports two types of calibrated modeling:</p> <ul> <li> <p><code>CalibratedLinear</code>: a calibrated linear model combines calibrated features using a standard linear layer, optionally followed by an output calibrator.</p> </li> <li> <p><code>CalibratedLattice</code>: a calibrated lattice model combines calibrated features using a lattice layer, optionally followed by an output calibrator. The lattice layer can learn higher-order feature interactions, which can help increase model flexibility and thereby performance on more complex prediction tasks.</p> </li> </ul>"},{"location":"concepts/plotting/","title":"Plotting","text":"<p>The <code>plots</code> module provides useful plotting utility functions for visualizing calibrated models.</p>"},{"location":"concepts/plotting/#feature-calibrators","title":"Feature Calibrators","text":"<p>For any calibrated model, you can plot feature calibrators. The plotting utility will automatically determine the feature type and generate the corresponding calibrator visualization:</p> <pre><code>pyl.plots.calibrator(clf.model, \"feature\")\n</code></pre> Categorical Calibrator Numerical Calibrator <p>The <code>calibrator(...)</code> function expects a calibrated model as the first argument so that you can use these functions even if you train a calibrated model manually without the <code>Classifier</code> class.</p>"},{"location":"concepts/plotting/#linear-coefficients","title":"Linear Coefficients","text":"<p>For calibrated linear models, you can also plot the linear coefficients as a bar chart to better understand how the model is combining calibrated feature values:</p> <pre><code>pyl.plots.linear_coefficients(clf.model)\n</code></pre> <p></p>"},{"location":"concepts/shape_constraints/","title":"Shape Constraints","text":"<p>Shape constraints play a crucial role in making calibrated models interpretable by allowing users to impose specific behavioral rules on their machine learning models. These constraints help to reduce \u2013 or even eliminate \u2013 the impact of noise and inherent biases contained in the data.</p> <p><code>Monotonicity</code> constraints ensure that the relationship between an input feature and the output prediction consistently increases or decreases. Let's consider our house price prediction task once more. A monotonic constraint on the square footage feature would guarantee that increasing the size of the property increases the predicted price. This makes sense.</p> <p>Unimodality constraints (coming soon) create a single peak in the model's output, ensuring that there is only one optimal value for a given input feature. For example, a feature for price used when predicting sales volume may be unimodal since lower prices generally lead to higher sales, but prices that are too low may indicate low quality with one single optimal price.</p> <p>Convexity/Concavity constraints (coming soon) ensure that the given feature's value has a convex/concave relationship with the model's output. Looking again at the feature for price for predicting sales volume, it may be that there is a range of optimal prices and not one single optimal price, which would instead be a concavity constraint.</p> <p>Trust constraints (coming soon) define the relative importance of input features depending on other features. For instance, a trust constraint can ensure that a model predicting product sales relies more on the star rating (1-5) when the number of reviews is higher, which forces the model's predictions to better align with real-world expectations and rules.</p> <p>Dominance constraints (coming soon) are intended to embed that a dominant feature is more important than a weak feature. For example, you might want to constrain a model predicting click-through-rate (CTR) for a specific web link to be more sensitive to past CTR for that web link than the average CTR for the whole website.</p> <p>Together, these shape constraints help create machine learning models that are both interpretable and trustworthy.</p>"},{"location":"walkthroughs/uci_adult_income/","title":"UCI Adult Income","text":"<p>For this walkthrough you are going to fit a <code>Classifier</code> to the UCI Adult Income dataset to predict whether or not a given input makes more or less than $50K.</p>"},{"location":"walkthroughs/uci_adult_income/#install-and-import-packages","title":"Install and import packages","text":"<p>On top of PyTorch Lattice, you will be using <code>scikit-learn</code> for calculating metrics to evaluate your classifiers. First, make sure you have these packages installed:</p> <pre><code>$ pip install pytorch-lattice scikit-learn\n</code></pre> <p>Next, import these packages in your script:</p> <pre><code>from sklearn import metrics\n\nimport pytorch_lattice as pyl\n</code></pre>"},{"location":"walkthroughs/uci_adult_income/#load-the-uci-adult-income-dataset","title":"Load the UCI Adult Income dataset","text":"<p>The <code>Classifier</code> expects a <code>pandas.DataFrame</code> containing the training data and a <code>numpy.ndarray</code> containing the labels for each example. You can use the PyTorch Lattice datasets module to load the data in this form:</p> <pre><code>X, y = pyl.datasets.adult()\n</code></pre>"},{"location":"walkthroughs/uci_adult_income/#create-and-configure-a-classifier","title":"Create and configure a <code>Classifier</code>","text":"<p>Next, you'll want to create a <code>Classifier</code> that you can use to fit to a calibrated model to the data. When creating a <code>Classifier</code>, the only required field is the list of features to use. For this guide, you are only going to use four features: <code>age</code>, <code>education_num</code>, <code>occupation</code>, and <code>hours_per_week</code>.</p> <p>By default the calibrated modeling type used will be a calibrated linear model. If you'd like to further configure the model, you can provide a <code>model_config</code> with the attributes of your choice. For this guide, you will train a <code>CalibratedLattice</code> model.</p> <pre><code>model_config = pyl.model_configs.LatticeConfig()\nclf = pyl.Classifier(\n    [\"age\", \"education_num\", \"occupation\", \"hours_per_week\"], model_config\n)\n</code></pre>"},{"location":"walkthroughs/uci_adult_income/#configure-features","title":"Configure features","text":"<p>One of the primary benefits of using PyTorch Lattice models is the ability to easily add shape constraints that guarantee certain real-world expectations. For example, it would make sense that someone with a higher level of education would be more likely to make more than $50K compared to someone with a lower level of education, all else being equal. Similarly, you might expect someone who works more hours per week to be more likely to make more than $50K compared to someone who works less hours per week, all else being equal.</p> <pre><code>clf.configure(\"education_num\").monotonicity(\"increasing\")\nclf.configure(\"hours_per_week\").monotonicity(\"increasing\")\n</code></pre> <p>PyTorch Lattice makes it very easy to ensure that the model behaves as expected even after training on data. By setting the <code>monotonicity</code> field to <code>increasing</code> for both <code>education_num</code> and <code>hours_per_week</code>, you will guarantee that increasing either of those features will increase the prediction (so long as no other feature values change).</p> <p>Of course, the classifier is still trained on data, so the only thing guaranteed is the relationship. How much increasing <code>education_num</code> or <code>hours_per_week</code> will increase the prediction will still be learned from training. Ultimately you are reducing the risk of unknown outcomes while still learning from data.</p>"},{"location":"walkthroughs/uci_adult_income/#fit-the-classifier-to-the-data","title":"Fit the classifier to the data","text":"<p>Now that you've configured the classifier, fitting it to the data is easy:</p> <pre><code>clf.fit(X, y, batch_size=1024)  # using a larger batch size for faster training\n</code></pre> <p>There are additional training configuration options that you can set such as the number of epochs for which to fit the classifier, the learning rate, and the batch size, which you can set as parameters of the <code>fit</code> function.</p>"},{"location":"walkthroughs/uci_adult_income/#generate-predictions-and-evaluate-auc","title":"Generate predictions and evaluate AUC","text":"<p>Once you've fit your classifier it's easy to generate predictions:</p> <pre><code>preds = clf.predict(X)\n</code></pre> <p>You can then use <code>scikit-learn</code> to calculate <code>AUC</code> to evaluate the predictive quality of your classifier:</p> <pre><code>fpr, tpr, _ = metrics.roc_curve(y, preds)\nprint(f\"Train AUC: {metrics.auc(fpr, tpr)}\")\n# Train AUC: 0.8165439029459205\n</code></pre>"},{"location":"walkthroughs/uci_adult_income/#plot-calibrators-for-analysis","title":"Plot calibrators for analysis","text":"<p>Plotting the calibrators for each feature can help visualize how the model is understanding the features. First, try plotting the calibrator for <code>occupation</code>:</p> <pre><code>pyl.plots.calibrator(clf.model, \"occupation\")\n</code></pre> <p></p> <p>You can see here how each category for <code>occupation</code> gets calibrated before going into the lattice layer of the model, which shows us relatively how the model understands each category. For example, you can see that the model things that <code>Sales</code> and <code>Armed-Forces</code> have a similar likelihood of making more than $50K.</p> <p>Interestingly, plotting the calibrator for <code>hours_per_week</code> shows that there's a flat region starting around ~52 hours. This indicates that there is a chance that the <code>hours_per_week</code> feature is not actually monotonically increasing, in which case you might consider training a new classifier where you do not constrain this feature.</p> <p></p> <p>When setting constraints, there are two things to keep in mind:</p> <ol> <li>Do you want to guarantee the constrained behavior regardless of performance? In this case, setting the constraint can make sure that model behavior matches your expectations on unseen examples, which is especially useful when using a model to make decisions.</li> <li>Does the model have better performance on a validation dataset if you remove the constraint? It is important to remember that adding constraints to a feature may result in worse performance on the training set but actually result in better performance on the validation set. This is because the constraint helps the model to better handle unseen examples if the constraint should in fact be set.</li> </ol>"}]}